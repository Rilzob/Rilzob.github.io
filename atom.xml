<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>极氙世界</title>
  <icon>https://www.gravatar.com/avatar/01a8e9aa2bb51f443bf32f125ff052ed</icon>
  <subtitle>追求梦想，永不止步</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://rilzob.com/"/>
  <updated>2019-03-24T04:18:32.669Z</updated>
  <id>https://rilzob.com/</id>
  
  <author>
    <name>Rilzob</name>
    <email>watermirrorsir@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>第二章 向量</title>
    <link href="https://rilzob.com/2019/03/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E5%90%91%E9%87%8F/"/>
    <id>https://rilzob.com/2019/03/24/数据结构第二章 向量/</id>
    <published>2019-03-24T01:54:35.164Z</published>
    <updated>2019-03-24T04:18:32.669Z</updated>
    
    <content type="html"><![CDATA[<p>  本章的讲解围绕向量结构的高效实现而逐步展开，包括其作为抽象数据类型的接口规范以及对应的算法，尤其是高效维护动态向量的技巧。此外，还针对有序向量，系统介绍经典的查找与排序算法，并就其性能做一分析对比，这也是本章的难点与重点所在。最后，还引入复杂度下界的概念，并通过建立比较树模型，针对基于比较式算法给出复杂度下界的统一界定方法。<br><a id="more"></a></p><ul><li>数据结构是数据项的结构化集合。数据结构划分为<strong>线性结构、半线性结构和非线性结构</strong>三大类。</li><li>最为基本的线性结构统称为<strong>序列(sequence)</strong>，根据其中数据项的逻辑次序与其物理存储地址的对应关系不同，又可进一步地将序列分为<strong>向量(Vector)</strong>和<strong>列表(List)</strong>。</li><li>在向量中，所有数据项的物理存放位置与其逻辑次序完全吻合，此时的逻辑次序也称作<strong>秩(Rank)</strong>。而在列表中，逻辑上相邻的数据项在物理上未必相邻，而是采用间接定址的方式通过封装后的位置(position)相互引用。</li></ul><h1 id="从数组到向量"><a href="#从数组到向量" class="headerlink" title="从数组到向量"></a>从数组到向量</h1><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><ul><li>具体地，数组A[]中的每一个元素都唯一对应于某一下标编号，记作A[0,n) = {A[0], A[1], .., A[n-1]}。其中，对于任何 0 ≤ i &lt; j &lt; n，A[i]都是A[j]的<strong>前驱(predecessor)</strong>，A[j]都是A[i]的<strong>后继(successor)</strong>。特别地，对于任何i ≥ 1，A[i-1]称作A[i]的<strong>直接前驱(immediate predecessor)</strong>；对于任何i ≤ n - 2，A[i+1]称作A[i]的<strong>直接后继(immediate successor)</strong>。任一元素的所有前驱构成其<strong>前缀(prefix)</strong>，所有后继构成其<strong>后缀(suffix)</strong>。 </li><li>具体地，若数组A[]存放空间的起始位置为A，且每个元素占用s个单位的空间，则元素A[i]对应的物理地址为：<strong>A + i * s</strong>，因其中元素的物理地址与其下标之间满足这种线性关系，故亦称作<strong>线性数组(linear array)</strong>。</li></ul><h2 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h2><ul><li>以此前介绍的线性递归为例，运行过程中所出现过的所有递归实例，按照相互调用的关系可构成一个线性序列。在此序列中，各递归实例的秩反映了它们各自被创建的时间先后，每一递归实例的秩等于早于它出现的实例总数。反过来，通过r亦可唯一确定$e = v_r$。这是向量特有的元素访问方式，称作<strong>循秩访问(call-by-rank)</strong>。</li><li>向量的特点：不限定同一向量中的各元素都属于同一基本类型，它们本身可以是来自于更具一般性的某一类的对象。另外，各元素也不见得同时具有某一数值属性，故而并不保证它们之间能够相互比较大小。</li></ul><h1 id="构造与析构"><a href="#构造与析构" class="headerlink" title="构造与析构"></a>构造与析构</h1><p>  向量结构在内部维护一个元素类型为<code>T</code>的私有数组<code>_elem[]</code>：其容量由私有变量<code>_capacity</code>指示；有效元素的数量(即向量当前的实际规模)，则有<code>_size</code>指示。此外还进一步地约定，在向量元素的秩、数组单元的逻辑编号以及物理地址之间，具有如下对应关系：<br><em>向量中秩为r的元素，对应于内部数组中的<code>_elem[r]</code>，其物理地址为<code>_elem+r</code></em></p><h2 id="默认构造方法"><a href="#默认构造方法" class="headerlink" title="默认构造方法"></a>默认构造方法</h2><p>  默认的构造方法是，首先根据创建者指定的初始容量，向系统申请空间，以创建内部私有数组<code>_elem[]</code>；若容量未明确指定，则使用默认值<code>DEFAULT_CAPACITY</code>。接下来，鉴于初生的向量尚不包含任何元素，故将指示规模的变量<code>_size</code>初始化为0。<br>整个过程顺序进行，没有任何迭代，故若忽略用于分配数组空间的时间，共需常数时间。</p><h2 id="基于复制的构造方法"><a href="#基于复制的构造方法" class="headerlink" title="基于复制的构造方法"></a>基于复制的构造方法</h2><p>  向量的另一典型创建方式，是以某个已有的向量或数组为蓝本，进行(局部或整体的)克隆。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vector_constructor_by_copying.h</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="comment">// 元素类型</span></span><br><span class="line"><span class="keyword">void</span> Vector&lt;T&gt;::copyFrom(T <span class="keyword">const</span> *A, Rank lo, Rank hi) &#123; <span class="comment">// 以数组区间A[lo,hi)为蓝本复制向量</span></span><br><span class="line">    _elem = <span class="keyword">new</span> T[_capacity = <span class="number">2</span> * (hi - lo) ]; _size = <span class="number">0</span>; <span class="comment">// 分配空间，规模清零</span></span><br><span class="line">    <span class="keyword">while</span>(lo &lt; hi) <span class="comment">// A[lo, hi)内的元素逐一</span></span><br><span class="line">        _elem[_size++] = A[lo++]; <span class="comment">// 复制至_elem[0, hi-lo)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>  <code>copyFrom()</code>首先根据待复制区间的边界，换算出新向量的初始规模；再以<strong>双倍</strong>的容量，为内部数组<code>_elem[]</code>申请空间。最后通过一趟迭代，完成区间<code>A[lo, hi)</code> <strong>(注意这里是左闭右开)</strong> 内各元素的顺次复制。<br>若忽略开辟新空间所需的时间，运行时间应正比于区间宽度，即$O(hi - lo) = O(size)$。</p><p>  需强调的是，由于向量内部含有动态分配的空间，默认的运算符”=”不足以支持向量之间的直接赋值(因为向量内存储数据的数据类型并非全是基本数据类型)。<br>为适应此类赋值操作的需求，重载向量的赋值运算符。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vector_assignment.h</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; Vector&lt;T&gt;&amp; Vector&lt;T&gt;::<span class="keyword">operator</span>=(Vector&lt;T&gt; <span class="keyword">const</span>&amp; V) &#123; <span class="comment">// 重载</span></span><br><span class="line">    <span class="keyword">if</span> (_elem) <span class="keyword">delete</span> [] _elem; <span class="comment">// 释放原有内容</span></span><br><span class="line">    copyFrom(V.elem, <span class="number">0</span>, V.size()); <span class="comment">// 整体复制</span></span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>; <span class="comment">// 返回当前对象的引用，以便链式赋值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="析构方法"><a href="#析构方法" class="headerlink" title="析构方法"></a>析构方法</h2><p>与析构函数不同，同一对象只能有一个析构函数，不得重载。<br>向量对象的析构过程，只需释放用于存放元素的内部数组<code>_elem[]</code>，将其占用的空间交还操作系统。<code>_capacity</code>和<code>_size</code>之类的内部变量无需做任何处理，它们将作为向量对象自身的一部分被系统回收，此后既无需也无法被引用。<br>若不计系统用于空间回收的时间，整个析构过程只需O(1)时间。</p><h1 id="动态空间管理"><a href="#动态空间管理" class="headerlink" title="动态空间管理"></a>动态空间管理</h1><h2 id="静态空间管理"><a href="#静态空间管理" class="headerlink" title="静态空间管理"></a>静态空间管理</h2><ul><li>内部数组所占物理空间的容量，若在向量的生命期内不允许调整，则称作<strong>静态空间管理策略</strong>。</li><li>向量实际规模与其内部数组容量的比值(即_size/_capacity)，亦称作<strong>装填因子(load factor)</strong>。</li></ul><p><em>如何才能保证向量的装填因子既不致于超过1，也不致于太接近于0？</em><br>为此，需要改用动态空间管理策略。其中一种有效的方法，即使用所谓的可扩充向量。</p><h2 id="可扩充向量"><a href="#可扩充向量" class="headerlink" title="可扩充向量"></a>可扩充向量</h2><p>  <strong>可扩充向量(extendable vector)</strong> 的原理：若内部数组仍有空余，则操作可照常执行。每经一次插入(删除)，可用空间都会减少(增加)一个单元。一旦空间耗尽，就动态地扩大内部数组的容量。这里的难点及关键在于：<br><em>如何实现扩容？新的容量取作多少才算适宜？</em></p><p>  一种可行的方法，我们需要另行申请一个容量更大的数组B[]，并将原数组中的成员集体搬迁至新的空间，此后访客顺利地插入新元素e而不致溢出。当然，原数组所占的空间，需要及时释放并归还操作系统。</p><h2 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h2><p>基于以上策略的扩容算法<code>expand()</code>。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vector_expand.h</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">void</span> Vector&lt;T&gt;::expand() &#123; <span class="comment">// 向量空间不足时扩容</span></span><br><span class="line">    <span class="keyword">if</span>(_size &lt; _capacity) <span class="keyword">return</span>; <span class="comment">// 尚未满员时，不必扩容</span></span><br><span class="line">    <span class="keyword">if</span>(_capacity &lt; DEFAULT_CAPACITY) _capacity = DEFAULT_CAPACITY; <span class="comment">// 不低于最小容量</span></span><br><span class="line">    T* oldElem = _elem; _elem = <span class="keyword">new</span> T[_capacity &lt;&lt;= <span class="number">1</span>]; <span class="comment">// 容量加倍</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; _size; i++)</span><br><span class="line">        _elem[i] = oldElem[i]; <span class="comment">// 复制原向量内容(T为基本类型，或已重载赋值操作符'='）</span></span><br><span class="line">    <span class="keyword">delete</span> [] oldElem; <span class="comment">// 释放原空间</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>  请注意，新数组的地址由操作系统分配，与原数据区没有直接的关系。这种情况下，若直接饮用数组，往往会导致共同指向原数组的其他指针失效，称为野指针 <strong>(wild pointer)</strong>；而封装为向量后，即可继续准确地引用各元素，从而有效地避免野指针的风险。</p><p>  <strong>这里的关键在于，新数组的容量总是取作原数组的两倍</strong>————这正是上述后一问题的答案。取作二倍的目的是预留一些空间后，使得将来足够长的时间内，不会因为有必要扩容而打断我们的计算过程。下面介绍通过分摊分析对用于扩容的时间成本进行分析。</p><h2 id="分摊分析-amortized-analysis"><a href="#分摊分析-amortized-analysis" class="headerlink" title="分摊分析(amortized analysis)"></a>分摊分析(amortized analysis)</h2><h3 id="分摊复杂度"><a href="#分摊复杂度" class="headerlink" title="分摊复杂度"></a>分摊复杂度</h3><p>  对可扩充向量的足够多次连续操作，并将其间所消耗的时间，分摊至所有的操作。如此分摊平均至单词操作的时间成本，称作<strong>分摊运行时间(amortized running time)。</strong><br>  注意这一指标与<strong>平均运行时间(average running time)</strong> 有着本质的区别。后者是按照某种假定的概率分布，对各种情况下所需执行时间的加权平均，故亦称作期望运行时间(expected running time)。而前者则要求，参与分摊的操作必须构成和来自一个真实可行的操作序列，而且该序列还必须足够地长。<br>  相对而言，<u>分摊复杂度可以针对计算成本和效率，做出更为客观而准确的设计</u>(优点)。比如在这里，在任何一个可扩充向量的生命期内，在任何足够长的连续操作序列中，以任何固定间隔连续出现上述最坏情况的概率均为0，故常规的平均复杂度根本不具任何参考意义。</p><h3 id="O-1-分摊时间"><a href="#O-1-分摊时间" class="headerlink" title="O(1)分摊时间"></a>O(1)分摊时间</h3><p>  假定数组的初始容量为某一常数N。既然是估计复杂度的上界，故不妨设向量的初始规模也为N——即将溢出。另外不难看出，除插入操作外，向量其余的接口操作既不会直接导致溢出，也不会增加伺候溢出的可能性，因此不妨考察最坏的情况，假设在伺候需要连续地进行n次<code>insert()</code>操作，n &gt;&gt; N。首先定义如下函数：<br><em>size(n) = 连续插入n个元素后向量的规模</em><br><em>capacity(n) = 连续插入n个元素后数组的容量</em><br><em>T(n) = 为连续插入n个元素而花费于扩容的时间</em><br>  其中，向量规模从N开始随着操作的进程逐步递增，故有：$size(n) = N + n$<br>  既然不致溢出，故装填因子绝不会超出100%。同时，这里的扩容采用了“懒惰”策略——只有在的确即将发生溢出时，才不得不将容量加倍——因此装填因子也始终不低于50%。<br>概括起来，始终应有：<br>$size(n) ≤ capacity(n) ≤ 2 * size(n)$<br>考虑到N为常数，故有：<br>$capacity(n) = Θ(size(n)) = Θ(n)$<br>  容量以2为比例按指数速度增长，在容量达到capacity(n)之前，共做过$Θ(log_2n)$次扩容，每次扩容所需时间线性正比于当时的容量(或规模)，且同样以2为比例按指数速度增长。因此，消耗于扩容的时间累计不过：<br>$T(n) = 2N + 4N + 8N + … + capacity(n) &lt; 2 * capacity(n) = Θ(n)$<br>  将其分摊到其间的连续n次操作，单次操作所需的分摊运行时间应为$O(1)$。</p><h3 id="其它扩容策略"><a href="#其它扩容策略" class="headerlink" title="其它扩容策略"></a>其它扩容策略</h3><p>  早期可扩充向量多采用另一策略：一旦有必要，则追加固定数目的单元。实际上，无论采用的固定常数多大，在最坏情况下，此类数组单次操作的分摊时间复杂度都高达$Ω(n)$。</p><h2 id="缩容"><a href="#缩容" class="headerlink" title="缩容"></a>缩容</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vector_shrink.h</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">void</span> Vector&lt;T&gt;::shrink() &#123; <span class="comment">// 装填因子过小时压缩向量所占空间</span></span><br><span class="line">    <span class="keyword">if</span>(_capacity &lt; DEFAULT_CAPACITY &lt;&lt; <span class="number">1</span>) <span class="keyword">return</span>; <span class="comment">// 不致收缩到DEFAULT_CAPACITY以下</span></span><br><span class="line">    <span class="keyword">if</span>(_size &lt;&lt; <span class="number">2</span> &gt; _capacity) <span class="keyword">return</span>; <span class="comment">// 以25%为界</span></span><br><span class="line">    T* oldElem = _elem; _elem = <span class="keyword">new</span> T[_capacity &gt;&gt;= <span class="number">1</span>]; <span class="comment">// 容量减半</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; _size; i++) &#123;</span><br><span class="line">        _elem[i] = oldElem[i]; <span class="comment">// 复制原向量内容</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> [] oldElem; <span class="comment">// 释放原空间</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  与<code>expand()</code>操作类似，尽管单次<code>shrink()</code>操作需要线性量级$Ω(n)$的时间，但其分摊复杂度亦为$O(1)$。</p><h1 id="常规向量"><a href="#常规向量" class="headerlink" title="常规向量"></a>常规向量</h1><h2 id="直接引用元素"><a href="#直接引用元素" class="headerlink" title="直接引用元素"></a>直接引用元素</h2><p>  重载操作符”[]”，使向量ADT访问元素的方式与数组直接通过下标访问元素的方式(形如”A[i]”)相同。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vector_bracket.h </span><br><span class="line">代码<span class="number">2.6</span> 重载向量操作符[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; T&amp; Vector&lt;T&gt;::<span class="keyword">operator</span>[](Rank r) <span class="keyword">const</span> <span class="comment">// 重载下标操作符</span></span><br><span class="line">&#123; <span class="keyword">return</span> _elem[r];&#125; <span class="comment">// assert: 0 &lt;= r &lt; _size</span></span><br></pre></td></tr></table></figure></p><h2 id="置乱器"><a href="#置乱器" class="headerlink" title="置乱器"></a>置乱器</h2><h3 id="置乱算法"><a href="#置乱算法" class="headerlink" title="置乱算法"></a>置乱算法</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">permute.h</span><br><span class="line">代码<span class="number">2.7</span> 向量整体置乱算法permute()</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;<span class="function"><span class="keyword">void</span> <span class="title">permute</span><span class="params">(Vector&lt;T&gt;&amp; V)</span></span>&#123; <span class="comment">//随机置乱向量，使各元素等概率出现在各位置</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = V.size(); i &gt; <span class="number">0</span>; i--) <span class="comment">// 自后向前</span></span><br><span class="line">        swap(V[i<span class="number">-1</span>], V[rand() % i]); <span class="comment">// V[i-1]与V[0,i)中某一随机元素交换</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  该算法从置乱区间的末元素开始，逆序地向前逐一处理各元素。对每一个当前元素V[i - 1]，先通过调用<code>rand()</code>函数在[0, i)之间等概率地随机选取一个元素，再令二者互换位置。注意，这里的交换操作<code>swap()</code>，隐含了三次基于重载操作符”[]”的赋值。<br>每经过一步这样的迭代，置乱区间都会向前拓展一个单元。因此经过O(n)步迭代后，即实现了整个向量的置乱。</p><p>  从理论上讲，使用这里的<code>permute()</code>算法，不仅可以枚举出同一向量所有可能的排列，而且能够保证生成各种排列的概率相等。</p><h3 id="区间置乱接口"><a href="#区间置乱接口" class="headerlink" title="区间置乱接口"></a>区间置乱接口</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vector_unsort.h</span><br><span class="line">代码<span class="number">2.8</span> 向量区间置乱接口unsort()</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">void</span> Vector&lt;T&gt;::unsort(Rank lo, Rank hi) &#123; <span class="comment">// 等概率随机置乱区间[lo,hi)</span></span><br><span class="line">    T* V = _elem + lo; <span class="comment">// 将子向量_elem[lo,hi)视作另一向量V[0,hi-lo)</span></span><br><span class="line">    <span class="keyword">for</span> (Rank i = hi - lo; i &gt; <span class="number">0</span> ; i--) &#123; <span class="comment">// 自后向前</span></span><br><span class="line">        swap(V[i<span class="number">-1</span>], V[rand() % i]); <span class="comment">// 将V[i-1]与V[0, i)中某一元素随机交换</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  通过该接口，可以均匀地置乱任一向量区间[lo, hi)内的元素，故通用性有所提高。可见，只要将该区间等效地视作另一向量V，即可从形式上完整地套用以上<code>permute()</code>算法的流程。<br>尽管如此，还要特别留意代码2.7和代码2.8之间的细微差异：后者是通过下标，直接访问内部数组的元素；而前者则是借助重载的操作符”[]”，通过秩间接地访问向量的元素。</p><h2 id="判等器与比较器"><a href="#判等器与比较器" class="headerlink" title="判等器与比较器"></a>判等器与比较器</h2><p>  从算法的角度来看，”判断两个对象是否相等”与”判断两个对象的相对大小”都是至关重要的操作，它们直接控制者算法执行的分支方向。这两种操作之间既有联系也有区别，不能相互替代。比如，有些对象只能比对但不能比较；反之，支持比较的对象未必支持比对。不过，出于简化的考虑，在很多场合并不需要严格地将二者区分开来。<br>  算法实现的简洁性和通用性，在很大程度上体现于：针对整数等特定数据类型的某种实现，可否推广至可比较或可比对的任何数据类型，而不必关心如何定义以及判定其大小或相等关系。若能如此，就可以将比对和比较操作的具体实现剥离出来，直接讨论算法流程本身。<br>  为此，通常可以采用两种方法。其一，将比对操作和比较操作分别分装成通用的判等器和比较器。其二，在定义对应的数据类型时，通过重载”&lt;”和”==”之类的操作符，给出大小和相等关系的具体定义及其判别方法。<br>  在一些复杂的数据结构中，内部元素本身的类型可能就是指向其它对象的指针；而从外部更多关注的，则往往是其所指对象的大小。若不加处理而直接根据指针的数值(即被指对象的物理地址)进行比较，则所得结果将毫无意义。</p><h2 id="无序查找"><a href="#无序查找" class="headerlink" title="无序查找"></a>无序查找</h2><h3 id="判等器"><a href="#判等器" class="headerlink" title="判等器"></a>判等器</h3><p>仅支持比对，但未必支持比较的向量，称作<strong>无序向量(unsorted vector)。</strong></p><h3 id="顺序查找"><a href="#顺序查找" class="headerlink" title="顺序查找"></a>顺序查找</h3><p>逐个比对的查找方式，称作<strong>顺序查找(sequential search)。</strong></p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vector_find.h</span><br><span class="line">代码<span class="number">2.10</span> 无序向量元素查找接口find()</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="comment">// 无序向量的顺序查找：返回最后一个元素e的位置；失败时，返回lo - 1</span></span><br><span class="line">Rank Vector&lt;T&gt;::find(T <span class="keyword">const</span> &amp;e, Rank lo, Rank hi) <span class="keyword">const</span> &#123; <span class="comment">// assert: 0 &lt;= lo &lt; hi &lt;= _size</span></span><br><span class="line">    <span class="keyword">while</span> ((lo &lt; hi--) &amp;&amp; (e != _elem[hi])); <span class="comment">// 从后向前，顺序查找</span></span><br><span class="line">    <span class="keyword">return</span> hi; <span class="comment">// 若hi &lt; lo, 则意味着失败；否则hi即命中元素的秩</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  其中若干细微之处，需要体会。比如，当同时多个命中元素时，本书统一约定返回其中秩最大者——故这里采用了<strong>自后向前</strong>的查找次序。如此，一旦命中即可立即返回，从而省略掉不必要的比对。另外，查找失败时约定统一返回-1。这不仅简化了对查找失败情况的判别，同时也使此时的返回结果更加易于理解——只要假想者在秩为-1处植入一个与任何对象对相等的哨兵元素，则返回该元素的秩当且仅当查找失败。<br>  最后一处需要留意。while循环的控制逻辑由两部分组成，首先判断是否已抵达通配符，再判断当前元素与目标元素是否相等。利用了C/C++语言中逻辑表达式的短路求值特性，在前一判断非真厚循环会立即终止，而不致因试图引用已越界的秩(-1)而出错。</p><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><p>  最坏情况，查找终止于首元素<code>_elem[lo]</code>，运行时间为O(hi - lo) = O(n)。最好情况下，查找命中于末元素<code>_elem[hi - 1]</code>，仅需O(1)时间。对于规模相同、内部组成不同的输入，渐进运行时间却有本质区别，故此类算法也称作<strong>输入敏感的(input sensitive)</strong> 算法。</p><h2 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h2><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vector_insert.h</span><br><span class="line">代码<span class="number">2.11</span> 向量元素插入接口insert()</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="comment">// 将e作为秩为r元素插入</span></span><br><span class="line">Rank Vector&lt;T&gt;::insert(Rank r, <span class="keyword">const</span> T &amp;e) &#123; <span class="comment">// assert: 0 &lt;= r &lt;= size</span></span><br><span class="line">    expand(); <span class="comment">// 若有必要，扩容</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = _size; i &gt; r; i--) &#123;</span><br><span class="line">        _elem[i] = _elem[i<span class="number">-1</span>]; <span class="comment">// 自后向前，后继元素顺次后移一个单元</span></span><br><span class="line">    &#125;</span><br><span class="line">    _elem[r] = e; _size++; <span class="comment">// 置入新元素并更新容量</span></span><br><span class="line">    <span class="keyword">return</span> r; <span class="comment">// 返回秩</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  插入前首先调用<code>expand()</code>算法核对是否即将溢出，若有必要，则加倍扩容。为保证数组元素的物理地址连续，随后需要将后缀<code>_elem[r, _size)</code>（若非空)整体后移一个单元。这些后继元素<strong>自后向前</strong>的搬迁次序不能颠倒，否则会因元素被覆盖而造成数据丢失(自后向前的原因)。在单元<code>_elem[r]</code>腾出之后，方可将待插入对象e置入其中。</p><h3 id="复杂度-1"><a href="#复杂度-1" class="headerlink" title="复杂度"></a>复杂度</h3><p>  时间主要消耗于后继元素的后移，线性正比于后缀的长度，故总体为$O(  size - r + 1)$。<br>  新插入元素越靠后(前)所需时间越短(长)。特别地，r取最大值_size时为最好情况，只需$O(1)$时间；r取最小值0时，需要$O(size)$时间。一般地，若插入位置等概率分布，则平均运行时间为$O(size) = O(n)$，线性正比于向量的实际规模。</p><h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><p>  删除操作重载有两个接口，<code>remove(lo, hi)</code>用以删除区间[lo, hi)内的元素，而<code>remove(r)</code>用以删除秩为r的单个元素。乍看起来，利用后者即可实现前者：令r从hi - 1到lo递减，反复调用<code>remove(r)</code>。实际可行的思路恰好相反，<strong>应将单元素删除视作区间删除的特例</strong>，并基于后者实现前者(单元素删除与区间删除的关系)。如此可将移动操作的总次数控制在$O(m)$以内，而与待删除区间的宽度无关。</p><h3 id="区间删除：remove-lo-hi"><a href="#区间删除：remove-lo-hi" class="headerlink" title="区间删除：remove(lo, hi)"></a>区间删除：<code>remove(lo, hi)</code></h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vector_removeInterval.h</span><br><span class="line">代码<span class="number">2.12</span> 向量区间删除接口remove(lo, hi)</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">int</span> Vector&lt;T&gt;::remove(Rank lo, Rank hi) &#123; <span class="comment">//</span></span><br><span class="line">    <span class="keyword">if</span> (lo == hi) <span class="keyword">return</span> <span class="number">0</span>; <span class="comment">// 出于效率考虑，单独处理退化情况，比如remove(0, 0)</span></span><br><span class="line">    <span class="keyword">while</span> (hi &lt; _size) _elem[lo++] = _elem[hi++] ; <span class="comment">// [hi, _size)顺次前移hi - lo个单元</span></span><br><span class="line">    _size = lo; <span class="comment">// 更新规模，直接丢弃尾部[lo, _size = hi)区间</span></span><br><span class="line">    shrink(); <span class="comment">// 若有必要，则缩容</span></span><br><span class="line">    <span class="keyword">return</span> hi - lo; <span class="comment">// 返回被删除元素的数目</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  设[lo, hi)为向量的合法区间，则其后缀[hi, n)需整体前移hi - lo个单元。与插入算法同理，这里后继元素<strong>自前往后</strong>的移动次序也不能颠倒。</p><h3 id="单元素删除：remove-r"><a href="#单元素删除：remove-r" class="headerlink" title="单元素删除：remove(r)"></a>单元素删除：<code>remove(r)</code></h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vector_remove.h</span><br><span class="line">代码<span class="number">2.13</span> 向量单元素删除接口remove()</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; T Vector&lt;T&gt;::remove(Rank r) &#123; <span class="comment">// 删除向量中秩为r的元素, 0 &lt;= r &lt;= size</span></span><br><span class="line">    T e = _elem[r]; <span class="comment">// 备份被删除元素</span></span><br><span class="line">    remove(r, r + <span class="number">1</span>); <span class="comment">// 调用区间删除算法，等效于对区间[r, r+1)的删除</span></span><br><span class="line">    <span class="keyword">return</span> e; <span class="comment">// 返回被删除元素</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="复杂度-2"><a href="#复杂度-2" class="headerlink" title="复杂度"></a>复杂度</h3><p><code>remove(lo, hi)</code>的计算成本，主要消耗于后续元素的前移，线性正比于后缀的长度，总体不过$O(m+1) =  O(size - hi + 1)$。区间删除操作所需的时间，应该仅取决于后继元素的数目，而与被删除区间本社你的宽度无关。特别地，基于该接口实现的单元素删除接口<code>remove(r)</code>需耗时$O(size - r)$。也就是说，被删除元素在向量中的位置越靠后(前)所需时间越短(长)，最好为$O(1)$，最坏为$O(n) = O(size)$。</p><h2 id="唯一化"><a href="#唯一化" class="headerlink" title="唯一化"></a>唯一化</h2><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vector_deduplicate.h</span><br><span class="line">代码<span class="number">2.14</span> 无序向量清除重复元素接口depulicate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">int</span> Vector&lt;T&gt;::deduplicate() &#123; <span class="comment">// 删除无序向量中重复元素(高效版)</span></span><br><span class="line">    <span class="keyword">int</span> oldSize = _size; <span class="comment">// 记录原规模</span></span><br><span class="line">    Rank i = <span class="number">1</span>; <span class="comment">// 从_elem[1]开始</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; _size) <span class="comment">// 自前向后逐一考察各元素_elem[i]</span></span><br><span class="line">        ( find( _elem[i], <span class="number">0</span>, i) &lt; <span class="number">0</span>) ? <span class="comment">// 在其前缀中寻找与之雷同者(至多一个)</span></span><br><span class="line">        i++ : remove(i); <span class="comment">// 若无雷同则继续考察其后继，否则删除雷同者</span></span><br><span class="line">    <span class="keyword">return</span> oldSize - _size; <span class="comment">// 向量规模变化量，即被删除元素总数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  该算法自前往后逐一考察各元素<code>_elem[i]</code>，并通过调用<code>find()</code>接口，在其前缀中寻找与之雷同者。若找到，则随即删除；否则，转而考察当前元素的后继。</p><h3 id="正确性"><a href="#正确性" class="headerlink" title="正确性"></a>正确性</h3><p>  算法的正确性由以下不变性保证:</p><p><em>在while循环中，在当前元素的前缀<code>_elem[0, i)</code>内，所有元素彼此互异。</em></p><p>  初次进入循环时i=1，只有唯一的前驱<code>_elem[0]</code>，故不变性自然满足。<br>  假设在转至元素e = <code>_elem[i]</code>之前不变性一直成立。于是，经过针对该元素的一步迭代之后，无非两种结果：<br>  1)若元素e的前缀<code>_elem[0, i)</code>中不含与之雷同的元素，则在做过i++之后，新的前缀<code>_elem[0, i)</code>将继续满足不变性，而且其规模增加一个单位。<br>  2)反之，若含存在与e雷同的元素，则由此前一直满足的不变性可知，这样的雷同元素不超过一个。因此在删除e之后，前缀<code>_elem[0, i)</code>依旧保持不变性。</p><h3 id="复杂度-3"><a href="#复杂度-3" class="headerlink" title="复杂度"></a>复杂度</h3><p>  这里所需的时间，主要消耗于<code>find()</code>和<code>remove()</code>两个接口，前一部分时间应先行正比于查找区间的宽度，即前驱的总数；后一部分时间应线性正比于后继的总数。因此，每步迭代所需时间为$O(n)$，总体复杂度应为$O(n^2)$。</p><h2 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h2><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vector_traverse.h</span><br><span class="line">代码<span class="number">2.15</span> 向量遍历接口traverse()</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">void</span> Vector&lt;T&gt;::traverse(<span class="keyword">void</span> (*visit)(T &amp;)) &#123; <span class="comment">// 借助函数指针机制</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; _size; i++) &#123;</span><br><span class="line">        visit(_elem[i]); <span class="comment">// 遍历向量</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">template</span> &lt;<span class="keyword">typename</span> VST&gt; <span class="comment">// 元素类型、操作器</span></span><br><span class="line"><span class="keyword">void</span> Vector&lt;T&gt;::traverse(VST &amp;visit) &#123; <span class="comment">// 借助函数对象机制</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; _size; i++) &#123;</span><br><span class="line">        visit(_elem[i]); <span class="comment">// 遍历向量</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  <code>traverse()</code>遍历的过程，就是<strong>自前向后</strong>逐一对各元素实施同一基本操作。而具体采用何种操作，可通过两种方式指定。前一种方式借助函数指针<code>*visit()</code>指定某一函数，该函数只有一个参数，其类型为对向量元素的引用，故通过该函数即可直接访问或修改向量元素。另外，也可以函数对象的形式，指定具体的遍历操作。这类对象的操作符”()”经重载之后，在形式上等效于一个函数接口，故此得名。<br>  相比较而言，后一形式的功能更强，适用范围更广。比如，函数对象的形式支持对向量元素的关联修改。也就是说，对各元素的修改不仅可以相互独立地进行，也可以根据某个(些)元素的数值相应地修改另一元素。前一形式也可实现这类功能，但要繁琐很多。</p><h3 id="复杂度-4"><a href="#复杂度-4" class="headerlink" title="复杂度"></a>复杂度</h3><p>  遍历操作本身只包含一层线性的循环迭代，故除了向量规模的因素外，遍历所需时间应线性正比于所统一指定的基本操作所需的时间。</p><h1 id="有序向量"><a href="#有序向量" class="headerlink" title="有序向量"></a>有序向量</h1><p>  若向量<code>S[0, n)</code>中的所有元素不仅按线性次序存放，而且其数值大小也按此次序单调分布，则称作<strong>有序向量(sorted vector)</strong>。有序向量不要求元素互异，故通常约定其中的元素自前(左)向后(右)构成一个非降序列，即对任意$0 ≤ i &lt; j &lt; n$都有$S[i] ≤ S[j]$。</p><h2 id="有序性甄别"><a href="#有序性甄别" class="headerlink" title="有序性甄别"></a>有序性甄别</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vector_disordered.h</span><br><span class="line">代码<span class="number">2.17</span> 有序向量甄别算法disordered()</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">int</span> Vector&lt;T&gt;::disordered() <span class="keyword">const</span> &#123; <span class="comment">// 返回向量中逆序相邻元素对的总数</span></span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">0</span>; <span class="comment">// 计数器</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; _size; i++) &#123; <span class="comment">// 逐一检查_size - 1对相邻元素</span></span><br><span class="line">        <span class="keyword">if</span> (_elem[i - <span class="number">1</span>] &gt; _elem[i]) n++; <span class="comment">// 逆序则计数</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> n; <span class="comment">// 向量有序当且仅当n = 0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  顺序扫描整个向量，逐一比较每一对相邻元素——向量已经有序，当且仅当它们都是顺序的。</p><h2 id="唯一化-1"><a href="#唯一化-1" class="headerlink" title="唯一化"></a>唯一化</h2><h3 id="低效版"><a href="#低效版" class="headerlink" title="低效版"></a>低效版</h3><h4 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vector_uniquify_slow.h</span><br><span class="line">代码<span class="number">2.18</span> 有序向量uniquify()接口的平凡实现</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">int</span> Vector&lt;T&gt;::uniquify() &#123; <span class="comment">// 有序向量重复元素剔除算法(低效版)</span></span><br><span class="line">    <span class="keyword">int</span> oldSize = _size; <span class="keyword">int</span> i = <span class="number">1</span>; <span class="comment">// 当前比对元素的秩，起始于首元素</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; _size) <span class="comment">// 从前向后，逐一比对各对相邻元素</span></span><br><span class="line">        _elem[i<span class="number">-1</span>] == _elem[i] ? remove(i) : i++; <span class="comment">// 若雷同，则删除后者；否则，转至后一元素</span></span><br><span class="line">    <span class="keyword">return</span> oldSize - _size; <span class="comment">// 向量规模变化量，即被删除元素总数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="正确性-1"><a href="#正确性-1" class="headerlink" title="正确性"></a>正确性</h4><p>  其正确性基于如下事实：有序向量中的重复元素必然前后紧邻。于是，可以自前向后地逐一检查各对相邻元素:若二者雷同则调用<code>remove()</code>接口删除靠后者，否则转向下一对相邻元素。如此，扫描结束后向量中将不再含有重复元素。</p><h4 id="复杂度-5"><a href="#复杂度-5" class="headerlink" title="复杂度"></a>复杂度</h4><p>  运行时间主要消耗于while循环，共需迭代<code>_size - 1 = n - 1</code>步。此外，在最坏情况下，每次循环都需要执行一次<code>remove()</code>操作，由前面可知<code>remove()</code>操作的复杂度线性正比于被删除元素的后继元素总数。因此，当大量甚至所有元素均雷同时，用于所有这些<code>remove()</code>操作的时间总量将高达：<br>  $(n - 2) + (n - 3) + … + 2 + 1 = O(n^2)$<br>  与向量未排序时相同，说明该方法未能充分利用此时向量的有序性。</p><h3 id="改进思路"><a href="#改进思路" class="headerlink" title="改进思路"></a>改进思路</h3><p>  低效版唯一化过程复杂度过高的根源是，在对<code>remove()</code>接口的各次调用中，同一元素可能作为后继元素向前移动多次，且每次仅移动一个单元。<br>  因为此时的每组重复元素都必然前后紧邻地集中分布，所以可以区间为单位成批地删除前后紧邻的各组重复元素，并将其后继元素(若存在)统一地大跨度前移。具体地，若<code>V[lo, hi)</code>为一组相邻的重复元素，则所有的后继元素<code>V[hi, _size)</code>可统一地整体前移hi - lo - 1个单元。</p><h3 id="高效版"><a href="#高效版" class="headerlink" title="高效版"></a>高效版</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vector_uniquify.h</span><br><span class="line">代码<span class="number">2.19</span> 有序向量uniquify()接口的高效实现</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">int</span> Vector&lt;T&gt;::uniquify() &#123; <span class="comment">// 有序向量重复元素剔除算法(高效版)</span></span><br><span class="line">    Rank i = <span class="number">0</span>, j = <span class="number">0</span>; <span class="comment">// 各对互异"相邻"元素的秩</span></span><br><span class="line">    <span class="keyword">while</span> (++j &lt; _size) <span class="comment">// 逐一扫描，直至末元素</span></span><br><span class="line">        <span class="keyword">if</span> (_elem[i] != _elem[j]) <span class="comment">// 跳过雷同者</span></span><br><span class="line">            _elem[++i] = _elem[j]; <span class="comment">// 发现不同元素时，向前移至紧邻于前者后侧</span></span><br><span class="line">    _size = ++i; shrink(); <span class="comment">// 直接截除尾部多余元素</span></span><br><span class="line">    <span class="keyword">return</span> j - i; <span class="comment">// 向量规模变化量，即被删除元素总数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  既然各组重复元素必然彼此相邻地构成一个子区间，故只需依次保留各区间的起始元素。于是，这里引入了变量i和j。每经过若干次移动，i和j都将分别指向下一对相邻子区间的首元素；在将后者转移至前者的后继位置之后(相比低效版这里是元素的复制而不是前移，因此不需要<code>remove()</code>操作)，即可重复上述过程。</p><h3 id="复杂度-6"><a href="#复杂度-6" class="headerlink" title="复杂度"></a>复杂度</h3><p>  while循环的每一次迭代，仅需对元素数值做一次比较，向后移动一到两个位置指针，并至多向前复制一个元素，故只需常数时间。而在整个算法过程中，每经过一次迭代秩j都必须加1，鉴于j不能超过向量的规模n，故共需迭代n次。由此可知，<code>uniquify()</code>算法的时间复杂度应为$O(n)$，较之<code>uniquify_slow()</code>的$O(n^2)$,整整提高了一个线性因子。<br>  反过来，在遍历所有元素之前不可能确定是否有重复元素，故就渐进复杂度而言，能在$O(n)$时间内完成向量的唯一化已属最优。能做到这一点的关键在于向量已经排序。</p><h2 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h2><h2 id="二分查找-版本A"><a href="#二分查找-版本A" class="headerlink" title="二分查找(版本A)"></a>二分查找(版本A)</h2><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vector_search_binary_a.h</span><br><span class="line">代码 <span class="number">2.21</span> 二分查找算法(版本A)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二分查找算法(版本A):在有序向量的区间[lo,hi)内查找元素e, 0 &lt;= lo &lt;= hi &lt;= _size</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="function"><span class="keyword">static</span> Rank <span class="title">binSearch</span><span class="params">(T* A, T <span class="keyword">const</span>&amp; e, Rank lo, Rank hi)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (lo &lt; hi)&#123; <span class="comment">// 每步迭代可能要做两次比较判断，有三个分支</span></span><br><span class="line">        Rank mi = (lo + hi) &gt;&gt; <span class="number">1</span>; <span class="comment">// 以中点为轴点</span></span><br><span class="line">        <span class="keyword">if</span> (e &lt; A[mi]) hi = mi; <span class="comment">// 深入前半段[lo, mi)继续查找</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (A[mi] &lt; e) lo = mi + <span class="number">1</span>; <span class="comment">// 深入后半段(mi, hi)继续查找</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> mi; <span class="comment">// 在mi处命中</span></span><br><span class="line">    &#125; <span class="comment">// 成功查找可以提前终止</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// 查找失败</span></span><br><span class="line">&#125; <span class="comment">// 有多个命中元素时，不能保证返回秩最大者；查找失败时，简单返回-1，而不能指示失败的位置</span></span><br></pre></td></tr></table></figure><p>  为在有序向量区间[lo, hi)内查找元素e，该算法以中点$mi = (lo + hi)/2$为界，将其大致平均地分为前、后两个子向量。随后通过一至两次比较操作，确定问题转化的方向。通过快捷的整数移位操作回避了相对更加耗时的除法运算。另外，通过引入lo、hi和mi等变量，将减治算法通常的递归模式改成了迭代模式。</p><h3 id="复杂度-7"><a href="#复杂度-7" class="headerlink" title="复杂度"></a>复杂度</h3><p>  以上算法采取的策略可概括为，以”当前区间内居中的元素”作为目标元素的试探对象。从应对最坏情况的保守角度来看，这一策略是最优的——每一步迭代之后无论沿着哪个方向深入，新问题的规模都将缩小一半。因此，这一策略也称作<strong>二分查找(binary search)。</strong><br>  随着迭代的不断深入，有效的查找区间宽度将按1/2的比例以几何级数的速度递减。于是，经过至多$log_2(hi -lo)$步迭代后，算法必然终止。鉴于每步迭代仅需常数时间，故总体时间复杂度不超过:$O(log_2(hi - lo)) = O(logn)$。<br>  与之前的顺序查找算法$O(n)$复杂度相比，$O(logn)$几乎改进了一个线性因子。</p><h3 id="查找长度"><a href="#查找长度" class="headerlink" title="查找长度"></a>查找长度</h3><p>  以上迭代过程所涉及的计算，主要分为两类：元素的大小比较、秩的算术运算及其赋值。虽然二者均属于$O(1)$复杂度的基本操作，但元素的秩无非是(无符号)的整数，而向量元素的类型则通常更为复杂，甚至复杂到未必能够保证在常数时间内完成(习题【2-17】)。因此就时间复杂度的常系数而言，前一类计算的权重远远高于后者，而查找算法的整体效率也更主要地取决于所执行的元素大小比较操作的次数，即所谓<strong>查找长度(search length)</strong>。</p><h4 id="成功查找长度"><a href="#成功查找长度" class="headerlink" title="成功查找长度"></a>成功查找长度</h4><p>  对于长度为n的有序向量，共有n种可能的成功查找，分别对应于某一元素。实际上，每一种成功查找所对应的查找长度，仅取决于n以及目标元素所对应的秩，而与元素的具体数值无关。<br>  为了估计出一半情况下的成功查找长度，不失一般性地，仍在等概率条件下考察长度为$n = 2^k - 1$的有序向量，并将其对应的平均成功查找长度记作$C_{average}(k)$，将所有元素对应的查找长度总和记作$C(k) = C_{average}(k) \cdot (2^k - 1)$。<br>  特别地，当k = 1时向量长度n = 1，成功查找仅有一种情况，故有边界条件:<br>  $C_{average}(1) = C(1) = 2​$<br>  以下采用递推分析法。对于长度为$n = 2^k - 1$的有序向量，每步迭代都有三种可能的分支:经过1次成功的比较后，转化为一个规模为$2^{k-1}-1$的新问题；经2次失败的比较后，终止于向量中的某一元素，并确认在此处成功命中；经1次失败的比较另加1次成功的比较后，转化为另一个规模为$2^{k-1} -1​$的新问题。<br>  根据以上递推分析的结论，可得递推式如下:<br>$$<br>\begin{aligned}<br>C(k)&amp; = [C(k-1) + (2^{k-1} -1 )] + 2 + [C(k-1) + 2 \times (2^{k-1} -1 )] \\<br>&amp; = 2 \cdot C(k-1) + 3 \cdot 2^{k-1} - 1<br>\end{aligned}<br>$$<br>  若令：<br>  $F(k) = C(k) - 3k \cdot 2^{k-1} - 1$<br>  则有:<br>  $F(1) = -2​$<br>$$<br>\begin{aligned}<br>  F(k) &amp; = 2 \cdot F(k - 1) = 2^2 \cdot F(k - 2) = 2^3 \cdot F(k - 3) = … \\<br>  &amp; = 2^{k-1} \cdot F(1) = -2^k<br>  \end{aligned}<br>$$<br>  于是：<br>$$<br>\begin{aligned}<br>  C(k) &amp; = F(k) + 3k \times 2^{k-1} + 1 \\<br>  &amp; = -2^k + 3k \times 2^{k-1} + 1 \\<br>  &amp; = (3k/2 -1) \cdot (2^k - 1) + 3k/2<br>  \end{aligned}<br>$$<br>  进而：<br>$$<br>  \begin{aligned}<br>  C_{average}(k) &amp; = C(k) / (2^k - 1) \\<br>  &amp; = 3k/2 - 1 + 3k/2/(2^k - 1) \\<br>  &amp; = 3k/2 - 1 + O(\varepsilon) \\<br>  \end{aligned}<br>$$<br>  也就是说，若忽略末尾趋于收敛的波动项，平均查找长度应为：<br>  $O(1.5k) = O(1.5 \cdot log_2n)​$</p><h4 id="失败查找长度"><a href="#失败查找长度" class="headerlink" title="失败查找长度"></a>失败查找长度</h4><p>  按照上述代码，失败查找的终止条件必然是”lo ≥ hi”，也就是说，只有在有效区间宽度缩减至0时，查找方以失败告终。因此，失败查找的时间复杂度应为确定的$Θ(logn)$。<br>  仿照以上对平均成功查找长度的递推分析方法，不难证明(习题【2-20】)，一般情况下的平均失败查找长度亦为$O(1.5 \cdot log_2n)$。</p><h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><p>  尽管二分查找算法(版本A)即便在最坏情况下也可保证$O(logn)$的渐进时间复杂度，但就其常系数1.5而言仍有改进余地。下面介绍Fibonacci查找将完成这改进。</p><h2 id="Fibonacci查找"><a href="#Fibonacci查找" class="headerlink" title="Fibonacci查找"></a>Fibonacci查找</h2><h3 id="递推方程"><a href="#递推方程" class="headerlink" title="递推方程"></a>递推方程</h3><p>  递推方程法既是复杂度分析的重要方法，也是优化算法时确定突破口的有力武器。<br>  最终求解所得到的平均复杂度，主要取决于$(2_{k-1} - 1)$和$2 \times (2_{k-1} - 1)$两项，其中的$(2_{k-1} - 1)$为子向量的宽度，而系数1和2则是算法为深入前、后子向量，所需做的比较操作次数。以此前的二分查找算法版本A为例，之所以存在均衡性方面的缺陷，根源来自于这两项的大小不相匹配。<br>  基于这一理解，发现解决问题的思路不外乎两种：<br>  <em>其一，调整前、后区域的宽度，适当地加长(缩短)前(后)子向量；</em><br>  <em>其二，统一沿两个方向深入所需要执行的比较次数，比如都统一为一次；</em></p><h3 id="黄金分割"><a href="#黄金分割" class="headerlink" title="黄金分割"></a>黄金分割</h3><p>  简化起见，设向量长度n = fib(k) - 1<br>  <code>fibsearch(e, 0, n)</code>查找可以mi = fib(k - 1) - 1作为前、后子向量的切分点。如此，前、后子向量的长度将分别是：<br>  $fib(k-1) - 1$<br>  $fib(k-2) - 1 = (fib(k) - 1) - (fib(k - 1) - 1) - 1$<br>  于是，无论朝哪个方向深入，新向量的长度从形式上都依然是某个Fibonacci数减一，故这一处理手法可以反复套用，直至因在S[mi]处命中或向量长度收缩至零而终止。这种查找算法，亦称作<strong>Fibonacci查找(Fibonaccian search)</strong>。</p><h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vector_search_fibonaccin.h</span><br><span class="line">代码 <span class="number">2.22</span> Fibonacci查找算法</span><br><span class="line"></span><br><span class="line"><span class="comment">// Fibonacci查找算法(版本A):在有序向量的区间[lo, hi)内查找元素e, 0 &lt;= lo &lt;= hi &lt;= _size</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="function"><span class="keyword">static</span> Rank <span class="title">fibSearch</span><span class="params">(T* A, T <span class="keyword">const</span>&amp; e, Rank lo, Rank hi)</span></span>&#123;</span><br><span class="line">    <span class="function">Fib <span class="title">fib</span><span class="params">(hi - lo)</span></span>; <span class="comment">// 用O(log_phi(n = hi - lo)时间创建Fib数列</span></span><br><span class="line">    <span class="keyword">while</span> (lo &lt; hi)&#123; <span class="comment">// 每步迭代可能要做两次比较判断，有三个分支</span></span><br><span class="line">        <span class="keyword">while</span> (hi - lo &lt; fib.get()) fib.prev(); <span class="comment">// 通过向前顺序查找(分摊O(1))——至多迭代几次</span></span><br><span class="line">        Rank mi = lo + fib.get() - <span class="number">1</span>; <span class="comment">// 确定形如Fib(k) - 1 的轴点</span></span><br><span class="line">        <span class="keyword">if</span> (e &lt; A[mi]) hi = mi; <span class="comment">// 深入前半段[lo, mi)继续查找</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (A[mi] &lt; e) lo = mi + <span class="number">1</span>; <span class="comment">// 深入后半段(mi, hi)继续查找</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> mi; <span class="comment">// 在mi处命中</span></span><br><span class="line">    &#125;<span class="comment">// 成功查找可以提前终止</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// 查找失败</span></span><br><span class="line">&#125; <span class="comment">// 有多个命中元素时，不能保证返回秩最大者；失败时，简单地返回-1，而不能指示失败的位置</span></span><br></pre></td></tr></table></figure><p>  算法主体框架与二分查找大致相同，主要区别在于以黄金分割点取代中点作为切分点。为此，需要借助Fib对象(习题【1-22】)，实现对Fibonacci数的高效设置与获取。<br>  尽管以下的分析多以长度为fib(k) - 1的向量为例，但这一实现完全可适用于长度任意的向量中的任意子向量。为此，只需在进入循环之前调用构造器Fib(n = hi - lo)，将初始长度设置为”不小于n的最小Fibonacci项”。这一步所需花费的$O(log_{\phi}n)$时间，分摊到后续的$O(log_{\phi}n)$步迭代中，并不影响算法整体的渐进复杂度。</p><h3 id="定量分析"><a href="#定量分析" class="headerlink" title="定量分析"></a>定量分析</h3><p>  Fibonacci查找算法最好、最坏情况的成功查找长度与二分算法的结论完全一致。<br>  依然将长度为n = fib(k) - 1的有序向量的平均成功查找长度记作$C_{average}(k)$，将所有元素对应的查找长度总和记作$C(k) = C_{average}(k) \cdot (fib(k) - 1)$。<br>  同理，可得边界条件及递推式如下:<br>  $C_{average}(2) = C(2) = 0$<br>  $C_{average}(3) = C(3) = 2$<br>$$<br>\begin{aligned}<br>  C(k) &amp; = [C(k-1) + (fib(k-1) - 1)] + 2 + [C(k-2) + 2 \times (fib(k-2) - 1)] \\<br>  &amp; = C(k - 2) + C(k - 1) + fib(k - 2) + fib(k) - 1<br>  \end{aligned}<br>$$<br>  结合以上边界条件，可以解得:<br>  (令$F(k) = -C(k) + k \cdot fib(k) + 1$，则有$F(0) = 1$，$F(1) = 2$，$F(k) = F(k-1) + F(k-2)$)<br>$$<br>\begin{aligned}<br>  C(k) &amp; = k \cdot fib(k) - fib(k + 2) + 1 \\<br>  &amp; = (k - \phi^2) \cdot fib(k) + 1 + O(\mathcal{E})<br>  \end{aligned}<br>$$<br>其中，$\phi = (\sqrt{5} + 1) / 2 = 1.618$<br>  于是<br>$$<br>\begin{aligned}<br>  C(k) &amp; = C(k) / (fib(k) - 1)\\<br>  &amp; = k - \phi^2 + 1 + (k - \phi^2) / (fib(k) - 1) + O(\mathcal{E}) \\<br>  &amp; = k - \phi^2 + 1 + O(\mathcal{E}) \\<br>  \end{aligned}<br>$$<br>  忽略末尾趋于收敛的波动项，平均查找长度的增长趋势为:<br>  $O(k) = O(log_{\phi}n) = O(log_{\phi}2 \cdot log_2n) = O(1.44 \cdot log_2n)$<br>  较之之前二分查找算法(版本A)的$O(1.5 \cdot log2n)$，效率略有提高。</p><h2 id="二分查找-版本B"><a href="#二分查找-版本B" class="headerlink" title="二分查找(版本B)"></a>二分查找(版本B)</h2><h3 id="从三分支对应两分支"><a href="#从三分支对应两分支" class="headerlink" title="从三分支对应两分支"></a>从三分支对应两分支</h3><p>  为了解决二分查找算法版本A的不均衡性，Fibonacci查找算法已通过采用黄金分割点，在一定程度上降低了时间复杂度的常系数。<br>  还有另一更为直接的方法，即令以上两项的常系数同时等于1。也就是说，无论朝哪个方向深入，都只需做1次元素的大小比较。相应地，算法在每步迭代中(或递归层次上)都只有两个分支方向，而不再是三个。<br>  具体过程与二分查找算法的版本A基本类似。不同之处是，在每个切分点<code>A[mi]</code>处，仅做一次元素比较。具体地，若目标元素小于<code>A[mi]</code>，则深入前端子向量<code>A[lo, mi]</code>继续查找；否则，深入后端子向量<code>A[mi, hi)</code>继续查找。</p><h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vector_search_binary_b.h</span><br><span class="line">代码 <span class="number">2.23</span> 二分查找算法(版本B)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二分查找算法(版本B)：在有序向量的区间[lo, hi)内查找元素e, 0 &lt;= lo &lt;= hi &lt;= _size</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="function"><span class="keyword">static</span> Rank <span class="title">binSearch</span><span class="params">(T* A, T <span class="keyword">const</span>&amp; e, Rank lo, Rank hi)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> ( <span class="number">1</span> &lt; hi - lo)&#123; <span class="comment">// 每步迭代仅需做一次比较，有两个分支；成功查找不能提前终止</span></span><br><span class="line">        Rank mi = (lo + hi) &gt;&gt; <span class="number">1</span>; <span class="comment">// 以中点为轴点</span></span><br><span class="line">        (e &lt; A[mi]) ? hi = mi : lo = mi; <span class="comment">// 经比较后确定深入[lo, mi)或[mi, hi)</span></span><br><span class="line">    &#125; <span class="comment">// 出口时hi = lo + 1, 查找区间仅含一个元素A[lo]</span></span><br><span class="line">    <span class="keyword">return</span> (e == A[lo]) ? lo : <span class="number">-1</span>; <span class="comment">// 查找成功时返回对应的秩；否则统一返回-1</span></span><br><span class="line">&#125;<span class="comment">// 有多个命中元素时，不能保证返回秩最大者；查找失败时，简单地返回-1，而不能指示失败的位置</span></span><br></pre></td></tr></table></figure><p>  注意与二分查找版本A的差异。首先，每一步迭代只需判断是否e &lt; A[mi]，即可相应地更新有效查找区间的右边界(hi = mi)或左边界(lo = mi)。另外，只有等到区间的宽度已不足2个单元时迭代才会终止，最后再通过一次比对判断查找是否成功。</p><h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><p>  尽管版本B中的后端子向量需要加入<code>A[mi]</code>，但得益于mi总是位于中央位置，整个算法$O(logn)$的渐进复杂度不受任何影响。<br>  版本B中只有在向量有效区间宽度缩短至1个单元时算法才会终止，而不能像版本A一旦命中就能及时返回。因此，最好情况下的效率有所倒退。作为补偿，最坏情况下的效率相应地有所提高。实际上无论是成功查找或失败查找，版本B各分支的查找长度更加接近，故整体性能更趋稳定。</p><h2 id="二分查找-版本C"><a href="#二分查找-版本C" class="headerlink" title="二分查找(版本C)"></a>二分查找(版本C)</h2><h3 id="实现-8"><a href="#实现-8" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vector_search_binary_c.h</span><br><span class="line">代码 <span class="number">2.24</span> 二分查找算法(版本C)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二分查找算法(版本C)：在有序向量的区间[lo, hi)内查找元素e, 0 &lt;= lo &lt;= hi &lt;= _size</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="function"><span class="keyword">static</span> Rank <span class="title">binSearch</span><span class="params">(T* A, T <span class="keyword">const</span>&amp; e, Rank lo, Rank hi)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (lo &lt; hi)&#123; <span class="comment">// 每步迭代仅需做一次比较判断，有两个分支</span></span><br><span class="line">        Rank mi = ( lo + hi ) &gt;&gt; <span class="number">1</span>; <span class="comment">// 以中点为轴点</span></span><br><span class="line">        ( e &lt; A[mi] ) ? hi = mi : lo = mi + <span class="number">1</span>; <span class="comment">// 经比较后确定深入[lo, mi)或(mi, hi)</span></span><br><span class="line">    &#125;<span class="comment">// 成功查找不能提前终止</span></span><br><span class="line">    <span class="keyword">return</span> --lo; <span class="comment">// 循环结束时，lo为大于e的元素的最小秩，故lo - 1即不大于e的元素的最大秩</span></span><br><span class="line">&#125; <span class="comment">// 有多个命中元素时，总能保证返回秩最大者；查找失败时，能够返回失败位置</span></span><br></pre></td></tr></table></figure><p>  该版本的主体结构与版本B一致，故不难理解，二者的时间复杂度相同。</p><h3 id="正确性-2"><a href="#正确性-2" class="headerlink" title="正确性"></a>正确性</h3><p>  版本C与版本B的差异，主要有三点。首先，只有当有效区间的宽度缩短至0(而不是1)时，查找方告终止。另外，在每次转入后端分支时，子向量的左边界取作mi + 1而不是mi。<br>  通过数学归纳可以证明，版本C中的循环体，具有如下不变性:</p><p>  <em>A[0, lo)中的元素皆不大于e；A[hi, n)中的元素皆大于e</em></p><p>  首先迭代时，lo = 0且hi = n，A[hi, n)均空，不变性自然成立。<br>  设在某次进入循环时以上不变性成立，以下无非两种情况。若e &lt; A[mi]，在令hi = mi并使A[hi, n)向左扩展之后，该区间内的元素皆不小于A[mi]，也仍然大于e。反之，若A[mi] ≤ e，在令lo = mi + 1并使A[0, lo)向右拓展之后，该区间内的元素皆不大于A[mi]，也仍然不大于e。上述不变性得以延续。<br>  循环终止时，lo = hi。考察此时的元素A[lo - 1]和A[lo]:作为A[lo, n) = A[hi, n)内的第一个元素，A[lo]必大于e。也就是说A[lo - 1]即是原向量中不大于e的最后一个元素。因此在循环结束后，无论成功与否，只需返回lo - 1即可——这也是版本C与版本B的第三点差异。</p><h1 id="排序与下界"><a href="#排序与下界" class="headerlink" title="排序与下界"></a>排序与下界</h1><h2 id="有序性"><a href="#有序性" class="headerlink" title="有序性"></a>有序性</h2><p>有序性在很多场合都能极大地提高计算的效率。</p><h2 id="排序及其分类"><a href="#排序及其分类" class="headerlink" title="排序及其分类"></a>排序及其分类</h2><p>在解决许多应用问题时一种普遍采用的策略是，首先将向量转换为有序向量，再调用有序向量支持的各种高效算法。</p><h3 id="排序算法分类"><a href="#排序算法分类" class="headerlink" title="排序算法分类"></a>排序算法分类</h3><p>排序算法有多种，可从多个角度对其进行分类。</p><ol><li>根据其处理数据的规模与存储的特点不同，可分为：</li></ol><ul><li>内部排序算法</li><li>外部排序算法<br>前者处理的数据规模相对不大，内存足以容纳；后者处理的数据规模很大，必须借助外部甚至分布式存储器，在排序计算过程的任一时刻，内存中只能容纳其中一小部分数据。</li></ul><ol start="2"><li>根据输入形式的不同，可分为：</li></ol><ul><li>离线算法(offline algorithm)</li><li>在线算法(online algorithm)<br>前一情况下，待排序的数据以批处理的形式整体给出；而在网络计算之类的环境中，待排序的数据通常需要实时生成，在排序算法启动后数据才陆续到达。</li></ul><ol start="3"><li>针对所依赖的体系结构不同，又可分为：</li></ol><ul><li>串行排序算法</li><li>并行排序算法</li></ul><ol start="4"><li>根据排序算法是否采用随机策略，可分为：</li></ol><ul><li>确定式</li><li>随机式</li></ul><p>本书讨论的范围，主要集中于确定式串行脱机的内部排序算法。</p><h2 id="下界"><a href="#下界" class="headerlink" title="下界"></a>下界</h2><p>  一般地，任一问题在最坏情况下的最低计算成本，即为该问题的<strong>复杂度下界(lower bound)</strong>。一旦某一算法的性能达到这一下界，即意味着它已是<strong>最坏情况下最优的(worst-case optimal)</strong>。<br>  以下结合比较树模型，介绍界定问题复杂度下界的一种重要方法。</p><h2 id="比较树"><a href="#比较树" class="headerlink" title="比较树"></a>比较树</h2><h3 id="基于比较的分支"><a href="#基于比较的分支" class="headerlink" title="基于比较的分支"></a>基于比较的分支</h3><p>  用节点(圆圈)表示算法过程中的不同状态，用有向边表示不同状态之间的相互转换，就可以将算法转化为树形结构。<br>  这一转化方法也可以推广并应用于其他算法。一般地树根结点对应算法入口处的起始状态；内部节点对应过程中的某步计算，通常属于基本操作；叶节点则对应经一系列计算后某次运行的终止状态。如此借助这一树形结构，可以涵盖对应算法所有可能的执行流程。</p><h3 id="比较树-1"><a href="#比较树-1" class="headerlink" title="比较树"></a>比较树</h3><p>  算法所有可能的执行过程，都可涵盖于这一树形结构中。具体地，该树具有以下性质：</p><ol><li>每一内部节点各对应于一次比对(称量)操作；</li><li>内部节点的左、右分支，分别对应于在两种比对结果(是否等重)下的执行方向；</li><li>叶节点(或等效地，根到叶节点的路径)对应于算法某次执行的完整过程及输出；</li><li>反过来，算法的每一运行过程都对应于从根到某一叶节点的路径。</li></ol><p>按上述规则与算法相对应的树，称作<strong>比较树(comparison tree)</strong>。<br>无论什么算法，只要其中的分支完全取决于不同变量或常量的比对或比较结果，则该算法所有可能的执行过程都可表示和概括为一棵比较树。反之，凡可如此描述的算法，都可称作基于<strong>比较式算法(comparison-based algorithm)</strong>，简称<strong>CBA式算法</strong>。<br>  CBA式算法在最坏情况下的最低执行成本，可由对应的比较树界定。</p><h2 id="估计下界"><a href="#估计下界" class="headerlink" title="估计下界"></a>估计下界</h2><h3 id="最小树高"><a href="#最小树高" class="headerlink" title="最小树高"></a>最小树高</h3><p>  考察任一CBA式算法，设CT(A)为与之对应的一棵比较树。<br>  根据比较树的性质，算法A每一次运行所需的时间，将取决于对应叶节点到根节点的距离(称作叶节点的深度)；而算法A在最坏情况下的运行时间，将取决于比较树中所有叶节点的最大深度(称作该树的高度，记作$h(CT(A))$)。因此就渐进的意义而言，算法A的时间复杂度应不低于$\Omega(h(CT(A)))$。<br>  如何估计这些比较树的最小高度？<br>  为此，只需考察书中所含叶节点(可能的输出结果)的数目。具体地，在一棵高度为h的二叉树中，叶节点的数目不可能多余$2^h$。因此反过来，若某一问题的输出结果不少于N种，则比较树中叶节点也不可能少于N个，树高h不可能低于$log_2N$(习题【7-3】)</p><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><p>  任一CBA式排序算法所对应比较树的高度应为:<br>  $h ≥ \lceil log_3(n!) \rceil = \lceil log_3e \cdot ln(n!) \rceil = \Omega(nlogn)$<br>  因此最坏情况下CBA排序算法至少需要$\Omega(nlogn)$时间，其中n为待排序元素数目。<br>  需要强调的是，这一$\Omega(nlogn)$下界是针对比较树模型而言的。事实上还有很多不属此类的排序算法，并且其中一些算法在最坏情况下的运行时间，有可能低于这一下界，但与上述结论并不矛盾。</p><h1 id="排序器"><a href="#排序器" class="headerlink" title="排序器"></a>排序器</h1><h2 id="起泡排序"><a href="#起泡排序" class="headerlink" title="起泡排序"></a>起泡排序</h2><h3 id="起泡排序-1"><a href="#起泡排序-1" class="headerlink" title="起泡排序"></a>起泡排序</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vector_bubblesort.h</span><br><span class="line">代码 <span class="number">2.26</span> 向量的起泡排序</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="comment">// 向量的起泡排序</span></span><br><span class="line"><span class="keyword">void</span> Vector&lt;T&gt;::bubbleSort(Rank lo, Rank hi) &#123; <span class="comment">// assert: 0 &lt;= lo &lt; hi &lt;= _size</span></span><br><span class="line">    <span class="keyword">while</span> (!bubble(lo, hi--)); <span class="comment">// 逐趟做扫描交换，直至全序</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  反复调用单趟扫描交换算法，直至逆序现象完全消除。</p><h3 id="扫描交换"><a href="#扫描交换" class="headerlink" title="扫描交换"></a>扫描交换</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vector_bubble.h</span><br><span class="line">代码 <span class="number">2.27</span> 单趟扫描交换</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="keyword">bool</span> Vector&lt;T&gt;::bubble(Rank lo, Rank hi) &#123; <span class="comment">// 一趟扫描交换</span></span><br><span class="line">    <span class="keyword">bool</span> sorted = <span class="literal">true</span>; <span class="comment">// 整体有序标志</span></span><br><span class="line">    <span class="keyword">while</span> (++lo &lt; hi) <span class="comment">// 自左向右，逐一检查各对相邻元素</span></span><br><span class="line">        <span class="keyword">if</span> (_elem[lo - <span class="number">1</span>] &gt; _elem[lo])&#123; <span class="comment">// 若逆序，则</span></span><br><span class="line">            sorted = <span class="literal">false</span>; <span class="comment">// 意味着尚未整体有序，并需要</span></span><br><span class="line">            swap (_elem[lo - <span class="number">1</span>], _elem[lo]); <span class="comment">// 通过交换使局部有序</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">return</span> sorted; <span class="comment">// 返回有序标志</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  依次比对各对相邻元素，每当发现逆序即令二者彼此交换；一旦经过某趟扫描后未发现任何逆序的相邻元素，即意味着排序任务已经完成，则通过返回标志”sorted”，以便主算法及时终止。</p><h3 id="重复元素与稳定性"><a href="#重复元素与稳定性" class="headerlink" title="重复元素与稳定性"></a>重复元素与稳定性</h3><p>  <strong>稳定性(stability)</strong>是对排序算法更为细致的要求，重在考察算法对重复元素的处理效果。具体地，在将向量A转换为有序向量S之后，设$A[i]$对应于$S[k_i]$。若对于A中每一对重复元素$A[i] = A[j]$(相应地$S[k_i] = S[k_j]$)，都有i &lt; j当且仅当$k_i &lt; k_j$，则称该排序算法是<strong>稳定算法(stable algorithm)</strong>。简而言之，稳定算法的特征是，重复元素之间的相对次序在排序前后保持一致。反之，不具有这一特征的排序算法都是<strong>不稳定算法(unstable algorithm)</strong>。</p><h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><h3 id="有序向量的两路归并"><a href="#有序向量的两路归并" class="headerlink" title="有序向量的两路归并"></a>有序向量的两路归并</h3><p>  <strong>二路归并</strong>，就是将两个有序序列合并成一个有序序列。归并排序所需的时间，也主要决定于各趟二路归并所需时间的总和。<br>  二路归并属于迭代式算法。每步迭代中，只需比较两个待归并向量的首元素，将小者取出并追加到输出向量的末尾，该元素在原向量中的后继则成为新的首元素。如此往复，直到，某一向量为空。最后，将另一非空的向量整体接至输出向量的末尾。</p><h3 id="分治策略"><a href="#分治策略" class="headerlink" title="分治策略"></a>分治策略</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vector_mergesort.h</span><br><span class="line">代码 <span class="number">2.28</span> 向量的归并排序</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="comment">// 向量归并排序</span></span><br><span class="line"><span class="keyword">void</span> Vector&lt;T&gt;::mergeSort(Rank lo, Rank hi) &#123; <span class="comment">// 0 &lt;= lo &lt; hi &lt;= _size</span></span><br><span class="line">    <span class="keyword">if</span> (hi - lo &lt; <span class="number">2</span>) <span class="keyword">return</span>; <span class="comment">// 单元素区间自然有序，否则...</span></span><br><span class="line">    <span class="keyword">int</span> mi = ( lo + hi ) / <span class="number">2</span>; <span class="comment">// 以中点为界</span></span><br><span class="line">    mergeSort(lo, mi); mergeSort(mi, hi); <span class="comment">// 分别排序</span></span><br><span class="line">    merge(lo, mi, hi); <span class="comment">// 归并排序</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  均匀地将向量<code>S[lo, hi)</code>划分成两个子向量。借助以上的二路归并算法，通过递归调用将二者分别转换为有序向量，得到与原向量S对应的整个有序向量。<br>  这里递归终止条件是当前向量长度：$n = hi -lo = 1$<br>  仅含单个元素的向量必然有序，这一处理分支自然也就可以作为递归基。</p><h3 id="二路归并接口的实现"><a href="#二路归并接口的实现" class="headerlink" title="二路归并接口的实现"></a>二路归并接口的实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vector_merge.h</span><br><span class="line">代码 <span class="number">2.29</span> 有序向量的二路归并</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="comment">// 有序向量的归并</span></span><br><span class="line"><span class="keyword">void</span> Vector&lt;T&gt;::merge(Rank lo, Rank mi, Rank hi) &#123; <span class="comment">// 各自有序的子向量[lo, mi)和[mi, hi)</span></span><br><span class="line">    T* A = _elem + lo; <span class="comment">// 合并后的向量A[0, hi - lo) = _elem[lo, hi)</span></span><br><span class="line">    <span class="keyword">int</span> lb = mi - lo; T* B = <span class="keyword">new</span> T[lb]; <span class="comment">// 前子向量B[0, lb) = _elem[lo, mi)</span></span><br><span class="line">    <span class="keyword">for</span> ( Rank i = <span class="number">0</span>; i &lt; lb; B[i] = A[i++]); <span class="comment">// 复制前子向量</span></span><br><span class="line">    <span class="keyword">int</span> lc = hi - mi; T* C = _elem + mi; <span class="comment">// 后子向量C[0, lc) = _elem[mi, hi)</span></span><br><span class="line">    <span class="keyword">for</span> (Rank i = <span class="number">0</span>, j = <span class="number">0</span>, k = <span class="number">0</span>; (j &lt; lb) || (k &lt; lc);)&#123; <span class="comment">// B[j]和C[k]中的小者续至A末尾</span></span><br><span class="line">        <span class="keyword">if</span> ((j &lt; lb) &amp;&amp; (!(k &lt; lc) || (B[j] &lt;= C[k]))) A[i++] = B[j++];</span><br><span class="line">        <span class="keyword">if</span> ((k &lt; lc) &amp;&amp; (!(j &lt; lb) || (C[k] &lt; B[j]))) A[i++] = C[k++];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> [] B; <span class="comment">//释放临时空间B</span></span><br><span class="line">&#125; <span class="comment">// 归并后得到完整的有序向量</span></span><br></pre></td></tr></table></figure><p>  约定参与归并的子向量在原向量中总是前、后相邻的，故借助三个入口参数即可界定其范围<code>[lo, mi)</code>和<code>[mi, hi)</code>。另外，为保证归并向量所得的子向量能够原地保存以便继续参与更高层的归并，这里使用了临时数组<code>B[]</code>存放前一向量<code>[lo, mi)</code>的副本(习题【2-28】)。</p><h3 id="归并时间"><a href="#归并时间" class="headerlink" title="归并时间"></a>归并时间</h3><p>  二路归并算法<code>merge()</code>的渐进时间成本，取决于其中循环迭代的总次数。<br>  每经过一次迭代，<code>B[i]</code>和<code>C[k]</code>之间的小者都会被移出并接至A的末尾(习题【2-29】和习题【2-30】)。这意味，每经过一次迭代，总和s = j + k都会加一。<br>  考察这一总和s在迭代过程中的变化。初始时，有s = 0 + 0 = 0;而在迭代期间，始终有:<br>  $s &lt; lb + lc = (mi - lo) + (hi - mi) = hi - lo$<br>因此，迭代次数及所需时间均不超过$O(hi - mi) = O(n)$。<br>  反之，按照算法的流程控制逻辑，无论子向量的内部元素组成及其相对大小如何，只有待到s = hi - lo时迭代方能终止。因此，该算法在最好情况下仍需$\Omega(n)$时间，概括而言应为$\Theta(n)$。</p><h3 id="推广"><a href="#推广" class="headerlink" title="推广"></a>推广</h3><p>  二路归并只需线性时间的结论，并不限于相邻且等长的子向量。实际上，即便子向量在物理地址空间上并非前后衔接，且长度相差悬殊，该算法也依然可行且仅需线性时间。<br>  更重要地，这一算法框架也可应用于列表——而且同样可以达到线性的时间效率。</p><h3 id="排序时间"><a href="#排序时间" class="headerlink" title="排序时间"></a>排序时间</h3><p>  归并排序算法的时间复杂度采用递推方程分析法，为此首先将归并排序算法处理长度为n的向量所需时间记作$T(n)$。根据算法构思与流程，为对长度为n的向量归并排序，需递归地对长度各为n/2的两个子向量做归并排序，再花费线性时间做一次二路归并。如此，可得到如下递推关系：<br>  $T(n) = 2 \times T(n/2) + O(n)$<br>  另外，当子向量长度缩短到1时，递归即可终止并直接返回该向量。故有边界条件<br>  $T(1) = O(1)$<br>  联立以上递推式，可以解得(习题【2-26】)<br>  $T(n) = O(nlogn)$<br>  也就是说，归并算法可在$O(nlogn)$时间内对长度为n的向量完成排序。因二路归并算法的效率稳定在$\Theta(n)$，故更准确地讲，归并排序算法的时间复杂度应为$\Theta(nlogn)$。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  本章的讲解围绕向量结构的高效实现而逐步展开，包括其作为抽象数据类型的接口规范以及对应的算法，尤其是高效维护动态向量的技巧。此外，还针对有序向量，系统介绍经典的查找与排序算法，并就其性能做一分析对比，这也是本章的难点与重点所在。最后，还引入复杂度下界的概念，并通过建立比较树模型，针对基于比较式算法给出复杂度下界的统一界定方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="数据结构" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="C++" scheme="https://rilzob.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>启动、中断、异常和系统调用</title>
    <link href="https://rilzob.com/2019/03/17/%E5%90%AF%E5%8A%A8%E3%80%81%E4%B8%AD%E6%96%AD%E3%80%81%E5%BC%82%E5%B8%B8%E5%92%8C%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/"/>
    <id>https://rilzob.com/2019/03/17/启动、中断、异常和系统调用/</id>
    <published>2019-03-17T13:25:37.660Z</published>
    <updated>2019-03-17T13:45:36.317Z</updated>
    
    <content type="html"><![CDATA[<h1 id="内容摘要"><a href="#内容摘要" class="headerlink" title="内容摘要"></a>内容摘要</h1><ul><li>启动<ul><li>计算机结构概述</li><li>计算机内存和硬盘布局</li><li>系统启动流程</li></ul></li><li>中断、异常和系统调用<ul><li>背景(3.3)</li><li>中断、异常和系统调用相比较(3.3)</li><li>中断和异常处理机制(3.3)</li><li>系统调用的概念和实现(3.4)</li><li>程序调用与系统调用的不同之处(3.4)</li><li>开销(3.4)</li><li>系统调用示例<a id="more"></a></li></ul></li></ul><h1 id="3-1-BIOS"><a href="#3-1-BIOS" class="headerlink" title="3.1 BIOS"></a>3.1 BIOS</h1><h2 id="计算机体系结构概述"><a href="#计算机体系结构概述" class="headerlink" title="计算机体系结构概述"></a>计算机体系结构概述</h2><p><img src="/2019/03/17/启动、中断、异常和系统调用/646b720b.png" alt="计算机体系结构概述"></p><h2 id="启动时计算机内存和磁盘布局"><a href="#启动时计算机内存和磁盘布局" class="headerlink" title="启动时计算机内存和磁盘布局"></a>启动时计算机内存和磁盘布局</h2><p><img src="/2019/03/17/启动、中断、异常和系统调用/7ca54080.png" alt="启动时计算机内存和磁盘布局"><br>CPU加电后会对里面寄存器做一个初始化到一个指定的状态，然后去执行第一条指令，第一条指令存储在内存中。内存会分为RAM随机访问存储和ROM只读存储两部分，系统的初始化代码存储在ROM中。<br>系统CPU完成初始化后，它处于实模式下，在实模式下它的地址计算把段寄存器左移4位，然后加上它的当前指令指针，这两个加在一起作为当前访问第一条指令的位置，还有一条限制是说在加电的时候，它处于实模式，这个时候地址总线并不是像我们现在用到的通常系统是32位，它只有20位的地址可用即用的区域是2的20次方就是1M，所以放的区域就只能放在最底下1M里头的一小块。<br>这块代码为了从硬盘上读数据，它必须提供相应的服务，如果没有这些服务，是没有办法访问到磁盘设备的，为了做到这件事情，在BIOS里头它需要提供这样的一些功能：<br><strong>(1)基本的输入输出</strong>：完成能够从磁盘上读数据，从键盘上读用户的输入并且可以在显示器上显示相应的输出。<br><strong>(2)系统的配置信息</strong>：由BIOS的设置来决定加电时是从硬盘启动、从网络启动还是说从光盘启动。依据这些设置执行它的启动程序，并且能从硬盘把加载程序和操作系统内容加载到系统当中来。<br><img src="/2019/03/17/启动、中断、异常和系统调用/fef5918d.png" alt="加载程序的内存地址空间"><br>具体过程：</p><p><strong>BIOS</strong></p><ol><li>初始化完成后，将加载程序从磁盘的引导扇区(这个引导扇区长度只有512字节，更长的它没有这个能力在BIOS程序，它不允许能读更多内容)加载到指定的位置(Ox7c00)；</li><li>跳转到其中的固定位置(CS:IP=0000:7c00)；</li><li>把控制权转到从磁盘上读进来的程序，这里是加载程序；</li></ol><p><strong>加载程序</strong></p><ol><li>将操作系统的代码和数据从硬盘加载到内存中；</li><li>跳转到操作系统的起始位置，把控制权交给操作系统，来继续执行操作系统功能；</li></ol><p><em>BIOS可以从磁盘读取加载程序，那么为什么不直接从BIOS里将操作系统的内核映像读取进来而通过加载程序再去读取呢？<br>说明：由于磁盘是由文件系统的并且文件系统多种多样，不能限制磁盘只使用一种文件系统，而且又不能在BIOS上加上认识所有文件系统的代码。为了增加这种灵活性，在这里就有一个基本约定就是我不需要认识格式也能从里头读到读到第一块，读到这块后会用这块里的加载程序来识别磁盘上的文件系统，认识磁盘上的文件系统后就可以读到操作系统内核的镜像并把它加载到内存当中来。这整个过程就是用加载程序读到操作系统来，这个过程后再把相应的控制权转到读进来的操作系统内核上，之后操作系统就可以运行了。</em></p><p><em>总结一下BIOS最初存放在内存中的ROM部分，加载程序和操作系统则存放在磁盘中，BIOS首先从磁盘中读取加载程序放入内存中，然后通过内存中的加载程序将磁盘中存储的操作系统也读取到内存中，最终跳转到操作系统在内存中的起始地址，把控制权交给操作系统。</em></p><p><strong>(3)开机自检和系统启动程序</strong></p><h2 id="BIOS系统调用"><a href="#BIOS系统调用" class="headerlink" title="BIOS系统调用"></a>BIOS系统调用</h2><ul><li>BIOS以中断调用的方式提供了基本的I/O功能<ul><li>INT 10h:字符显示</li><li>INT 13h:磁盘扇区读写</li><li>INT 15h:检测内存大小</li><li>INT 16h:键盘输入</li></ul></li><li>只能在x86实模式下访问，如果是在保护模式下上述功能无法使用</li></ul><h1 id="3-2-系统启动流程"><a href="#3-2-系统启动流程" class="headerlink" title="3.2 系统启动流程"></a>3.2 系统启动流程</h1><h2 id="计算机启动流程"><a href="#计算机启动流程" class="headerlink" title="计算机启动流程"></a>计算机启动流程</h2><p>之前所说的加电后去读BIOS，BIOS再去读你的加载程序，加载程序去读内核映像，这个实际上又可以细化下去，因为我们在加载程序的时候，在BIOS里头并不能直接去读<strong>加载程序(bootloader)</strong>。</p><p>最早系统里只有一个分区的时候可以直接在分区里找文件系统，然而现在大多数的计算机里头并非只有一个分区，可能会有几个分区，并且每个分区可能会装不同的系统，这时就需要在前边加上一个<strong>主引导记录</strong>，这个主引导记录的目的就是说明要从哪个文件系统内去读加载程序。有了主引导记录后就进到当前某个分区里面，分区里面又有一个分区的<strong>引导扇区</strong>，这个<strong>活动分区</strong>的引导扇区再来加载文件系统的加载程序。这个过程当中实际上我们就需要知道中间这几个部分的格式是什么样子的，如果不知道，那么写出来的程序最终存到磁盘上机器是不能够从里头认识的。</p><p>总结一下首先系统加电，BIOS初始化硬件，之后BIOS读取磁盘上最前边的<strong>主引导扇区代码(主引导记录)</strong>，由主引导记录得知我们知道接下来要进入哪个分区，进入分区后主引导记录读取<strong>活动分区</strong>的<strong>引导扇区代码</strong>，引导扇区再来读取文件系统内的加载程序，层层递进，类似栈的结构。<br><img src="/2019/03/17/启动、中断、异常和系统调用/1f91e41e.png" alt="计算机启动流程"></p><h2 id="CPU初始化"><a href="#CPU初始化" class="headerlink" title="CPU初始化"></a>CPU初始化</h2><p>具体说来有这样的几个过程，首先CPU加电完成它的初始化到一个确定的状态去读第一条指令，我们需要知道CPU初始化之后它的代码段段寄存器和当前指令指针寄存器这两个寄存器的内容，算出它的第一条指令在内存当中的什么地方，这是它计算的依据。<br>因为它是实模式，所以CS和IP都是16位的，CS左移4位后与IP相加算出当前访问的第一条指令的位置。并且BIOS存放在内存中最底下的1M位置，原因是实模式下地址总线是只有20位。<br>CPU加电稳定后从0XFFFF0(CS:IP=0xf000:fff0)读第一条指令，第一条指令是跳转指令。<br><img src="/2019/03/17/启动、中断、异常和系统调用/a874dc69.png" alt="CPU初始化"></p><h2 id="BIOS初始化过程"><a href="#BIOS初始化过程" class="headerlink" title="BIOS初始化过程"></a>BIOS初始化过程</h2><p>BIOS除了从磁盘上读取加载程序，实际上还有很多事情要做。<br>(1) <u>硬件自检POST</u>：顾名思义硬件自检是为了检测出硬件的好坏。<br>(2) <u>检查系统中内存和显卡等关键部件的是否存在和工作状态。</u><br>(3) <u>查找并执行显卡等接口卡BIOS，进行<strong>设备初始化</strong>。</u>因为这些关键性的接口卡里也有它自己的初始化程序。<br>(4) <u>执行系统BIOS，进行系统检测，检测和配置系统中安装的即插即用设备(<strong>系统初始化</strong>)。</u>比如我想从一个USB接口的光驱里启动系统，如何启动？在这个BIOS里的自检是能够做到系统的自检，检测并配置这些即插即用的设备。<br>(5) <u>更新CMOS中的扩展系统配置数据ESCD。</u>上述都做完了后就已经知道系统里连接了哪些硬件，在BIOS里有一个<strong>系统配置表(ESCD)</strong>，就是<strong>扩展系统配置数据</strong>。通过这个数据就可以知道当前系统里有些什么设备，并且这个数据会随着设备的改变而改变。<br>(6) <u>按指定启动顺序从软盘、硬盘或光驱启动。</u>第5步也做完后就将控制权转移到从外部读进来的数据里或读进来的代码里，而这就是按照我们在BIOS里指定的顺序，从软盘、硬盘或者光盘或者指定的其他设备上读进第一块扇区。</p><h2 id="主引导记录MBR格式"><a href="#主引导记录MBR格式" class="headerlink" title="主引导记录MBR格式"></a>主引导记录MBR格式</h2><p>读进扇区后面临多个分区，这时候就需要主引导记录。主引导记录总共512字节，分为三部分：启动代码、硬盘分区表和结束标志字。</p><h3 id="启动代码"><a href="#启动代码" class="headerlink" title="启动代码"></a>启动代码</h3><p>启动代码占主引导记录中的446字节，它的主要作用有两点：</p><ol><li>检查分区表正确性。如果分区表是错误的，那么程序时无法正常加载的；</li><li>加载并跳转到磁盘上的引导程序；</li></ol><h3 id="硬盘分区表"><a href="#硬盘分区表" class="headerlink" title="硬盘分区表"></a>硬盘分区表</h3><p>硬盘分区表则占64字节，硬盘分区表负责描述分区状态和位置，每个分区描述信息占16字节。</p><h3 id="结束标志字"><a href="#结束标志字" class="headerlink" title="结束标志字"></a>结束标志字</h3><p>结束标志字占剩余的2字节。所有的引导扇区都有一个结束标志，这个结束标是55AA。有了结束标后，它才认为这是一个合法的主引导记录。</p><h2 id="分区引导扇区格式"><a href="#分区引导扇区格式" class="headerlink" title="分区引导扇区格式"></a>分区引导扇区格式</h2><p>由主引导记录进入分区后同样需要面对分区引导扇区，分区引导扇区由4个部分组成：</p><ol><li>跳转指令：跳转到启动代码。这条跳转指令与平台相关，CPU不同跳转指令不同(我猜是因为汇编语言分为Intel格式和AT&amp;T格式两种)</li><li>文件卷头：文件系统描述信息。</li><li>启动代码：跳转到加载程序。启动代码说明加载程序的位置，加载程序可以放在任意的地方只需启动代码标识出来即可。</li><li>结束标志：55AA，和主引导记录的结束标志字相同。</li></ol><h2 id="加载程序-bootloader"><a href="#加载程序-bootloader" class="headerlink" title="加载程序(bootloader)"></a>加载程序(bootloader)</h2><p>加载程序同样可以细化，主要分成三步：</p><ol><li>从文件系统中读取启动配置信息。加载程序并非直接去加载内核，而是从文件系统中读一个启动配置文件(这时候加载程序是能够认识文件系统的格式的)，这个启动配置文件在不同的操作系统里是不一样的，比如Windows和Linux都有自己的格式，这样Windows和Linux都有自己的加载程序的格式。</li><li>可选的操作系统内核列表和加载参数。依据配置信息选择启动的参数，比如是正常启动，还是在安全模式下启动，更或者是在调试状态下启动系统，这些区别都可以读出来。</li><li>依据配置加载指定内核并跳转到内核执行。参数已经选择好后，配置信息导致加载程序在加载内核的时候内核会不一样，依据配置加载内核。</li></ol><p><img src="/2019/03/17/启动、中断、异常和系统调用/5caace75.png" alt="加载程序"></p><p>虽然整个过程的描述已经细化，但是介绍的仍然是很粗的。如果要想写出实际的程序，那么还需要知道CPU的手册、CPU加电时的状态，BIOS里的规范，第一条指令在磁盘中的位置和它的格式，内核编译时的一些相应信息。有很多需要考虑的因素，这种考虑的因素又有很多细节和实际的硬件环境或者说周围的情况密切相关，这时就需要制定一组相应的标准作为系统启动的规范。</p><h2 id="系统启动规范"><a href="#系统启动规范" class="headerlink" title="系统启动规范"></a>系统启动规范</h2><p>系统启动规范主要分为BIOS和UEFI两种。</p><h3 id="BIOS"><a href="#BIOS" class="headerlink" title="BIOS"></a>BIOS</h3><p>BIOS是现在广泛使用的在PC机上的启动流程标准。BIOS是主板上的一段程序，包括系统设置，自检程序和系统自启动程序，它可以完成系统的启动。<br>BIOS从70年代后期最早出现，至今已有几十年的发展并发生很多变化，主要有BIOS-MBR、BIOS-GPT和PXE三种。之前提到的主引导记录BIOS-MBR实际上相当于最早的BIOS，它是从主板加电自检后进到磁盘上的唯一的一个分区上去加载它的引导记录，然而有了多分区磁盘后就需要选择从哪个分区启动，这时就在前面加上一个主引导记录来说明是选择了哪个分区进行启动。由前文介绍主引导扇区的格式可知，主引导记录里只能描述最多4个分区，每个占16个字节，因为启动代码和结束标志字已经将剩余的448字节全部占满，然而现在的计算机很多都会超过4个分区。为了解决这一问题出现了GPT(全局唯一标识分区表)，GPT可以在分区表里描述更多的分区结构，这样就不会有4个分区的限制了。BIOS-MBR和BIOS-GPT是BIOS的两个发展，PXE实际上是网络启动的一个标准，举个例子就是机器启动后想听过局域网或者其他的网络连接服务器，从服务器上下载内核镜像来执行，PXE就是这种启动的标准。<br>总的来说BIOS可以有一些局部的修改来完善对后续的支持，但这种支持总是会受到前边的制约，比如说在主引导记录里为了支持多分区就在中间加成了磁盘的主引导巨鹿，然后再加上活动分区里的引导记录，多了两层但实际多的这两层意义并不是特别的必要。所以可以设计一种全新的规范来解决这一问题，这就是UEFI。</p><h3 id="UEFI"><a href="#UEFI" class="headerlink" title="UEFI"></a>UEFI</h3><p>UEFI(统一可扩展固件接口)想达到的目的是在所有平台上提供一致的操作系统启动服务，为了做到这一点它从90年代开始推出它的第一个版本，直到现在都在不断的演变的过程中。<br><img src="/2019/03/17/启动、中断、异常和系统调用/efa2604d.png" alt="系统启动规范"></p><h1 id="3-3-中断、异常和系统调用比较"><a href="#3-3-中断、异常和系统调用比较" class="headerlink" title="3.3 中断、异常和系统调用比较"></a>3.3 中断、异常和系统调用比较</h1><p>这节介绍了中断、异常和系统调用的作用，是为了解决什么问题，主要的应用场景。以及他们之间的区别和共同点，还介绍了中断、异常和系统调用的实现机制。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前介绍过计算机启动后会加载操作系统的内核，然后将控制权交给操作系统内核，这一阶段是可以信任的。但在操作系统内核之上，实际还有很多的应用程序，没有办法做到对这些应用程序的完全的信任，然而这些应用程序要使用操作系统内核提供的服务，并且只有操作系统执行特短指令(具有特殊权限的指令，这类指令只用于操作系统或其他系统软件，一般不直接提供给用户使用)，这时就需要解决一个操作系统内核与外界打交道的问题，也就是说可以信任的内核必须对外界提供某种访问的接口。</p><p>同样在使用计算机的过程中我们除了跟应用程序打交道外，程序或计算机系统在运行过程中会有各种各样的问题，为了能够让计算机系统对外界作出适当的反映比如及时反映键盘的输入，需要提出<strong>中断</strong>机制，也就是当外设与系统有交互的时候需要如何处理。<br>还有一种情况是使用应用程序的过程中出现了一些问题，这些问题是程序编写者事先没有预料到的，对于这种异常情况把它的控制权转交给操作系统，由操作系统来处理它，这就是应用程序执行中遇到意外交由<strong>异常</strong>来做处理。</p><p><strong>系统调用</strong>则是为了解决用户程序如何来使用系统服务的问题。操作系统需要通过系统调用来提供一个接口，让应用程序既方便的使用内核提供的服务，又不至于用户的行为对内核的安全产生影响。提供服务的方式有多种可以通过内核提供服务，还可以使用函数库，这里需要作出判断。</p><ul><li>为什么需要中断、异常和系统调用<ul><li>在计算机运行中，内核是被信任的第三方</li><li>只有内核可以执行特权指令</li><li>方便应用程序</li></ul></li><li>中断和异常希望解决的问题<ul><li>当外设连接计算机时，会出现什么现象？</li><li>当应用程序处理意想不到的行为时，会出现什么现象？</li></ul></li><li>系统调用希望解决的问题<ul><li>用户应用程序时如何得到系统服务？</li><li>系统调用和功能调用的不同之处是什么？</li></ul></li></ul><h2 id="内核的进入与退出"><a href="#内核的进入与退出" class="headerlink" title="内核的进入与退出"></a>内核的进入与退出</h2><p><img src="/2019/03/17/启动、中断、异常和系统调用/692c3df7.png" alt="内核的进入与退出"><br>从这个图中可以看到操作系统内核和外界打交道基本上就是中断、异常和系统调用这三个接口。</p><h2 id="中断、异常和系统调用"><a href="#中断、异常和系统调用" class="headerlink" title="中断、异常和系统调用"></a>中断、异常和系统调用</h2><p>系统调用(System call)是应用程序主动向操作系统发出的服务请求。<br>异常(Exception)则是非法指令或者其他原因导致的指令执行失败(如：内存出错)之后的处理请求。<br>中断(hardware interrupt)是硬件设备对操作系统提出的处理请求。</p><h2 id="中断、异常和系统调用的比较"><a href="#中断、异常和系统调用的比较" class="headerlink" title="中断、异常和系统调用的比较"></a>中断、异常和系统调用的比较</h2><ul><li>源头<ul><li>中断：外设</li><li>异常：应用程序意想不到的行为</li><li>系统调用：应用程序请求操作提供服务</li></ul></li><li>响应方式<ul><li>中断：异步</li><li>异常：同步。因为异常是与当前指令有关的，必须处理完当前纸条异常所产生指令所导致的问题才可以继续下去。</li><li>系统调用：异步或同步</li></ul></li><li>处理机制<ul><li>中断：持续，对用户应用程序是透明的</li><li>异常：杀死或者重新执行意想不到的应用程序指令。异常会处理当前所出现的问题。</li><li>系统调用：等待和持续。等待用户提出之后处理，等待然后再继续。</li></ul></li></ul><h2 id="中断处理机制"><a href="#中断处理机制" class="headerlink" title="中断处理机制"></a>中断处理机制</h2><p>这节标题中的中断实际上可以理解为系统调用、中断和异常这三种情况的总称。</p><h3 id="硬件处理"><a href="#硬件处理" class="headerlink" title="硬件处理"></a>硬件处理</h3><ul><li>在CPU初始化时设置中断使能标志。也就是说在许可外界打扰CPU的执行之前CPU是不会对外界的任何中断请求发出响应<ul><li>依据内部或外部事件设置中断标志</li><li>依据中断向量调用相应中断服务例程。</li></ul></li></ul><p>中断产生了后通常是一个电平的上升沿或者说是一个高电平，那CPU会记录下这间事情，也就是说会有一个中断标志表示出现了一个中断，然后这时候需要知道中断是由什么设备产生的，需要知道中断源的编号，这一部分是由硬件来做的。</p><h3 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h3><ul><li>现场保护(编译器)</li><li>中断服务程序(服务例程)</li><li>清除中断标记(服务例程)</li><li>现场恢复(编译器)</li></ul><h2 id="中断嵌套"><a href="#中断嵌套" class="headerlink" title="中断嵌套"></a>中断嵌套</h2><ul><li>硬件中断服务例程可被打断<ul><li>不同硬件中断源可能硬件中断处理时出现</li><li>硬件中断服务例程中需要临时禁止中断请求</li><li>中断请求会保持到CPU做出响应</li></ul></li><li>异常服务例程可能被打断<ul><li>异常服务例程执行时可能出现硬件中断</li></ul></li><li>异常服务例程可嵌套<ul><li>异常服务例程可能出现缺页</li></ul></li></ul><h1 id="3-4-系统调用"><a href="#3-4-系统调用" class="headerlink" title="3.4 系统调用"></a>3.4 系统调用</h1><h2 id="系统调用"><a href="#系统调用" class="headerlink" title="系统调用"></a>系统调用</h2><ul><li>操作系统服务的编程接口</li><li>通常由高级语言编写(C或者C++)</li><li>程序访问通常是通过高层次的API接口而不是直接进行系统调用。写程序的时候通常并不直接去使用系统调用而把系统调用封装到一个库里面，应用程序是访问这些库里的库函数来实现的。</li><li>不同的系统里用户使用的接口是不一样的，三种最常用的应用程序编程接口(API)<ul><li>Win32 API 用于 Windows</li><li>POSIX API 用于POSIX-based systems(包括UNIX,LINUX,MAC OS X的所有版本)</li><li>Java API用于JAVA虚拟机(JVM)</li></ul></li></ul><h2 id="系统调用的实现"><a href="#系统调用的实现" class="headerlink" title="系统调用的实现"></a>系统调用的实现</h2><ul><li>每个系统调用对应一个系统调用号<ul><li>系统调用接口根据系统调用号来维护表的索引</li></ul></li><li>系统调用接口调用内核态中的系统调用功能实现，并返回系统调用的状态和结果</li><li>用户不需要知道系统调用的实现<ul><li>需要设置调用参数和获取返回结果</li><li>操作系统接口的细节大部分都隐藏在应用编程接口后<blockquote><p>通过运行程序支持的库来管理</p></blockquote></li></ul></li></ul><h2 id="函数调用和系统调用的不同处"><a href="#函数调用和系统调用的不同处" class="headerlink" title="函数调用和系统调用的不同处"></a>函数调用和系统调用的不同处</h2><p>调用一个函数需要把参数压到堆栈里面去，然后转到相应函数去执行，执行时候从堆栈里获取参数信息执行，返回的结果放在那里再返回回来，这样在上面的函数调用就知道相关的返回结果，然后利用这个结果继续往下执行。而对于系统调用来说，它由于内核是受保护的，而应用程序是它自己的区域，为了保护内核的实现，这里内核和用户态的应用程序之间使用不同的堆栈，所以在这里会有一个堆栈的切换，切换之后由于处于内核态，就可以使用特权指令，这些特权指令所导致的结果就是这时可以直接对设备进行控制，而这种操作在用户态是不可能进行的。</p><p>系统调用使用的是INT和IRET指令用于系统调用，函数调用使用的是CALL和RET指令，这四条指令在指令集是完全不同的。</p><ul><li>系统调用<ul><li>INT和IRET指令用于系统调用<blockquote><p>系统调用时，堆栈切换和特权级的转换</p></blockquote></li></ul></li><li>函数调用<ul><li>CALL和RET用于常规调用<blockquote><p>常规调用时没有堆栈切换</p></blockquote></li></ul></li></ul><h2 id="中断、异常和系统调用的开销"><a href="#中断、异常和系统调用的开销" class="headerlink" title="中断、异常和系统调用的开销"></a>中断、异常和系统调用的开销</h2><ul><li>超过函数调用。原因是有一个用户态到内核态的切换</li><li>开销：<ul><li>引导机制</li><li>建立内核堆栈</li><li>验证参数</li><li>内核态映射到用户态的地址空间<blockquote><p>更新页面映射权限</p></blockquote></li><li>内核态独立地址空间<blockquote><p>TLB</p></blockquote></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;内容摘要&quot;&gt;&lt;a href=&quot;#内容摘要&quot; class=&quot;headerlink&quot; title=&quot;内容摘要&quot;&gt;&lt;/a&gt;内容摘要&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;启动&lt;ul&gt;
&lt;li&gt;计算机结构概述&lt;/li&gt;
&lt;li&gt;计算机内存和硬盘布局&lt;/li&gt;
&lt;li&gt;系统启动流程&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;中断、异常和系统调用&lt;ul&gt;
&lt;li&gt;背景(3.3)&lt;/li&gt;
&lt;li&gt;中断、异常和系统调用相比较(3.3)&lt;/li&gt;
&lt;li&gt;中断和异常处理机制(3.3)&lt;/li&gt;
&lt;li&gt;系统调用的概念和实现(3.4)&lt;/li&gt;
&lt;li&gt;程序调用与系统调用的不同之处(3.4)&lt;/li&gt;
&lt;li&gt;开销(3.4)&lt;/li&gt;
&lt;li&gt;系统调用示例&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="操作系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>Python中将字符串所表示的值进行int或float转换</title>
    <link href="https://rilzob.com/2018/12/24/Python%E4%B8%AD%E5%B0%86%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%89%80%E8%A1%A8%E7%A4%BA%E7%9A%84%E5%80%BC%E8%BF%9B%E8%A1%8Cint%E6%88%96float%E8%BD%AC%E6%8D%A2/"/>
    <id>https://rilzob.com/2018/12/24/Python中将字符串所表示的值进行int或float转换/</id>
    <published>2018-12-24T03:58:20.615Z</published>
    <updated>2018-12-24T03:58:20.615Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个小技巧，在做编译原理课设的过程中需要我将通过单词扫描器得到的单词进行类型判断是<code>int</code>类型还是<code>float</code>类型并调用对应的类型转换函数<code>int()</code>和<code>float()</code>，然而我并有找到单独一个函数去做这个事情，于是通过使用几个函数组合解决了这个问题。</p><a id="more"></a><p>解决这个问题的思路是通过使用<code>replace(&#39;.&#39;, &#39;&#39;)</code>函数将字符串中的’.’替换为空，然后调用<code>isdigit()</code>判断替换后的字符串是否全由数字组成，如果是则排除是字符串的组合。然后再次调用<code>isdigit()</code>函数辨别字符串中是否存在<code>&#39;.&#39;</code>，如果有则说明是浮点数，否则为整数。</p><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">var0 = <span class="string">'1234'</span></span><br><span class="line">var1 = <span class="string">'123.4'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> var.replace(<span class="string">'.'</span>, <span class="string">''</span>).isdigit():</span><br><span class="line">        <span class="keyword">if</span> var.isdigit():</span><br><span class="line">            <span class="keyword">return</span> int(var)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> float(var)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> var</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'var0:'</span>, convert(var0))</span><br><span class="line">print(<span class="string">'var1:'</span>, convert(var1))</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">'var0:'</span>, <span class="number">1234</span>)</span><br><span class="line">(<span class="string">'var1:'</span>, <span class="number">123.4</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一个小技巧，在做编译原理课设的过程中需要我将通过单词扫描器得到的单词进行类型判断是&lt;code&gt;int&lt;/code&gt;类型还是&lt;code&gt;float&lt;/code&gt;类型并调用对应的类型转换函数&lt;code&gt;int()&lt;/code&gt;和&lt;code&gt;float()&lt;/code&gt;，然而我并有找到单独一个函数去做这个事情，于是通过使用几个函数组合解决了这个问题。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://rilzob.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Python动态生成变量名和exec函数及eval函数的用法</title>
    <link href="https://rilzob.com/2018/12/21/Python%E5%8A%A8%E6%80%81%E7%94%9F%E6%88%90%E5%8F%98%E9%87%8F%E5%90%8D%E5%92%8Cexec%E5%87%BD%E6%95%B0%E5%8F%8Aeval%E5%87%BD%E6%95%B0%E7%9A%84%E7%94%A8%E6%B3%95/"/>
    <id>https://rilzob.com/2018/12/21/Python动态生成变量名和exec函数及eval函数的用法/</id>
    <published>2018-12-21T04:17:04.965Z</published>
    <updated>2018-12-21T04:36:31.637Z</updated>
    
    <content type="html"><![CDATA[<p>最近在做编译原理课程设计，在实现基于DAG的局部优化算法时需要生成很多变量且变量需要以n1、n2、n3····n100这种形式命名作为DAG结点的编码，使用其他静态编译语言据我了解只能在代码中手动写出这100个变量名，但是查阅资料发现Python能够实现<strong>动态生成变量名</strong>而不像静态语言一样笨拙。</p><p>解决动态生成变量名的问题有几种方法，类似<code>locals</code>函数、<code>exec</code>函数。其中我选择的是<code>exec</code>函数，选择的原因是这两个函数起先我都不了解，于是尝试写一些demo学习使用它们，然而<code>locals</code>函数好像在我的应用场景中并不适用又或者是由于我的使用方法不对导致的无法得到想要的结果，诸多原因使得我最终选择了<code>exec</code>函数来动态生成变量名。</p><a id="more"></a><p>单独使用<code>exec</code>函数其实并不能动态生成变量名，与<code>format</code>函数加以配合才能达到该目的。</p><h1 id="exec函数"><a href="#exec函数" class="headerlink" title="exec函数"></a>exec函数</h1><p>这里首先介绍一下<code>exec</code>函数。<br><code>exec</code>函数是Python的built-in函数(内置函数)。<code>exec</code>函数的<strong>实际作用</strong>是<em>动态执行Python代码</em>。也就是说<code>exec</code>函数可以执行复杂的Python代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exec(source, globals=<span class="keyword">None</span>, locals=<span class="keyword">None</span>, /)</span><br></pre></td></tr></table></figure><p><strong>参数说明</strong>：</p><ul><li><strong>source</strong>：必选参数，表示需要被指定的Python代码。它必须是字符串或者code对象。如果<code>source</code>是一个字符串，该字符串会先被解析为一组Python语句，然后执行。如果<code>source</code>是code对象，那么它只是被简单的执行。</li><li><strong>globals</strong>：可选参数，表示全局命名空间(存放全局变量)，如果被提供，则必须是一个字典对象。</li><li><strong>locals</strong>：可选参数，表示局部命名空间(存放局部变量)，如果被提供，可以是任何映射对象。如果参数被忽略，那么它将会取与<code>globals</code>相同的值。</li></ul><p>如果<code>globals</code>和<code>locals</code>都被忽略，那么它们将取<code>exec</code>函数被调用环境下的全局命名空间和局部命名空间。</p><p><strong>返回值</strong>：<code>exec</code>函数的返回值永远为<code>None</code>。</p><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>j = <span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>exec(<span class="string">"ans = i + j"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"Answer is: "</span>, ans)</span><br><span class="line">Answer <span class="keyword">is</span>: <span class="number">5</span></span><br></pre></td></tr></table></figure></p><p>解释一下在上个例子中，<code>ans</code>变量并没有显式的定义，但仍然可以在<code>print</code>函数中调用。这是由于<code>exec()</code>语句执行了<code>&quot;ans = i + j&quot;</code>中的代码，定义了<code>ans</code>变量。</p><h1 id="format函数"><a href="#format函数" class="headerlink" title="format函数"></a>format函数</h1><p><code>str.format</code>函数是一种格式化字符串的函数，它增强了字符串格式化的功能。</p><p>基本语法是通过<code>{}</code>和<code>:</code>来代替之前的<code>%</code>。<br><code>format</code>函数可以接受不限个参数，位置可以不按顺序。</p><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="string">"&#123;&#125; &#123;&#125;"</span>.format(<span class="string">"hello"</span>, <span class="string">"world"</span>)    <span class="comment"># 不设置指定位置，按默认顺序</span></span><br><span class="line"><span class="string">'hello world'</span></span><br><span class="line">&gt;&gt;&gt;<span class="string">"&#123;1&#125; &#123;0&#125; &#123;1&#125;"</span>.format(<span class="string">"hello"</span>, <span class="string">"world"</span>)  <span class="comment"># 设置指定位置</span></span><br><span class="line"><span class="string">'world hello world'</span></span><br></pre></td></tr></table></figure></p><p>同样可以设置参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"网站名：&#123;name&#125;, 地址 &#123;url&#125;"</span>.format(name=<span class="string">"菜鸟教程"</span>, url=<span class="string">"www.runoob.com"</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>网站名：菜鸟教程, 地址 www.runoob.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过字典设置参数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>site = &#123;<span class="string">"name"</span>: <span class="string">"菜鸟教程"</span>, <span class="string">"url"</span>: <span class="string">"www.runoob.com"</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"网站名：&#123;name&#125;, 地址 &#123;url&#125;"</span>.format(**site))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>网站名：菜鸟教程, 地址 www.runoob.com</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 通过列表索引设置参数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>my_list = [<span class="string">'菜鸟教程'</span>, <span class="string">'www.runoob.com'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"网站名：&#123;0[0]&#125;, 地址 &#123;0[1]&#125;"</span>.format(my_list))  <span class="comment"># "0" 是必须的</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>网站名：菜鸟教程, 地址 www.runoob.com</span><br></pre></td></tr></table></figure></p><p>也可以向<code>str.format()</code>传入对象：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AssignValue</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.value = value</span><br><span class="line">my_value = AssignValue(<span class="number">6</span>)</span><br><span class="line">print(<span class="string">'value 为: &#123;0.value&#125;'</span>.format(my_value))  <span class="comment"># "0" 是可选的</span></span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value 为: <span class="number">6</span></span><br></pre></td></tr></table></figure></p><p>菜鸟教程中还有一些数字格式化的具体教程，感兴趣的可以看一下。</p><h1 id="动态生成变量名"><a href="#动态生成变量名" class="headerlink" title="动态生成变量名"></a>动态生成变量名</h1><p>将上述介绍的<code>exec</code>函数和<code>format</code>函数结合起来就能够做到动态生成变量名。</p><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">   ...:     exec(<span class="string">'var&#123;&#125; = &#123;&#125;'</span>.format(i, i))</span><br><span class="line">   ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: print(var0, var1, var2, var3 ,var4)</span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br></pre></td></tr></table></figure></p><h1 id="eval函数"><a href="#eval函数" class="headerlink" title="eval函数"></a>eval函数</h1><p><code>eval</code>函数与<code>exec</code>函数有些相似但又有些不同<code>exec</code>，所以这里同时介绍<code>eval</code>函数，与<code>exec</code>函数对比记忆加深理解。<br><code>eval</code>函数同样能够做到动态执行代码，但是它所能够执行的代码相比<code>exec</code>函数有特殊的限定。</p><p><code>eval</code>函数的<strong>实际作用</strong>是<em>计算指定表达式的值</em>。也就是说它要执行的Python代码只能是单个表达式(注意<code>eval</code>不支持任何形式的赋值操作)，而不能是复杂的代码逻辑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eval(source, globals=<span class="keyword">None</span>, locals=<span class="keyword">None</span>, /)</span><br></pre></td></tr></table></figure><p><strong>参数说明</strong>与<code>exec</code>函数的一样。</p><p><strong>返回值</strong>：<br>如果<code>source</code>是一个code对象，且创建该code对象时，<code>complie</code>函数的mode参数是<code>exec</code>，那么<code>eval</code>函数的返回值是None；<br>否则，如果<code>source</code>是一个输出语句，如<code>print()</code>，则<code>eval()</code>的返回结果为<code>None</code>；<br>否则，<code>source</code>表达式的结果就是<code>eval()</code>的返回值。</p><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">()</span>:</span></span><br><span class="line">    y = <span class="number">20</span>   <span class="comment">#局部变量y</span></span><br><span class="line">    a = eval(<span class="string">"x+y"</span>)</span><br><span class="line">    print(<span class="string">"a:"</span>,a)      <span class="comment">#x没有就调用全局变量</span></span><br><span class="line">    b = eval(<span class="string">"x+y"</span>,&#123;<span class="string">"x"</span>:<span class="number">1</span>,<span class="string">"y"</span>:<span class="number">2</span>&#125;)     <span class="comment">#定义局部变量，优先调用</span></span><br><span class="line">    print(<span class="string">"b:"</span>,b)</span><br><span class="line">    c = eval(<span class="string">"x+y"</span>,&#123;<span class="string">"x"</span>:<span class="number">1</span>,<span class="string">"y"</span>:<span class="number">2</span>&#125;,&#123;<span class="string">"y"</span>:<span class="number">3</span>,<span class="string">"z"</span>:<span class="number">4</span>&#125;)  </span><br><span class="line">    print(<span class="string">"c:"</span>,c)  </span><br><span class="line">    d = eval(<span class="string">"print(x,y)"</span>)</span><br><span class="line">    print(<span class="string">"d:"</span>,d)   <span class="comment">#对于变量d，因为print()函数不是一个计算表达式，因此没有返回值</span></span><br><span class="line">func()</span><br></pre></td></tr></table></figure></p><p>输出结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a: <span class="number">30</span></span><br><span class="line">b: <span class="number">3</span></span><br><span class="line">c: <span class="number">4</span></span><br><span class="line"><span class="number">10</span> <span class="number">20</span></span><br><span class="line">d: <span class="keyword">None</span></span><br></pre></td></tr></table></figure></p><h1 id="eval函数和exec函数的区别"><a href="#eval函数和exec函数的区别" class="headerlink" title="eval函数和exec函数的区别"></a>eval函数和exec函数的区别</h1><ol><li><code>eval</code>函数只能计算单个表达式的值，而<code>exec</code>函数可以动态运行代码段；</li><li><code>eval</code>函数可以有返回值，而<code>exec</code>函数返回值永远为<code>None</code>；</li></ol><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p> <a href="https://www.cnblogs.com/technologylife/p/9211324.html" target="_blank" rel="noopener">Python动态变量名定义与调用 - Pyerlife - 博客园</a><br> <a href="https://www.cnblogs.com/yangmingxianshen/p/7810496.html" target="_blank" rel="noopener">python中的exec()、eval()以及complie() - 明王不动心 - 博客园</a><br> <a href="http://www.runoob.com/python/att-string-format.html" target="_blank" rel="noopener">Python format 格式化函数 | 菜鸟教程</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在做编译原理课程设计，在实现基于DAG的局部优化算法时需要生成很多变量且变量需要以n1、n2、n3····n100这种形式命名作为DAG结点的编码，使用其他静态编译语言据我了解只能在代码中手动写出这100个变量名，但是查阅资料发现Python能够实现&lt;strong&gt;动态生成变量名&lt;/strong&gt;而不像静态语言一样笨拙。&lt;/p&gt;
&lt;p&gt;解决动态生成变量名的问题有几种方法，类似&lt;code&gt;locals&lt;/code&gt;函数、&lt;code&gt;exec&lt;/code&gt;函数。其中我选择的是&lt;code&gt;exec&lt;/code&gt;函数，选择的原因是这两个函数起先我都不了解，于是尝试写一些demo学习使用它们，然而&lt;code&gt;locals&lt;/code&gt;函数好像在我的应用场景中并不适用又或者是由于我的使用方法不对导致的无法得到想要的结果，诸多原因使得我最终选择了&lt;code&gt;exec&lt;/code&gt;函数来动态生成变量名。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://rilzob.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>《推荐系统实践》读书笔记总结</title>
    <link href="https://rilzob.com/2018/12/04/%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/"/>
    <id>https://rilzob.com/2018/12/04/《推荐系统实践》读书笔记总结/</id>
    <published>2018-12-04T09:01:44.924Z</published>
    <updated>2019-03-24T03:32:34.041Z</updated>
    
    <content type="html"><![CDATA[<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>本书着重介绍了推荐系统的各种算法设计和系统设计的方法，并且利用一些公开的数据集离线评测了各种算法。对于无法通过离线评测知道算法性能的情况，本书引用了很多著名的用户调查实验来比较不同的算法。</p><p>但需要申明的一点是，本书的很多离线实验都是在一两个数据集上完成的，所以本书得到的所有结论都不是定论，可能换一个数据集就会得到完全相反的结论。这主要是因为不同网站中的用户行为有很大的差异，所以推荐系统很难有放之四海而皆准的结论。因此，需要在自己的数据集上重复本书介绍的实验和算法，再得到审核自己具体情况的结论。</p><a id="more"></a><p>2009年ACM推荐系统大会上Strand研究人员做的一个报告“推荐系统十堂课”，在这个报告中Strand的研究人员总结了他们设计推荐系统的经验，提出了10条在设计推荐系统中学习到的经验和教训。</p><ol><li><strong>确定你真的需要推荐系统。</strong>推荐系统只有在用户遇到信息过载时才必要。如果你的网站物品不太多，或者用户兴趣都比较单一，那么也许并不需要推荐系统。所以不要纠结于推荐系统这个词，不要为了做推荐系统而做推荐系统，而是应该从用户的角度出发，设计出能够真正帮助用户发现内容的系统，无论这个系统算法是否复杂，只要能够真正帮助用户，就是一个好的系统。</li><li><strong>确定商业目标和用户满意度之间的关系。</strong>对用户好的推荐系统不代表商业上有用的推荐系统，因此要首先确定用户满意的推荐系统和商业上需求的差距。一般来说，有些时候用户满意和商业需求并不吻合。但是一般情况下，用户满意度总是符合企业的长期利益，因此这一条的主要观点是要平衡企业的长期利益和短期利益之间的关系。</li><li><strong>选择合适的开发人员。</strong>一般来说，如果是一家大公司，应该雇用自己的开发人员来专门进行推荐系统的开发。</li><li><strong>忘记冷启动的问题。</strong>不断地创新，互联网上有任何你想要的数据。只要用户喜欢你的产品，他们就会不断贡献新的数据。</li><li><strong>平衡数据和算法之间的关系。</strong>使用正确的用户数据对推荐系统至关重要。对用户行为数据的深刻理解是设计好推荐系统的必要条件，因此分析数据是设计系统中最重要的部分。数据分析决定了如何设计模型，而算法只是决定了最终如何优化模型。</li><li><strong>找到相关的物品很容易，但是何时以何种方式将它们展现给用户是很困难的。</strong>不要为了推荐而推荐。</li><li><strong>不要浪费时间计算相似兴趣的用户，可以直接利用社会网络数据。</strong></li><li><strong>需要不断地提升算法的扩展性。</strong></li><li><strong>选择合适的用户反馈方式。</strong></li><li><strong>设计合理的评测系统，时刻关注推荐系统各方面的性能。</strong></li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这本书很早之前就想看了而且很早之前就买了，然后可能由于当时看到这里面复杂的数学公式和算法望而生畏就没有读下去，只读了一点。最近不是很忙，加上心血来潮花了一周时间粗略地看了一遍并且整理了读书笔记。同时，这也是我第一次写读书笔记，笔记的很大部分都是摘抄书上内容，而另一部分则是将书上内容用我自己的语言加以概括。这么做的原因有两点：<em>(1)第一次整理读书笔记不清楚该如何整理；(2)关于推荐系统方面的内容是我第一次了解，所以很多知识都是第一次听说，因此觉得书上的所有内容我都需要学习、整理。</em>尽管这次我整理了书中几乎所有的公式，但是没有一个公式是我真正推导验证过的，只知皮毛。当然这么做也是有我的理由的，我觉得即使我这次推导了一遍公式，记忆很牢固但是如果长期不去使用、复习，那么忘记是迟早的事。</p><p>为了让自己心里不那么惭愧，我总结了通过这次整理读书笔记学到的阅读整理技巧，聊以自慰：</p><ol><li><strong>读技术性书籍不要奢求读一次便要全部掌握记牢，欲速则不达。</strong>读技术性书籍有好几种阅读方法，你可以尝试带有目的性的去读，当前你需要去了解哪部分知识便去读那部分知识，这样可以做当通过实际生活中的应用加深你对相应知识的理解，两全其美。当然，你也可以去读两三遍，第一遍读的时候知识粗略了解，能够做到如果遇到问题知道在书中哪部分寻找解答方法即可，这就可以算作第二遍去阅读。第三遍阅读则是你遇到的问题多得已经几乎让你将书中提到的算法全部实现过了，这时候你再去阅读一遍目的是将整个知识体系梳理一遍，因为这时候估计你已经能够做到遇到1马上知道1后面跟的是2了。这两种阅读方法无法比较好坏，只能说面对的情况不同，选择的阅读方法就不同。</li><li><strong>读书笔记的整理也很相似，当你第一遍阅读的时候可以只在书上画出你觉得重要的部分</strong>。不要急于去整理，<u>原因</u>是当你第一次阅读某一领域的书籍时会遇见很多你不了解的知识，如果直接去整理摘抄的话，不说你会收获多少，时间成本就很大（我在整理这本书的读书笔记的时候就深刻意识到了这点，因为我可能只花一两个小时去阅读一章，但是会花掉我四五个小时去整理摘抄并且这个过程是十分枯燥无趣的，令人很难坚持完成），而且长时间不复习很容易会忘记，更令人难过的是你会认为自己将阅读这本书的时间算作是浪费掉的，这种挫败感至少对我来说是难以化解的，从而丧失了积极反馈，成就感荡然无存。真正该去整理的时间是在第二次复习的时候，因为当你结合你实际问题去阅读的时候，实际问题会给你提供一个角度，你会从这个角度看待这部分知识。同样你也会从这个角度去理解这部分知识，进而解决实际问题，这可能就是大佬说的每次阅读会学到不同的东西。</li></ol><p>最后评价一下这本书。看到扉页上写到2012年6月第一版，距离现在已经六年多了。不难想象这本书所涉及到的机器学习的知识在当时可以算作是十分前沿的技术，因为当时还没有深度学习的提出，AI领域的爆发式发展，梯度下降等机器学习算法还没像现在这般耳熟能详。如果我能在那个年代读到这样一本书，那么我一定是最”前卫”的程序员。因此，我觉得这本书可以算作是推荐系统领域的经典入门必读书籍，这个称号当之无愧，国人能写出这种水平的书籍我感到无比自豪。书中引用很多论文并且介绍了这些论文中的算法，严谨细致，为对推荐领域更为感兴趣的读者提供了丰富的扩展阅读材料，并且介绍的众多算法无一不是当时最为流行的推荐系统算法，这一点从作者所引用论文的发表时间就可以看出。由浅入深，先介绍简单算法再一步步加以改进，使读者更加容易消化理解。同时书中也加入了作者自己的观点以及经验，干货十足。然而令人可惜地是本书没有第二版，书中介绍的Amazon和Twitter的一些例子在如今看来早已过时，国内也出现了以推荐系统为核心技术的独角兽公司例如今日头条等。</p><p>但瑕不掩瑜，我仍然以读到此书为我的一大幸事。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;后记&quot;&gt;&lt;a href=&quot;#后记&quot; class=&quot;headerlink&quot; title=&quot;后记&quot;&gt;&lt;/a&gt;后记&lt;/h1&gt;&lt;p&gt;本书着重介绍了推荐系统的各种算法设计和系统设计的方法，并且利用一些公开的数据集离线评测了各种算法。对于无法通过离线评测知道算法性能的情况，本书引用了很多著名的用户调查实验来比较不同的算法。&lt;/p&gt;
&lt;p&gt;但需要申明的一点是，本书的很多离线实验都是在一两个数据集上完成的，所以本书得到的所有结论都不是定论，可能换一个数据集就会得到完全相反的结论。这主要是因为不同网站中的用户行为有很大的差异，所以推荐系统很难有放之四海而皆准的结论。因此，需要在自己的数据集上重复本书介绍的实验和算法，再得到审核自己具体情况的结论。&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实践读书笔记（八）</title>
    <link href="https://rilzob.com/2018/12/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89/"/>
    <id>https://rilzob.com/2018/12/01/推荐系统实践读书笔记（八）/</id>
    <published>2018-12-01T06:29:45.701Z</published>
    <updated>2019-03-24T03:32:19.070Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第8章-评分预测问题"><a href="#第8章-评分预测问题" class="headerlink" title="第8章 评分预测问题"></a>第8章 评分预测问题</h1><p>之前讨论的都是TopN推荐，但同样评分预测问题也是推荐系统研究的核心。<strong>评分预测问题</strong>就是如何通过已知的用户历史评分记录预测未知的用户评分记录。</p><p>本章主要讨论评分预测这一推荐领域的经典问题。因为这一问题的研究集中在学术界，所以本章的介绍也比较偏学术，相对前面各章会增加一些公式和理论的讨论。</p><a id="more"></a><h2 id="8-1-离线实验方法"><a href="#8-1-离线实验方法" class="headerlink" title="8.1 离线实验方法"></a>8.1 离线实验方法</h2><p>评分预测问题基本都通过<strong>离线实验</strong>进行研究。在给定用户评分数据集后，研究人员会将数据集按照一定的方式分成训练集和测试集，然后根据测试集建立用户兴趣模型来预测测试集中的用户评分。对于测试集中的一对用户和物品$(u, i)$，用户$u$对物品$i$的真实评分$r_{ui}$，而推荐算法预测的用户$u$对物品$i$的评分为$\hat r_{ui}$，那么一般可以用<strong>均方根误差RMSE</strong>度量预测的精度：<br>$$<br>RMSE = \frac{\sqrt{\sum_{(u,i)\in T}(r_{ui} - \hat r_{ui})^2}}{\vert Test \vert}<br>$$<br>评分预测的<u>目的</u>就是找到最好的模型最小化测试集的RMSE。</p><p><u>关于如何划分训练集和测试集</u>，如果是和时间无关的预测任务，可以以均匀分布随机划分数据集，即对每个用户，随机选择一些评分记录作为测试集，剩下的记录作为测试集。如果是和时间相关的任务，那么需要将用户的旧行为作为训练集，将用户的新行为作为测试集。Netflix通过如下方式划分数据集，首先将每个用户的评分记录按照从早到晚进行排序，然后将用户最后10%的评分记录作为测试集，前90%的评分记录作为训练集。</p><h2 id="8-2-评分预测算法"><a href="#8-2-评分预测算法" class="headerlink" title="8.2 评分预测算法"></a>8.2 评分预测算法</h2><p>本节从简单到复杂地介绍具有代表性的算法，并给出它们在Netflix数据集上的效果。</p><h3 id="8-2-1-平均值"><a href="#8-2-1-平均值" class="headerlink" title="8.2.1 平均值"></a>8.2.1 平均值</h3><p>最简单的评分预测算法是利用平均值预测用户对物品的评分的。下面各节分别介绍各种不同的平均值。</p><h4 id="1-全局平均值"><a href="#1-全局平均值" class="headerlink" title="1.全局平均值"></a>1.全局平均值</h4><p>在平均值里最简单的是<strong>全局平均值</strong>。它的<u>定义</u>为训练集中所有评分记录的评分平均值：<br>$$<br>\mu = \frac{\sum_{(u,i) \in Train}r_{ui}}{\sum_{(u,i) \in Train }1}<br>$$<br>而最终的预测函数可以直接定义为：<br>$$<br>\hat r_{ui} = \mu<br>$$</p><h4 id="2-用户评分平均值"><a href="#2-用户评分平均值" class="headerlink" title="2.用户评分平均值"></a>2.用户评分平均值</h4><p>用户$u$的评分平均值$\bar r_u$定义为用户$u$在训练集中所有评分的平均值：<br>$$<br>\bar r_u = \frac{\sum_{i \in N(u)}r_{ui}}{\sum_{i \in N(u)}1}<br>$$</p><p>而最终的预测函数可以定义为：<br>$$<br>\hat r_{ui} = \bar r_{u}<br>$$</p><h4 id="3-物品评分平均值"><a href="#3-物品评分平均值" class="headerlink" title="3.物品评分平均值"></a>3.物品评分平均值</h4><p>物品$i$的评分平均值$\bar r_i$定义为物品$i$在训练集中接收的所有评分的平均值：<br>$$<br>\bar r_i = \frac{\sum_{u \in N(i)}r_{ui}}{\sum_{u \in N(i)}1}<br>$$<br>而最终的预测函数可以定义为：<br>$$<br>\hat r_{ui} = \bar r_i<br>$$</p><h4 id="4-用户分类对物品分类的平均值-类类平均值"><a href="#4-用户分类对物品分类的平均值-类类平均值" class="headerlink" title="4.用户分类对物品分类的平均值(类类平均值)"></a>4.用户分类对物品分类的平均值(类类平均值)</h4><p>假设有两个分类函数，一个是用户分类函数$\phi$，一个是物品分类函数$\varphi$。$\phi(u)$定义了用户$u$所属的类，$\varphi(u)$定义了物品$i$所属的类。那么，我们可以利用训练集中同类用户对同类物品评分的平均值预测用户对物品的评分，即：<br>$$<br>\hat r_{ui} = \frac{\sum_{(v,j)\in Train, \phi(u)=\phi(v),\varphi(i)=\varphi(j)} r_{vj}}{\sum_{(v,j)\in Train, \phi(u)=\phi(v),\varphi(i)=\varphi(j)}1}<br>$$<br>前面提出的全局平均值，用户评分平均值和物品评分平均值都是类类平均值的一种特例。</p><ul><li>如果定义$\phi(u)=0,\varphi(i)=0$，那么$\hat r_{ui}$就是全局平均值。</li><li>如果定义$\phi(u) = u,\varphi(i) = 0$，那么$\hat r_{ui}$就是用户评分平均值。</li><li>如果定义$\phi(u) = 0,\varphi(i) = i$，那么$\hat r_{ui}$就是物品评分平均值。</li></ul><p>除了这3种特殊的平均值，在用户评分数据上还可以定义很多不同的分类函数。</p><ul><li><strong>用户和物品的平均分</strong>     对于一个用户，可以计算他的评分平均分。然后将所有用户按照<strong>评分平均分</strong>从小到大排序，并将用户按照平均分平均分成N类。物品也可以用同样的方式分类。</li><li><strong>用户活跃度和物品流行度</strong>     对于一个用户，将他评分的物品数量定义为他的活跃度。得到用户活跃度之后，可以将用户通过<strong>活跃度</strong>从小到大排序，然后平均分为N类。物品的流行度定义为给物品评分的用户数目，物品也可以按照流行度均匀分成N类。</li></ul><p>下面的Python代码给出了类类平均值的计算方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PredictAll</span><span class="params">(records, user_cluster, item_cluster)</span>:</span></span><br><span class="line">    total = dict() </span><br><span class="line">    count = dict() </span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> records:</span><br><span class="line">        <span class="keyword">if</span> r.test != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span> </span><br><span class="line">        gu = user_cluster.GetGroup(r.user) </span><br><span class="line">        gi = item_cluster.GetGroup(r.item) </span><br><span class="line">        basic.AddToMat(total, gu, gi, r.vote) </span><br><span class="line">        basic.AddToMat(count, gu, gi, <span class="number">1</span>) </span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> records:</span><br><span class="line">        gu = user_cluster.GetGroup(r.user) </span><br><span class="line">        gi = item_cluster.GetGroup(r.item)</span><br><span class="line">        average = total[gu][gi] / (<span class="number">1.0</span> * count[gu][gi] + <span class="number">1.0</span>) </span><br><span class="line">        r.predict = average</span><br></pre></td></tr></table></figure><p>在这段代码中，<code>user_cluster.GetGroup</code>函数接收一个用户ID，然后根据一定的算法返回用户的类别。<code>item_cluster.GetGroup</code>函数接收一个物品的ID，然后根据一定的算法返回物品的类别。<code>total[gu][gi]/count[gu][gi]</code>记录了第<code>gu</code>类用户给第<code>gi</code>类物品评分的平均分。</p><p>上文提到，<code>user_cluster</code>和<code>item_cluster</code>有很多不同的定义方式，下面的Python代码给出了不同的<code>user_cluster</code>和<code>item_cluster</code>定义方式。其中，<code>Cluster</code>是基类，对于任何用户和物品，它的<code>GetGroup</code>函数都返回0，因此如果<code>user_cluster</code>和<code>item_cluter</code>都是<code>Cluster</code>类型，那么最终的预测函数就是全局平均值。<code>IdCluster</code>的<code>GetGroup</code>函数接收一个ID，会返回这个ID，那么如果<code>user_cluster</code>是<code>Cluster</code>类型，而<code>item_cluster</code>是<code>IdCluster</code>类型，那么最终的预测函数给出的就是物品平均值。在MovieLens数据集上利用不同平均值方法计算RMSE，实验结果表明对用户使用<code>UserVoteCluster</code>，对物品采用<code>ItemVoteCluster</code>，可以获得最小的RMSE。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cluster</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,records)</span>:</span> </span><br><span class="line">        self.group = dict()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetGroup</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IdCluster</span><span class="params">(Cluster)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, records)</span>:</span></span><br><span class="line">        Cluster.__init__(self, records)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetGroup</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserActivityCluster</span><span class="params">(Cluster)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, records)</span>:</span></span><br><span class="line">        Cluster.__init__(self, records) </span><br><span class="line">        activity = dict() </span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> records:</span><br><span class="line">            <span class="keyword">if</span> r.test != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            basic.AddToDict(activity, r.user, <span class="number">1</span>) </span><br><span class="line">        k = <span class="number">0</span> </span><br><span class="line">        <span class="keyword">for</span> user, n <span class="keyword">in</span> sorted(activity.items(), \ key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">False</span>):</span><br><span class="line">            c = int((k * <span class="number">5</span>) / (<span class="number">1.0</span> * len(activity))) </span><br><span class="line">            self.group[user] = c </span><br><span class="line">            k += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetGroup</span><span class="params">(self, uid)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> uid <span class="keyword">not</span> <span class="keyword">in</span> self.group:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span> </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.group[uid]</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItemPopularityCluster</span><span class="params">(Cluster)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, records)</span>:</span></span><br><span class="line">        Cluster.__init__(self, records) </span><br><span class="line">        popularity = dict() </span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> records:</span><br><span class="line">            <span class="keyword">if</span> r.test != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            basic.AddToDict(popularity, r.item, <span class="number">1</span>) </span><br><span class="line">        k = <span class="number">0</span> </span><br><span class="line">        <span class="keyword">for</span> item, n <span class="keyword">in</span> sorted(popularity.items(), \ key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">False</span>):</span><br><span class="line">            c = int((k * <span class="number">5</span>) / (<span class="number">1.0</span> * len(popularity))) </span><br><span class="line">            self.group[item] = c </span><br><span class="line">            k += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetGroup</span><span class="params">(self, item)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> self.group:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span> </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.group[item]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserVoteCluster</span><span class="params">(Cluster)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, records)</span>:</span></span><br><span class="line">        Cluster.__init__(self, records) </span><br><span class="line">        vote = dict() </span><br><span class="line">        count = dict() </span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> records:</span><br><span class="line">            <span class="keyword">if</span> r.test != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            basic.AddToDict(vote, r.user, r.vote) </span><br><span class="line">            basic.AddToDict(count, r.user, <span class="number">1</span>) </span><br><span class="line">        k = <span class="number">0</span> </span><br><span class="line">        <span class="keyword">for</span> user, v <span class="keyword">in</span> vote.items():</span><br><span class="line">            ave = v / (count[user] * <span class="number">1.0</span>) </span><br><span class="line">            c = int(ave * <span class="number">2</span>) </span><br><span class="line">            self.group[user] = c</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetGroup</span><span class="params">(self, uid)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> uid <span class="keyword">not</span> <span class="keyword">in</span> self.group:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span> </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.group[uid]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItemVoteCluster</span><span class="params">(Cluster)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, records)</span>:</span></span><br><span class="line">        Cluster.__init__(self, records) </span><br><span class="line">        vote = dict() </span><br><span class="line">        count = dict() </span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> records:</span><br><span class="line">            <span class="keyword">if</span> r.test != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            basic.AddToDict(vote, r.item, r.vote) </span><br><span class="line">            basic.AddToDict(count, r.item, <span class="number">1</span>) </span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> item, v <span class="keyword">in</span> vote.items():</span><br><span class="line">            ave = v / (count[item] * <span class="number">1.0</span>) </span><br><span class="line">            c = int(ave * <span class="number">2</span>) </span><br><span class="line">            self.group[item] = c</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetGroup</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> self.group:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span> </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.group[item]</span><br></pre></td></tr></table></figure><h3 id="8-2-2-基于邻域的方法"><a href="#8-2-2-基于邻域的方法" class="headerlink" title="8.2.2 基于邻域的方法"></a>8.2.2 基于邻域的方法</h3><p> 基于用户的邻域算法和基于物品的邻域算法都可以应用到评分预测中。基于用户的邻域算法认为预测一个用户对一个物品的评分，需要参考和这个用户兴趣相似的用户对该物品的评分，即：<br>$$<br>\hat r_{ui} = \bar r_{u} + \frac{\sum_{v \in S(u,K) \cap N(i)}w_{uv}(r_{vi} - \bar r_v)}{\sum_{v \in S(u,K) \cap N(i)}\vert w_{uv} \vert}<br>$$<br>$S(u,K)$是和用户$u$兴趣最相似的$K$个用户的集合，$N(i)$是对物品$i$评过分的用户集合，$r_{vi}$是用户$v$对物品$i$的评分，$\bar r_v$是用户$v$对他评过分的所有物品评分的平均值。用户之间的相似度$w_{uv}$可以通过<strong>皮尔逊系数</strong>计算：<br>$$<br>w_{uv} = \frac{\sum_{i \in I}(r_{ui}-\bar r_u)\cdot (r_{vi} - \bar r_v)}{\sqrt{\sum_{i \in I} (r_{ui} - \bar r_u)^2 \sum_{i \in I} (r_{vi} - \bar r_v)^2}}<br>$$<br>下面的Python代码实现了用户相似度的计算和最终的预测函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UserSimilarity</span><span class="params">(records)</span>:</span></span><br><span class="line">    item_users = dict() </span><br><span class="line">    ave_vote = dict() </span><br><span class="line">    activity = dict() </span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> records:</span><br><span class="line">        addToMat(item_users, r.item, r.user, r.value)</span><br><span class="line">        addToVec(ave_vote, r.user, r.value)</span><br><span class="line">        addToVec(activity, r.user, <span class="number">1</span>) </span><br><span class="line">    ave_vote = &#123;x:y/activity[x] <span class="keyword">for</span> x,y <span class="keyword">in</span> ave_vote.items()&#125; </span><br><span class="line">    nu = dict() </span><br><span class="line">    W = dict() </span><br><span class="line">    <span class="keyword">for</span> i,ri <span class="keyword">in</span> item_users.items():</span><br><span class="line">        <span class="keyword">for</span> u,rui <span class="keyword">in</span> ri.items():</span><br><span class="line">            addToVec(nu, u, (rui - ave_vote[u])*(rui - ave_vote[u])) </span><br><span class="line">            <span class="keyword">for</span> v,rvi <span class="keyword">in</span> ri.items():</span><br><span class="line">                <span class="keyword">if</span> u == v:</span><br><span class="line">                    <span class="keyword">continue</span> </span><br><span class="line">                addToMat(W, u, v, \ (rui - ave_vote[u])*(rvi - ave_vote[v])) </span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> W:</span><br><span class="line">        W[u] = &#123;x:y/math.sqrt(nu[x]*nu[u]) <span class="keyword">for</span> x,y <span class="keyword">in</span> W[u].items()</span><br><span class="line">    <span class="keyword">return</span> W</span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">PredictAll</span><span class="params">(records, test, ave_vote, W, K)</span>:</span></span><br><span class="line">        user_items = dict() </span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> records:</span><br><span class="line">            addToMat(user_items, r.user, r.item, r.value) </span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> test:</span><br><span class="line">            r.predict = <span class="number">0</span> </span><br><span class="line">            norm = <span class="number">0</span> </span><br><span class="line">            <span class="keyword">for</span> v,wuv <span class="keyword">in</span> sorted(W[r.user].items(), \ key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:K]:</span><br><span class="line">                <span class="keyword">if</span> r.item <span class="keyword">in</span> user_items[v]:</span><br><span class="line">                    rvi = user_items[v][r.item]</span><br><span class="line">                    r.predict += wuv * (rvi - ave_vote[v]) </span><br><span class="line">                    norm += abs(wuv) </span><br><span class="line">            <span class="keyword">if</span> norm &gt; <span class="number">0</span>:</span><br><span class="line">                r.predict /= norm</span><br><span class="line">            r.predict += ave_vote[r.user]</span><br></pre></td></tr></table></figure><p>基于物品的邻域算法在预测用户$u$对物品$i$的评分时，会参考用户u对和物品i相似的其他物品的评分，即：<br>$$<br>\hat r_{ui} = \bar r_i + \frac{\sum_{j \in S(u,K) \cap N(u)}w_{ij} (r_{uj} - \bar r_i)}{\sum_{j \in S(u,K) \cap N(u)} \vert w_{ij} \vert}<br>$$<br>$S(i,K)$是和$i$最相似的物品集合，$N(u)$是用户$u$评过分的物品集合，$w_{ij}$是物品之间的相似度，$\bar r_i$是物品$i$的平均分。对于如何计算物品的相似度，Badrul Sarwar等在论文(参见Badrul Sarwar、George Karypis、Joseph Konstan和John Riedl的“Item-based Collaborative Filtering Recommendation Algorithms”（ACM 2001 Article，2001）)里做了详细的研究，文章比较了3种主要的相似度。</p><p>第一种是普通的<strong>余弦相似度(cosine similarity)</strong>:<br>$$<br>w_{ij} = \frac{\sum_{u \in U} r_{ui} \cdot r_{uj}}{\sqrt{\sum_{u \in U} r_{ui}^2 \sum_{u \in U} r_{uj}^2}}<br>$$<br>第二种是<strong>皮尔逊系数(pearson correlation)</strong>:<br>$$<br>w_{ij} = \frac{\sum_{u \in U}(r_{ui}-\bar r_i)\cdot (r_{uj} - \bar r_j)}{\sqrt{\sum_{u \in U} (r_{ui} - \bar r_i)^2 \sum_{u \in U} (r_{uj} - \bar r_j)^2}}<br>$$<br>第三种是被Sarwar称为<strong>修正的余弦相似度(adjust cosine similarity):</strong><br>$$<br>w_{ij} = \frac{\sum_{u \in U}(r_{ui}-\bar r_u)\cdot (r_{uj} - \bar r_u)}{\sqrt{\sum_{u \in U} (r_{ui} - \bar r_u)^2 \sum_{u \in U} (r_{uj} - \bar r_u)^2}}<br>$$<br>Sarwar利用MovieLens最小的数据集对3种相似度进行了对比 ，并将MAE作为评测指标。实验结果表明利用修正后的余弦相似度进行评分预测可以获得最优的MAE。不过需要说明的是，在一个数据集上的实验并不意味着在其他数据集上也能获得相同的结果。</p><h3 id="8-2-3-隐语义模型与矩阵分解模型"><a href="#8-2-3-隐语义模型与矩阵分解模型" class="headerlink" title="8.2.3 隐语义模型与矩阵分解模型"></a>8.2.3 隐语义模型与矩阵分解模型</h3><p>做机器学习和数据挖掘研究的人经常会看到下面的各种名词，即隐含类别模型(Latent Class Model)、隐语义模型(Latent Factor Model)、pLSA、LDA、Topic Model、Matrix Factorization、Factorized Model。</p><p>这些名词在本质上应该是同一种思想体系的不同扩展。在推荐系统领域，提的最多的就是<strong>潜语义模型和矩阵分解模型</strong>。这两个名词说的是一回事，就是<strong>如何通过降维的方法将评分矩阵补全</strong>。</p><p>用户的评分行为可以表示成一个评分矩阵$R$，其中$R[u][i]$就是用户$u$对物品$i$的评分。但是，用户不会对所有的物品评分， 所以这个矩阵里有很多元素都是空的， 这些空的元素称为<strong>缺失值(missing value)</strong>。因此，评分预测从某种意义上说就是填空，如果一个用户对一个物品没有评过分，那么推荐系统就要预测这个用户是否是否会对这个物品评分以及会评几分。</p><h4 id="1-传统的SVD分解"><a href="#1-传统的SVD分解" class="headerlink" title="1.传统的SVD分解"></a>1.传统的SVD分解</h4><p>一个空的矩阵有很多种补全方法，选择其中一种对矩阵扰动最小的补全方法。什么是对矩阵扰动最小？就是补全后矩阵的特征值和补全之前矩阵的特征值相差不大，就算是扰动比较小。所以，最早的矩阵分解模型就是从数学上的<strong>SVD(奇异值分解)</strong>开始的(参见Daniel Billsus和Michael J. Pazzani的“Learning Collaborative Information Filters”（1998）)。给定$m$个用户和$n$个物品，和用户对物品的评分矩阵$\mathbb R^{m \times n}$。首先需要对评分矩阵中的缺失值进行简单地补全，比如用全局平均值，或者用户/物品平均值补全，得到补全后的矩阵$R’$。接着，可以用SVD分解将$R’$分解成如下形式：<br>$$<br>R’ = U^TSV<br>$$<br>其中$U \in \mathbb R^{k \times m}$，$V \in \mathbb R^{k \times m}$是两个正交矩阵，$S \in \mathbb R^{k \times k}$是对角阵，对角线上的每一个元素都是矩阵的奇异值。为了对$R’$进行降维，可以取最大的$f$个奇异值组成对角矩阵$S_f$，并且找到这$f$个奇异值中每个值在$U$、$V$矩阵中对应的行和列，得到$U_f$、$V_f$，从而可以得到一个降维后的评分矩阵：<br>$$<br>R_f’ = U^T_f S_f V_f<br>$$<br>其中，$R_(u, i )$就是用户$u$对物品$i$评分的预测值。</p><p>SVD分解是早期推荐系统研究常用的矩阵分解方法，不过该方法具有以下<u>缺点</u>，因此很难在实际系统中应用。</p><ul><li>该方法首先需要用一个简单的方法补全稀疏评分矩阵。一般来说，推荐系统中的评分矩阵是非常稀疏的，一般都有95%以上的元素是缺失的。而一旦补全，评分矩阵就会变成一个稠密矩阵，从而使评分矩阵的存储需要非常大的空间，这种空间的需求在实际系统中是不可能接受的。</li><li>该方法依赖的SVD分解方法的计算复杂度很高，特别是在稠密的大规模矩阵上更是非常慢。</li></ul><h4 id="2-Simon-Funk的SVD分解"><a href="#2-Simon-Funk的SVD分解" class="headerlink" title="2.Simon Funk的SVD分解"></a>2.Simon Funk的SVD分解</h4><p>正是由于上面的两个缺点，SVD分解算法提出几年后在推荐系统领域都没有得到广泛的关注。直到2006年Netflix Prize开始后，Simon Funk在博客上公布了一个算法(称为<strong>Funk-SVD</strong>)(参见Simon Funk的博客，文章地址为<a href="http://sifter.org/~simon/journal/20061211.html" target="_blank" rel="noopener">http://sifter.org/~simon/journal/20061211.html</a> )，一下子引爆了学术界对矩阵分解类方法的关注。而且，Simon Funk的博客也成为了很多学术论文经常引用的对象。 Simon Funk 提出的矩阵分解方法后来被 Netflix Prize 的冠军Koren称为<strong>Latent Factor Model(简称为LFM)</strong>。</p><p>第3章曾经简单介绍过LFM在TopN推荐中的应用，因此这里不再详细介绍这一方面背后的思想。从矩阵分解的角度说，如果将评分矩阵$R$分解为两个低维矩阵相乘：<br>$$<br>\hat R = P^TQ<br>$$<br>其中$P \in \mathbb R^{f \times m}$和$Q \in \mathbb R^{f \times n}$是两个降维后的矩阵。 那么，对于用户$u$对物品$i$的评分的预测值$\hat R(u,i) = \hat r_{ui}$，可以通过如下公式计算：<br>$$<br>\hat r_{ui} = \sum_f{p_{uf} q_{if}}<br>$$<br>其中$p_{uf} = P(u,f)$，$q_{if} = Q(i,f) $。那么，Simon Funk的思想很简单：可以直接通过训练集中的观察值利用最小化RMSE学习$P$、$Q$矩阵。</p><p>Simon Funk认为，既然用RMSE作为评测指标，那么如果能找到合适的$P$、$Q$来最小化训练集的预测误差，那么应该也能最小化测试集的预测误差。因此，Simon Funk定义损失函数为：<br>$$<br>C(p,q) = \sum_{(u,i) \in Train}(r_{ui} - \hat r_{ui})^2 = \sum_{(u,i) \in Train}(r_{ui} - \sum_{f = 1}^F p_{uf}q_{if})^2<br>$$<br>直接优化上面的损失函数可能会导致学习的过拟合， 因此还需要加入防止过拟合项$\lambda(\Vert p_u \Vert^2 + \Vert q_i \Vert^2)$，其中$\lambda$是正则化参数，从而得到：<br>$$<br>C(p,q) = \sum_{(u,i) \in Train}(r_{ui} - \sum_{f = 1}^F p_{uf}q_{if})^2 + \lambda(\Vert p_u \Vert^2 + \Vert q_i \Vert^2)<br>$$<br>要最小化上面的损失函数，可以利用随机梯度下降法(参见<a href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Stochastic_gradient_descent</a> )。该算法是最优化理论里最基础的优化算法，它首先通过求参数的偏导数找到最速下降方向，然后通过迭代法不断地优化参数。下面介绍优化方法的数学推导。</p><p>上面定义的损失函数里有两组参数$p_{uf}$和$q_{if}$，最速下降法需要首先对它们分别求偏导数，可以得到：<br>$$<br>\frac{\partial C}{\partial p_{uf}} = -2q_{ik} + 2\lambda p_{uk}<br>$$</p><p>$$<br>\frac{\partial C}{\partial p_{if}} = -2p_{uk} + 2\lambda q_{ik}<br>$$</p><p>然后，根据随机梯度下降法，需要将参数沿着最速下降方向向前推进，因此可以得到如下递推公式：<br>$$<br>p_{uf} = p_{uf} + \alpha(q_{ik} - \lambda p_{uk}) \\<br>q_{if} = q_{if} + \alpha(p_{uk} - \lambda q_{ik})<br>$$<br>其中，$\alpha$是学习速率(learning rate)，它的取值需要通过反复实验获得。</p><p>下面的代码实现了学习LFM模型时的迭代过程。在<code>LearningLFM</code>函数中，输入<code>train</code>是训练集中的用户评分记录，<code>F</code>是隐类的格式，<code>n</code>是迭代次数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LearningLFM</span><span class="params">(train, F, n, alpha, lambda)</span>:</span></span><br><span class="line">    [p,q] = InitLFM(train, F) </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">0</span>, n):</span><br><span class="line">        <span class="keyword">for</span> u,i,rui <span class="keyword">in</span> train.items():</span><br><span class="line">            pui = Predict(u, i, p, q) </span><br><span class="line">            eui = rui - pui </span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>,F):</span><br><span class="line">                p[u][k] += alpha * (q[i][k] * eui - <span class="keyword">lambda</span> * p[u][k]) </span><br><span class="line">                q[i][k] += alpha * (p[u][k] * eui - <span class="keyword">lambda</span> * q[i][k])</span><br><span class="line">        alpha *= <span class="number">0.9</span></span><br><span class="line">    <span class="keyword">return</span> list(p, q)</span><br></pre></td></tr></table></figure><p>如上面的代码所示，LearningLFM主要包括两步。(1)需要对P、Q矩阵进行初始化，(2)需要通过随机梯度下降法的迭代得到最终的$P$、$Q$矩阵。在迭代时，需要在每一步对学习参数$\alpha$进行衰减(alpha *= 0.9)，这是随机梯度下降法算法要求的，其目的是使算法尽快收敛。如果形象一点说就是，如果需要在一个区域找到极值，一开始可能需要大范围搜索，但随着搜索的进行，搜索范围会逐渐缩小。</p><p>初始化$P、Q$矩阵的方法很多，一般都是将这两个矩阵用随机数填充，但随机数的大小还是有讲究的，根据经验，随机数需要和$1/sqrt(F)$成正比。下面的代码实现了初始化功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InitLFM</span><span class="params">(train, F)</span>:</span></span><br><span class="line">    p = dict() </span><br><span class="line">    q = dict() </span><br><span class="line">    <span class="keyword">for</span> u, i, rui <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> p:</span><br><span class="line">            p[u] = [random.random()/math.sqrt(F) \ <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,F)]</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> q:</span><br><span class="line">            q[i] = [random.random()/math.sqrt(F) \ <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,F)] </span><br><span class="line">    <span class="keyword">return</span> list(p, q)</span><br></pre></td></tr></table></figure><p>而预测用户$u$对物品$i$的评分可以通过如下代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Predict</span><span class="params">(u, i, p, q)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p[u][f] * q[i][f] <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>,len(p[u]))</span><br></pre></td></tr></table></figure><p>LFM提出之后获得了很大的成功，后来很多著名的模型都是通过对LFM修修补补获得的，下面的各节分别介绍一下改进LFM的各种方法。这些改进有些是对模型的改进，有些是将新的数据引入到模型当中。</p><h4 id="3-加入偏置项后的LFM"><a href="#3-加入偏置项后的LFM" class="headerlink" title="3.加入偏置项后的LFM"></a>3.加入偏置项后的LFM</h4><p>上节提出的LFM预测公式通过隐类将用户和物品联系在一起。但是，实际情况下，一个评分系统有些<em>固有属性和用户物品无关</em>，而<em>用户也有些属性和物品无关</em>，<em>物品也有些属性和用户无关</em>。因此， Netflix Prize中提出了另一种LFM，其预测公式如下：<br>$$<br>\hat r_{ui} = \mu + b_u + b_i + p_u^T \cdot q_i<br>$$<br>相比上节的LFM预测公式增加了3项$\mu$、$b_u$、$b_i$。本章将这个模型称为BiasSVD。新增三项的作用如下：</p><ul><li><strong>$\mu$</strong> 训练集中所有记录的评分的全局平均数。在不同网站中，因为网站定位和销售的物品不同，网站的整体评分分布也会显示出一些差异。比如有些网站中的用户就是喜欢打高分，而另一些网站的用户就是喜欢打低分。而全局平均数可以表示网站本身对用户评分的影响。</li><li><strong>$b_u$</strong> 用户偏置(user bias)项。这一项表示了用户的评分习惯中和物品没有关系的那种因素。有的用户对什么都喜欢打高分或者低分。</li><li><strong>$b_i$</strong>  物品偏置(item bias)项。这一项表示了物品接受的评分中和用户没有什么关系的因素。与用户偏置项同理。</li></ul><p>增加的3个参数中，只有$b_u$、$b_i$是要通过<strong>机器学习</strong>训练出来的。同样可以求导，然后用<strong>梯度下降法</strong>求解这两个参数，只需对LearningLFM稍做修改，就可以支持BiasLFM模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LearningBiasLFM</span><span class="params">(train, F, n, alpha, lambda, mu)</span>:</span></span><br><span class="line">    [bu, bi, p,q] = InitLFM(train, F) </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">0</span>, n):</span><br><span class="line">        <span class="keyword">for</span> u,i,rui <span class="keyword">in</span> train.items():</span><br><span class="line">            pui = Predict(u, i, p, q, bu, bi, mu) </span><br><span class="line">            eui = rui - pui </span><br><span class="line">            bu[u] += alpha * (eui - <span class="keyword">lambda</span> * bu[u]) </span><br><span class="line">            bi[i] += alpha * (eui - <span class="keyword">lambda</span> * bi[i]) </span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>,F):</span><br><span class="line">                p[u][k] += alpha * (q[i][k] * eui - <span class="keyword">lambda</span> * p[u][k]) </span><br><span class="line">                q[i][k] += alpha * (p[u][k] * eui - <span class="keyword">lambda</span> * q[i][k])</span><br><span class="line">        alpha *= <span class="number">0.9</span> </span><br><span class="line">    <span class="keyword">return</span> list(bu, bi, p, q)</span><br></pre></td></tr></table></figure><p>而$b_u$、$b_i$在一开始只要初始化成全0的向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InitBiasLFM</span><span class="params">(train, F)</span>:</span></span><br><span class="line">    p = dict() </span><br><span class="line">    q = dict() </span><br><span class="line">    bu = dict() </span><br><span class="line">    bi = dict() </span><br><span class="line">    <span class="keyword">for</span> u, i, rui <span class="keyword">in</span> train.items():</span><br><span class="line">        bu[u] = <span class="number">0</span> </span><br><span class="line">        bi[i] = <span class="number">0</span> </span><br><span class="line">        <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> p:</span><br><span class="line">            p[u] = [random.random()/math.sqrt(F) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,F)] </span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> q:</span><br><span class="line">            q[i] = [random.random()/math.sqrt(F) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,F)] </span><br><span class="line">    <span class="keyword">return</span> list(p, q)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Predict</span><span class="params">(u, i, p, q, bu, bi, mu)</span>:</span></span><br><span class="line">    ret = mu + bu[u] + bi[i] </span><br><span class="line">    ret += sum(p[u][f] * q[i][f] <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>,len(p[u])) </span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><h4 id="4-考虑邻域影响的LFM"><a href="#4-考虑邻域影响的LFM" class="headerlink" title="4.考虑邻域影响的LFM"></a>4.考虑邻域影响的LFM</h4><p>前面的LFM模型中并没有显式地考虑用户的历史行为对用户评分预测的影响。为此，Koren在Netflix Prize比赛中提出了一个模型(参见Yehuda Koren的“Factor in the Neighbors: Scalable and Accurate Collaborative Filtering”（ACM 2010 Article，2010）)，将用户历史评分的物品加入到了LFM模型中，Koren将该模型称为SVD++。</p><p>为了将基于邻域的方法设计成一个像LFM那样可以学习的模型，可以将ItemCF的预测算法改成如下形式：<br>$$<br>\hat r_{ui} = \frac{1}{\sqrt{\vert N(u) \vert}} \sum_{j \in N(u)}w_{ij}<br>$$<br>公式中的$w_{ij}$不再是根据ItemCF算法计算出的物品相似度矩阵，而是一个和P、Q一样的参数，它通过优化如下的损失函数进行优化：<br>$$<br>C(w) = \sum_{(u,i) \in Train}(r_{ui} - \sum_{j \in N(u)} w_{ij}r_{uj})^2 + \lambda w_{ij}^2<br>$$<br>这个模型有一个缺点，就是$w$将是一个比较稠密的矩阵，存储它需要比较大的空间。此外，如果有$n$个物品，那么该模型的参数个数就是$n^2$个，这个参数个数比较大容易造成结果的<strong>过拟合</strong>。因此，Koren提出用该对$w$矩阵也进行分解，将参数个数降低到$2 \times n \times F$个，模型如下：<br>$$<br>\hat r_{ui} = \frac{1}{\sqrt{\vert N(u) \vert}} \sum_{j \in N(u)}x_i^Ty_j = \frac{1}{\sqrt{\vert N(u) \vert}}x_i^T \sum_{j \in N(u)}y_j<br>$$<br>$x_i$、$y_j$是两个$F$维的向量。由此可见，该模型用$x_i^Ty_j$代替了$w_{ij}$，从而大大降低了参数的数量和存储空间。</p><p>再进一步，可以将前面的LFM和上面的模型相加，从而得到如下模型：<br>$$<br>\hat r_{ui} = \mu + b_u + b_i + p_u^T \cdot q_i + \frac{1}{\sqrt{\vert N(u) \vert}}x_i^T \sum_{j \in N(u)}y_j<br>$$<br>Koren又提出为了不增加太多参数造成过拟合，可以令$x=q$，从而得到最终的SVD++模型：<br>$$<br>\hat r_{ui} = \mu + b_u + b_i + q_i^T \cdot (p_u + \frac{1}{\sqrt{\vert N(u) \vert}} \sum_{j \in N(u)}y_j)<br>$$<br>通过将损失函数对各个参数求偏导数，也可以轻松推导出迭代公式。下面给出SVD++模型训练的实现代码，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LearningBiasLFM</span><span class="params">(train_ui, F, n, alpha, lambda, mu)</span>:</span></span><br><span class="line">    [bu, bi, p, q, y] = InitLFM(train, F) </span><br><span class="line">    z = dict() </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">0</span>, n): </span><br><span class="line">        <span class="keyword">for</span> u,items <span class="keyword">in</span> train_ui.items(): </span><br><span class="line">            z[u] = p[u] </span><br><span class="line">            ru = <span class="number">1</span> / math.sqrt(<span class="number">1.0</span> * len(items)) </span><br><span class="line">            <span class="keyword">for</span> i,rui <span class="keyword">in</span> items items(): </span><br><span class="line">                <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>,F): </span><br><span class="line">                    z[u][f] += y[i][f] * ru </span><br><span class="line">            sum = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,F)] </span><br><span class="line">            <span class="keyword">for</span> i,rui <span class="keyword">in</span> items items(): </span><br><span class="line">                pui = Predict() </span><br><span class="line">                eui = rui - pui </span><br><span class="line">                bu[u] += alpha * (eui - <span class="keyword">lambda</span> * bu[u]) </span><br><span class="line">                bi[i] += alpha * (eui - <span class="keyword">lambda</span> * bi[i]) </span><br><span class="line">                <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>,F): </span><br><span class="line">                    sum[k] += q[i][k] * eui * ru </span><br><span class="line">                    p[u][k] += alpha * (q[i][k] * eui - <span class="keyword">lambda</span> * p[u][k]) </span><br><span class="line">                    q[i][k] += alpha * ((z[u][k] + p[u][k]) * eui - <span class="keyword">lambda</span> * q[i][k]) </span><br><span class="line">            <span class="keyword">for</span> i,rui <span class="keyword">in</span> items items():</span><br><span class="line">                <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>,F):</span><br><span class="line">                    y[i][f] += alpha * (sum[f] - <span class="keyword">lambda</span> * y[i][f]) </span><br><span class="line">        alpha *= <span class="number">0.9</span> </span><br><span class="line">    <span class="keyword">return</span> list(bu, bi, p, q)</span><br></pre></td></tr></table></figure><h3 id="8-2-4-加入时间信息"><a href="#8-2-4-加入时间信息" class="headerlink" title="8.2.4 加入时间信息"></a>8.2.4 加入时间信息</h3><p>利用时间信息的方法也主要分为两种，一种是<em>将时间信息应用到基于邻域的模型中</em>，另一种是<em>将时间信息应用到矩阵分解模型中</em>。</p><h4 id="1-基于邻域的模型融合时间信息"><a href="#1-基于邻域的模型融合时间信息" class="headerlink" title="1.基于邻域的模型融合时间信息"></a>1.基于邻域的模型融合时间信息</h4><p>由于实际生活中用户数目太大，所以基于用户的邻域模型很少被使用，主要是因为存储用户相似度矩阵非常困难。因此，本节主要讨论如何将时间信息融合到基于物品的邻域模型中。</p><p>Netflix Prize 的参赛队伍 BigChaos在技术报告中提到了一种融入时间信息的基于邻域的模型，本节将这个模型称为TItemCF。该算法通过如下公式预测用户在某一个时刻会给物品什么评分：<br>$$<br>\hat r_{uit} = \frac{\sum_{j \in N(u) \cap S(i,K)}f(w_{ij}, \Delta t)r_{uj}}{\sum_{j \in N(u) \cap S(i,K)}f(w_{ij},\Delta t)}<br>$$<br>$\Delta t = t_{ui} - t_{uj}$是用户$u$对物品$i$和物品$j$评分的时间差，$w_{ij}$是物品$i$和$j$的相似度，$f(w_{ij}, \Delta t)$是一个考虑了时间衰减后的相似度函数，它的<u>主要目的</u>是提高用户最近的评分行为对推荐结果的影响，BigChaos在模型中采用了如下的$f$：<br>$$<br>f(w_{ij},\Delta t) = \sigma(\delta \cdot w_{ij} \cdot \text{exp}(\frac{-|\Delta t|}{\beta}) + \gamma) \\<br>\sigma(x) = \frac{1}{1+ \text{exp}(-x)}<br>$$<br>$\sigma(x)$是<strong>sigmoid函数</strong>，它的<u>目的</u>是将相似度压缩到(0，1)区间中。从上面的定义可以发现，随着$\Delta t$增加，$f(w_{ij}, \Delta t)$会越来越小，也就是说用户很久之前的行为对预测用户当前评分的影响越来越小。</p><h4 id="2-基于矩阵分解的模型融合时间信息"><a href="#2-基于矩阵分解的模型融合时间信息" class="headerlink" title="2.基于矩阵分解的模型融合时间信息"></a>2.基于矩阵分解的模型融合时间信息</h4><p>在引入时间信息后，用户评分矩阵不再是一个二维矩阵，而是变成了一个三维矩阵。不过可以仿照二维矩阵的方式对三维矩阵进行分解(参见Liang Xiang和Qing Yang的“Time-Dependent Models in Collaborative Filtering Based Recommender S WI-IAT 09)。回顾之前的BiasSVD模型：<br>$$<br>\hat r_{ui} = \mu + b_u + b_i + p^T_u \cdot q_i<br>$$<br>$\mu$可以看做对二维矩阵的零维分解，$b_u$、$b_i$可以看做对二维矩阵的一维分解，而$p^T_u \cdot q_i$则看做对二维矩阵的二维分解。仿照这种分解，将用户-物品-时间三维矩阵如下分解：<br>$$<br>\hat r_{uit} = \mu + b_u +b_i + b_t+ p^T_u \cdot q_i + x^T_u \cdot y_t + s^T_i z_t + \sum_f g_{u,f} h_{i,f}l_{t,f}<br>$$<br>这里$b_t$建模了系统整体平均分随时间变化的效应，$x^T_u \cdot y_t$建模了用户平均分随时间变化的效应，$s^T_i z_t$建模了物品平均分随时间变化的效应，而$\sum_f g_{u,f} h_{i,f}l_{t,f}$建模了用户兴趣随时间影响的效应。这个模型也可以很容易地利用前面提出的随机梯度下降法进行训练。本章将这个模型记为TSVD。</p><p>Koren在SVD++模型的基础上也引入了时间效应(参见Yehuda Koren的“Collaborative Filtering with temporal dynamics”（ACM 2009 Article，2009）)，回顾一下SVD++模型：<br>$$<br>\hat r_{ui} = \mu + b_u + b_i + q_i^T \cdot (p_u + \frac{1}{\sqrt{\vert N(u) \vert}} \sum_{j \in N(u)}y_j)<br>$$<br>对这个模型做如下改进以融合时间信息：<br>$$<br>\hat r_{ui} = \mu + b_u(t) + b_i(t) + q_i^T \cdot (p_u(t) + \frac{1}{\sqrt{\vert N(u) \vert}} \sum_{j \in N(u)}y_j) \\<br>b_u(t) = b_u + \alpha_u \cdot dev_u(t) + b_{ut} + b_{u,period(t)} \\<br>dev_u(t) = sign(t - t_u) \cdot \vert t - t_u \vert^\beta \\<br>b_i(t) = b_i + b_{it} + b_{i, period(t)}\\<br>p_{uf}(t) = p_{uf} + p_{utf}<br>$$<br>这里$t_u$是用户所有评分的平均时间。$period(t)$考虑了季节效应，可以定义为时刻$t$所在的月份。该模型同样可以通过随机梯度下降法进行优化。</p><h3 id="8-2-5-模型融合"><a href="#8-2-5-模型融合" class="headerlink" title="8.2.5 模型融合"></a>8.2.5 模型融合</h3><p>Netflix Prize的最终获胜队伍通过融合上百个模型的结果才取得了最终的成功。由此可见<strong>模型融合</strong>对提高评分预测的精度至关重要。本节讨论模型融合的两种不同技术。</p><h4 id="1-模型级联融合"><a href="#1-模型级联融合" class="headerlink" title="1.模型级联融合"></a>1.模型级联融合</h4><p>假设已经有一个预测器$\hat r^{(k)}$，对于每个用户-物品对$(u,i)$都给出预测值，那么可以在这个预测器的基础上设计骗一个预测期$\hat r^{(k + 1)}$来最小化损失函数：</p><p>$$<br>C = \sum_{(u,i) \in Train}(r_{ui} - \hat r_{ui}^{(k)} - \hat r_{ui}^{(k+1)})<br>$$<br>由上面的描述可以发现，级联融合很像<strong>Adaboost算法</strong>。和Adaboost算法类似，该方法每次产生一个新模型，按照一定的参数加到旧模型上去，从而使训练集误差最小化。不同的是，这里每次生成新模型时并不对样本集采样，针对那些预测错的样本，而是每次都还是利用全样本集进行预测，但每次使用的模型都有区别。</p><p>一般来说，级联融合的方法都用于简单的预测器，比如前面提到的平均值预测器。下面的Python代码实现了利用平均值预测器进行级联融合的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Predict</span><span class="params">(train, test, alpha)</span>:</span></span><br><span class="line">    total = dict() </span><br><span class="line">    count = dict() </span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> train:</span><br><span class="line">        gu = GetUserGroup(record.user) </span><br><span class="line">        gi = GetItemGroup(record.item) </span><br><span class="line">        AddToMat(total, gu, gi, record.vote - record.predict) </span><br><span class="line">        AddToMat(count, gu, gi, <span class="number">1</span>) </span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> test:</span><br><span class="line">        gu = GetUserGroup(record.user) </span><br><span class="line">        gi = GetUserGroup(record.item) </span><br><span class="line">        average = total[gu][gi] / (<span class="number">1.0</span> * count[gu][gi] + alpha) </span><br><span class="line">        record.predict += average</span><br></pre></td></tr></table></figure><p>通过在MovieLens数据集计算对平均值方法采用级联融合后的RMSE，可见即使是利用简单的算法进行级联融合，也能得到比较低的评分预测误差。</p><h4 id="2-模型加权融合"><a href="#2-模型加权融合" class="headerlink" title="2.模型加权融合"></a>2.模型加权融合</h4><p>假设有$K$个不同的预测器${\hat r^{(1)}，\hat r^{(2)}，···，\hat r^{(K)}}，本节主要讨论如何将它们融合起来获得最低的预测误差。</p><p>最简单的融合算法就是<strong>线性融合</strong>，即最终的预测器$\hat r$是这$K$个预测器的线性加权：<br>$$<br>\hat r = \sum_{k = 1}^K \alpha_k \hat r^{(k)}<br>$$<br>一般来说，评分预测问题的解决需要在训练集上训练$K$个不同的预测器，然后在测试集上作出预测。但是，如果我们继续在训练集上融合$K$个预测器，得到线性加权系数，就会造成过拟合的问题。因此，在模型融合时一般采用如下方法。</p><ul><li>假设数据集已经被分为了训练集A和测试集B，那么首先需要将训练集A按照相同的分割方法分为A1和A2，其中A2的生成方法和B的生成方法一致，且大小相似。</li><li>在A1上训练$K$个不同的预测器，在A2上作出预测。因为我们知道A2上的真实评分值，所以可以在A2上利用<strong>最小二乘法</strong>计算出线性融合系数$\alpha_k$。</li><li>在A上训练$K$个不同的预测器，在B上作出预测，并且将这$K$个预测器在B上的预测结果按照已经得到的线性融合系数加权融合，以得到最终的预测结果。</li></ul><p>除了线性融合，还有很多复杂的融合方法，比如<strong>利用人工神经网络的融合算法</strong>。其实，模型融合问题就是一个典型的回归问题，因此所有的回归算法都可以用于模型融合。</p><h3 id="8-2-6-Netflix-Prize的相关实验结果"><a href="#8-2-6-Netflix-Prize的相关实验结果" class="headerlink" title="8.2.6 Netflix Prize的相关实验结果"></a>8.2.6 Netflix Prize的相关实验结果</h3><p>Netflix Prize比赛的３年时间里，很多研究人员在同一个数据集上重复实验了前面几节提到的各种算法。本节引用他们的实验结果对比各个算法的性能。Netflix Prize采用RMSE评测预测准确度，因此本节的评测指标也是RMSE，具体见表8-4。</p><p><img src="/2018/12/01/推荐系统实践读书笔记（八）/著名算法的RMSE.png" alt="著名算法的RMSE"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第8章-评分预测问题&quot;&gt;&lt;a href=&quot;#第8章-评分预测问题&quot; class=&quot;headerlink&quot; title=&quot;第8章 评分预测问题&quot;&gt;&lt;/a&gt;第8章 评分预测问题&lt;/h1&gt;&lt;p&gt;之前讨论的都是TopN推荐，但同样评分预测问题也是推荐系统研究的核心。&lt;strong&gt;评分预测问题&lt;/strong&gt;就是如何通过已知的用户历史评分记录预测未知的用户评分记录。&lt;/p&gt;
&lt;p&gt;本章主要讨论评分预测这一推荐领域的经典问题。因为这一问题的研究集中在学术界，所以本章的介绍也比较偏学术，相对前面各章会增加一些公式和理论的讨论。&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实践读书笔记（七）</title>
    <link href="https://rilzob.com/2018/11/30/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89/"/>
    <id>https://rilzob.com/2018/11/30/推荐系统实践读书笔记（七）/</id>
    <published>2018-11-30T10:38:04.769Z</published>
    <updated>2019-03-24T03:32:02.803Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第7章-推荐系统实例"><a href="#第7章-推荐系统实例" class="headerlink" title="第7章 推荐系统实例"></a>第7章 推荐系统实例</h1><p>前面几章介绍了各种各样的数据和基于这些数据的推荐算法。在实际系统中，前面几章提到的数据大都存在，因此如何设计一个真实的推荐系统处理不同的数据，根据不同的数据设计算法，并将这些算法融合到一个系统当中是本章讨论的主要问题。</p><p>本章首先介绍推荐系统的外围架构，然后介绍推荐系统的架构，并对架构中每个模块的设计进行深入讨论。<br><a id="more"></a></p><h2 id="7-1-外围架构"><a href="#7-1-外围架构" class="headerlink" title="7.1 外围架构"></a>7.1 外围架构</h2><p>这一节主要讨论推荐系统是如何和网站的其他系统接口的。</p><p><img src="/2018/11/30/推荐系统实践读书笔记（七）/推荐系统和其他系统之间的关系.png" alt="推荐系统和其他系统之间的关系"></p><p><strong>UI系统</strong>负责给用户展示网页并和用户交互。网站会通过日志系统将用户在UI上的各种各样的行为记录到<strong>用户行为日志</strong>中。日志可能存储在内存缓存里，也可能存储在数据库中，也可能存储在文件系统中。而推荐系统通过分析用户的行为日志，给用户生成推荐列表，最终展示到网站的界面上。因此可以发现，推荐系统要发挥强大的作用，除了推荐系统本身，主要还依赖于两个条件——<em>界面展示</em>和<em>用户行为数据</em>。</p><p>目前流行的推荐系统界面存在一些共性：</p><ul><li>通过一定方式展示物品，主要包括物品的标题、缩略图和介绍等。</li><li>很多推荐界面都提供了推荐理由，理由可以增加用户对推荐结果的信任度。</li><li>推荐界面还需要提供一些按钮让用户对推荐结果进行反馈，这样才能让推荐算法不断改 善用户的个性化推荐体验。</li></ul><p>在设计推荐界面时，可以综合考虑其他网站的设计并结合自己网站的特点。</p><h3 id="数据收集和存储"><a href="#数据收集和存储" class="headerlink" title="数据收集和存储"></a>数据收集和存储</h3><p>从产生行为的用户角度看，有些行为是只有注册用户才能产生的，而有些行为是所有用户都可以产生的。从规模上看，浏览网页、搜索记录的规模都很大，因为这种行为所有用户都能产生，而且平均每个用户都会产生很多这些行为。购买、收藏行为规模中等，因为只有注册用户才能产生这种行为，但购买行为又是电商网站的主要行为，所以它们相对于评论来说规模更大，但相对于网页浏览行为来说规模要小得多，最后剩下的行为是注册用户里的一小部分人才有的，所以规模不会很大。同样有些行为需要实时存取，而有些并不需要。</p><p>按照前面数据的规模和是否需要实时存取，不同的行为数据将被存储在不同的媒介中。一般来说，需要实时存取的数据存储在数据库和缓存中，而大规模的非实时地存取数据存储在分布式文件系统（如HDFS）中。</p><p>数据能否实时存取在推荐系统中非常重要，因为推荐系统的实时性主要依赖于能否实时拿到用户的新行为。只有快速拿到大量用户的新行为，推荐系统才能够实时地适应用户当前的需求，给用户进行实时推荐。</p><h2 id="7-2-推荐系统架构"><a href="#7-2-推荐系统架构" class="headerlink" title="7.2 推荐系统架构"></a>7.2 推荐系统架构</h2><p>推荐系统联系用户和物品的方式主要有3种，在第4章开头部分介绍过，分别是：</p><ul><li>基于用户的推荐算法</li><li>基于物品的推荐算法</li><li>基于特征的推荐算法</li></ul><p>其中上述三种都可以将其抽象为基于特征的推荐算法，因为用户喜欢的物品可以算是用户特征，同样与用户兴趣相似的其他用户也是一种用户特征。然后根据抽象设计一种基于特征的推荐系统架构。当用户到来之后，推荐系统需要为用户生成特征，然后对每个特征找到和特征相关的物品，从而最终生成用户的推荐列表。因而，推荐系统的<u>核心任务</u>就被拆解成两部分，一个是<em>如何为给定用户生成特征</em>，另一个是<em>如何根据特征找到物品</em>。</p><p>用户特征种类很多，主要包括如下几类：</p><ul><li><strong>人口统计学特征</strong></li><li><strong>用户行为特征</strong></li><li><strong>用户的话题特征</strong>   可以根据用户的历史行为利用<strong>话题模型(topic model)</strong>将电视剧和电影聚合成不同的话题，并且计算出每个用户对什么话题感兴趣。 </li></ul><p>同时推荐系统的推荐任务也有很多种，如果要在一个系统中把上面提到的各种特征和任务都统筹考虑，那么系统将会非常复杂，而且很难通过配置文件方便地配置不同特征和任务的权重。因此，推荐系统需要由多个<strong>推荐引擎</strong>组成，每个推荐引擎负责一类特征和一种任务，而推荐系统的任务只是将推荐引擎的结果按照一定权重或者优先级合并、排序然后返回。</p><p>这样做有两个<u>好处</u>：</p><ul><li>可以方便地增加/删除引擎，控制不同引擎对推荐结果的影响。对于绝大多数需求，只需要通过不同的引擎组合实现。</li><li>可以实现推荐引擎级别的用户反馈。每一个推荐引擎其实代表了一种推荐策略，而不同的用户可能喜欢不同的推荐策略。可以将每一种策略都设计成一个推荐引擎，然后通过分析用户对推荐结果的反馈了解用户比较喜欢哪些引擎推荐出来的结果，从而对不同的用户给出不同的引擎组合权重。</li></ul><p>将推荐系统拆分成不同推荐引擎后，如何设计一个推荐引擎变成了推荐系统设计的<u>核心部分</u>。下一节讨论推荐引擎的设计方法。</p><h2 id="7-3-推荐引擎的架构"><a href="#7-3-推荐引擎的架构" class="headerlink" title="7.3 推荐引擎的架构"></a>7.3 推荐引擎的架构</h2><p>推荐系统架构主要包括3部分：</p><ul><li>该部分负责(1)从数据库或者缓存中拿到用户行为数据，通过(2)分析不同行为，(3)生成当前用户的特征向量。不过如果是使用非行为特征，就不需要使用行为提取和分析模块了。该模块的输出是用户特征向量。</li><li>该部分负责将用户的特征向量通过特征-物品相关矩阵转化为初始推荐物品列表。</li><li>该部分负责对初始的推荐列表进行过滤、排名等处理，从而生成最终的推荐结果。</li></ul><p><img src="/2018/11/30/推荐系统实践读书笔记（七）/推荐引擎的架构图.png" alt="推荐引擎的架构图"></p><p>下节对各个不同的部分分别详细解释。</p><h3 id="7-3-1-生成用户特征向量"><a href="#7-3-1-生成用户特征向量" class="headerlink" title="7.3.1 生成用户特征向量"></a>7.3.1 生成用户特征向量</h3><p>一般来说，用户的特征包括两种，一种是<em>用户的注册信息中可以提取出来的</em>，另一种特征主要是<em>从用户的行为中计算出来的</em>，本节着重讨论如何生成特征。</p><p>一个特征向量由特征以及特征的权重组成，在利用用户行为计算特征向量时需要考虑以下因素。</p><ul><li><strong>用户行为的种类</strong>   不同行为的影响不同，大多时候很难确定什么行为更加重要，一般的标准就是用户付出代价越大的行为权重越高。</li><li><strong>用户行为产生的时间</strong>   距离现在越近的行为权重越高。</li><li><strong>用户行为的次数</strong>    用户对同一个物品的同一种行为发生的次数也反映了用户对物品的兴趣，行为次数多的物品对应的特征权重越高。</li><li><strong>物品的热门程度</strong>    用户对热门物品的行为不能够反映用户的兴趣，而冷门物品则可以能够反映。推荐引擎在生成用户特征时会加重不热门物品对应的特征的权重。</li></ul><h3 id="7-3-2-特征—物品相关推荐"><a href="#7-3-2-特征—物品相关推荐" class="headerlink" title="7.3.2 特征—物品相关推荐"></a>7.3.2 特征—物品相关推荐</h3><p>在得到用户的特征向量后，可以根据<strong>离线相关表</strong>得到初始的物品推荐列表。离线相关表可以存储在MySQL中。<br>对于每个特征，我们可以在相关表中存储和它最相关的N个物品的ID。</p><p>在线使用的特征—物品相关表一般都不止一张。因为可能使用了不同的推荐算法。总之，对于一个推荐引擎可以在配置文件中配置很多相关表以及它们的权重，而在线服务在启动时会将这些相关表按照配置的权重相加，然后将最终的相关表保存在内存中，而在给用户进行推荐时，用的已经是加权后的相关表了。</p><p>特征—物品相关推荐模块还可以接受一个候选物品集合。候选物品集合的<u>目的</u>是保证推荐结果只包含候选物品集合中的物品。对推荐物品的范围进行限定。</p><p>书中对为什么不在过滤模块中将候选集合外的电视剧过滤掉，而要在相关推荐模块中处理候选物品列表作出了解释，不过我没有看明白，有机会再看一次这部分。</p><p>特征—物品相关推荐模块除了给用户返回物品推荐列表，还需要给推荐列表中的每个推荐结果产生一个解释列表，表明这个物品是因为哪些特征推荐出来的。下面的代码给出了相关推荐模块的大体工作流程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RecommendationCore</span><span class="params">(features, related_table)</span>:</span></span><br><span class="line">    ret = dict() </span><br><span class="line">    <span class="keyword">for</span> fid, fweight <span class="keyword">in</span> features.items() </span><br><span class="line">        <span class="keyword">for</span> item, sim <span class="keyword">in</span> related_table[fid].items():</span><br><span class="line">            ret[item].weight += sim * fweight</span><br><span class="line">            ret[item].reason[fid] = sim * fweight </span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><h3 id="7-3-3-过滤模块"><a href="#7-3-3-过滤模块" class="headerlink" title="7.3.3 过滤模块"></a>7.3.3 过滤模块</h3><p>在得到初步的推荐列表后，需要先按照产品需求对结果进行过滤，过滤掉不符合要求的物品，然后再把推荐列表展现给用户。</p><p>一般来说，过滤模块会过滤掉以下物品：</p><ul><li><strong>用户已经产生过行为物品</strong>   为了保证结果的新颖性</li><li><strong>候选物品以外的物品</strong>    候选物品集合一般有两个来源，一个是<em>产品需求</em>。另一个来源是<em>用户自己的选择</em>，过滤掉不满足用户所选条件的物品。</li></ul><h3 id="7-3-4-排名模块"><a href="#7-3-4-排名模块" class="headerlink" title="7.3.4 排名模块"></a>7.3.4 排名模块</h3><p>对推荐列表进行排名可以更好地提升用户满意度，一般排名模块需要包括很多不同的子模块，下面对不同的模块分别加以介绍。</p><h4 id="1-新颖性排名"><a href="#1-新颖性排名" class="headerlink" title="1.新颖性排名"></a>1.新颖性排名</h4><p>新颖性排名模块的<u>目的</u>是给用户尽量推荐他们不知道的、长尾中的物品。虽然前面的过滤模块已经过滤掉了用户曾经有过行为的物品，保证了一定程度的新颖性，但是用户在当前网站对某个物品没有行为并不代表用户不知道这个物品。</p><p>要准确了解用户是否已经知道某个物品是非常困难的，因此只能通过某种近似的方式知道，比如使用如下公式对推荐结果中热门的物品进行降权。<br>$$<br>p_{ui} = \frac{p_{ui}}{\log{(1 + \alpha \cdot popularity(i))}}<br>$$<br>不过，要实现推荐结果的新颖性，仅仅在最后对热门物品进行降权是不够的，而应在推荐引擎的各个部分考虑新颖性问题。</p><p>本章提到推荐系统架构主要是<strong>基于物品的推荐算法</strong>的，因此回顾一下基于物品的推荐算法的基本公式：<br>$$<br>p_{ui} = \sum_{j \in N(u) \cap S(i,K)} w_{ji} r_{uj}<br>$$<br>在上述公式中$j$是联系用户和推荐物品的特征。最终$p_{ui}$的大小主要取决于两个参数——$w_{ji}$和$r_{uj}$。其中，$r_{uj}$在通过用户行为生成用户特征向量时计算，而$w_{ji}$是离线计算的物品相似度。如果要提高推荐结果的新颖性，在计算这两个数时都要考虑新颖性。与上面同理对$r_{uj}$和$w_{ji}$进行降权。<br>$$<br>r_{uj} = \frac{r_{uj}}{\log(1+\alpha \cdot popularity(j))}<br>$$</p><p>$$<br>w_{ji} = \frac{w_{ji}}{\log(1+\alpha \cdot popularity(i))} (popularity(i) &gt; popularity(j))<br>$$</p><p>此外，也可以引入<strong>内容相似度矩阵</strong>，因为内容相似度矩阵中和每个物品相似的物品都不是很热门，所以引入内容相似度矩阵也能够提高最终推荐结果的新颖度。</p><h4 id="2-多样性"><a href="#2-多样性" class="headerlink" title="2.多样性"></a>2.多样性</h4><p>增加多样性可以让推荐结果覆盖尽可能多的用户兴趣。这里需要指出的是提高多样性并不是时时刻刻都很好。</p><p>第一种提高多样性的方法是将推荐结果按照某种物品的内容属性分成几类，然后在每个类中都选择该类中排名最高的物品组合成最终的推荐列表。这种方法的<u>好处</u>是比较简单直观，但这种方法也有严重的<u>缺点</u>。首先，选择什么样的内容属性进行分类对结果的影响很大。其次，就算选择了某种类别，但物品是否属于某个类别是编辑确定的，并不一定能够得到用户的公认。</p><p>第二种提高推荐结果多样性的方法是控制不同推荐结果的推荐理由出现的次数。本章提出的推荐系统对于每个推荐出来的物品都有一个推荐理由，这个推荐理由一般是产生推荐结果的重要特征。那么，要提高推荐结果的多样性，就需要让推荐结果尽量来自不同的特征，具有不同的推荐理由，而不是所有的推荐结果都对应一个理由。</p><p>下面的代码根据推荐理由增加推荐结果的多样性，这里输入的<code>recommendations</code>是按照权重从大到小排序的，程序中每次拿出一个推荐结果，如果这个结果已经被用过了，就会对推荐结果的权重除以2降权（这里具体除以几可以在实际应用中自己调整），最终将推荐结果重新按照权重从大到小排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReasonDiversity</span><span class="params">(recommendations)</span>:</span></span><br><span class="line">    reasons = set() </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> recommendations:</span><br><span class="line">        <span class="keyword">if</span> i.reason <span class="keyword">in</span> reasons:</span><br><span class="line">            i.weight /= <span class="number">2</span></span><br><span class="line">        reasons.add(i.reason) </span><br><span class="line">    recommendations = sortByWeight(recommendations)</span><br></pre></td></tr></table></figure><h4 id="3-时间多样性"><a href="#3-时间多样性" class="headerlink" title="3.时间多样性"></a>3.时间多样性</h4><p>时间多样性主要是为了保证用户不要每天来推荐系统都看到同样的推荐结果。在第5章已经提到，提高推荐系统的时间多样性要从两个地方着手。首先要<em>保证推荐系统的实时性</em>，在用户有新行为时实时调整推荐结果以满足用户最近的需求。这一点，在本章的推荐系统设计中已经考虑到了。如果用户有实时行为发生，那么行为提取和分析模块就能实时拿到行为数据并转化为新的特征，然后经过特征-物品相关模块转换成和新特征最相关的物品，因而推荐列表中就立即反应了用户最新行为的影响。提高推荐结果多样性的第二个方面是<em>要在用户没有新的行为时，也要保证推荐结果每天都有变化</em>。要实现这一点，只能通过如下方式。</p><ul><li>记录用户每次登陆推荐系统看到的推荐结果。</li><li>将这些结果发回日志系统。这种数据不需要实时存储，只要能保证小于一天的延时就足够了。</li><li>在用户登录时拿到用户昨天及之前看过的推荐结果列表，从当前推荐结果中将用户已经看到的推荐结果降权。</li></ul><h4 id="4-用户反馈"><a href="#4-用户反馈" class="headerlink" title="4.用户反馈"></a>4.用户反馈</h4><p>排名模块最重要的部分就是用户反馈模块。用户反馈模块主要通过分析用户之前和推荐结果的交互日志，预测用户会对什么样的推荐结果比较感兴趣。</p><p>如果推荐系统的目标是提高用户对推荐结果的点击率，那么可以利用<strong>点击模型(click model)</strong>预测用户是否会点击推荐结果。点击模型在很多领域得到了广泛应用，比如搜索结果的点击预测(参见论文“A dynamic bayesian network click model for web search ranking”，作者为Olivier Chapelle和Ya Zhang)、 搜索广告的点击预测(参见论文“Online learning from click data for sponsored search”，作者为Massimiliano Ciaramita、Vanessa Murdock和Vassilis Plachouras )、上下文广告的点击预测(参见论文“Contextual advertising by combining relevance with click feedback”，作者为Deepayan chakrabarti、Deepak Agarwal和Vanja Josifovski。)。点击预测的<u>主要问题</u>是预测用户看到某个推荐结果时是否会点击。那么要进行点击率预测，首先需要提取特征。在推荐系统的点击率预测中可以用如下特征预测用户$u$会不会点击物品$i$：</p><ul><li>用户$u$相关的特征，比如年龄、性别、活跃程度、之前有没有点击行为；</li><li>物品$i$相关的特征，比如流行度，平均分，内容属性；</li><li>物品$i$在推荐列表中的位置。用户的点击和用户界面的设计有很高的相关性，因此物品$i$在推荐列表中的位置对预测用户是否点击很重要；</li><li>用户之前是否点击过和推荐物品$i$具有同样<strong>推荐解释</strong>的其他推荐结果；</li><li>用户之前是否点击过和推荐物品$i$来自同样<strong>推荐引擎</strong>的其他推荐结果。</li></ul><p>点击模型需要离线计算好，在线将模型加载到内存中。为了提高在线预测的效率，一般只可以使用线性模型。</p><h2 id="7-4-扩展阅读"><a href="#7-4-扩展阅读" class="headerlink" title="7.4 扩展阅读"></a>7.4 扩展阅读</h2><p>关于推荐系统架构方面的文章很多，不过详细介绍架构的技术报告不多。知名公司亚马逊和Netflix等都只给出了一些简单的线索。本章提到的推荐系统架构主要是基于本书作者在Hulu工作时使用的架构抽象发挥出来的，对于Hulu架构感兴趣的读者可以参考Hulu的技术博客(参见<a href="http://tech.hulu.com/blog/2011/09/19/recommendation-system/" target="_blank" rel="noopener">http://tech.hulu.com/blog/2011/09/19/recommendation-system/</a> )。<br><strong>MyMedia</strong>(参见<a href="http://mymediaproject.codeplex.com/" target="_blank" rel="noopener">http://mymediaproject.codeplex.com/</a> )是一个比较著名的开源推荐系统架构。它是由欧洲研究人员开发的一个推荐系统开源框架。该框架同时支持评分预测和TopN推荐，全面支持各种数据和各种算法，对该项目感兴趣的用户可以访问该项目的网站<a href="http://www.mymediaproject.org/default.aspx" target="_blank" rel="noopener">http://www.mymediaproject.org/default.aspx</a> 。<br>本章提出的推荐系统架构基本上是从基于物品的推荐算法衍生出来的，因此本章的架构并<u>不适合</u>用来解决社会化推荐问题。如果要了解社会化推荐方面的架构，可以参考Twitter公开的一些文档。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第7章-推荐系统实例&quot;&gt;&lt;a href=&quot;#第7章-推荐系统实例&quot; class=&quot;headerlink&quot; title=&quot;第7章 推荐系统实例&quot;&gt;&lt;/a&gt;第7章 推荐系统实例&lt;/h1&gt;&lt;p&gt;前面几章介绍了各种各样的数据和基于这些数据的推荐算法。在实际系统中，前面几章提到的数据大都存在，因此如何设计一个真实的推荐系统处理不同的数据，根据不同的数据设计算法，并将这些算法融合到一个系统当中是本章讨论的主要问题。&lt;/p&gt;
&lt;p&gt;本章首先介绍推荐系统的外围架构，然后介绍推荐系统的架构，并对架构中每个模块的设计进行深入讨论。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实践读书笔记（六）</title>
    <link href="https://rilzob.com/2018/11/29/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>https://rilzob.com/2018/11/29/推荐系统实践读书笔记（六）/</id>
    <published>2018-11-29T09:04:48.454Z</published>
    <updated>2019-03-24T03:31:46.244Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第6章-利用社交网络数据"><a href="#第6章-利用社交网络数据" class="headerlink" title="第6章 利用社交网络数据"></a>第6章 利用社交网络数据</h1><p>美国著名的第三方调查机构尼尔森调查了影响用户相信某个推荐的因素(参见“Global Advertising Consumers Trust Real Friends and Virtual Strangers the Most”，<a href="http://blog.nielsen.com/nielsen-wire/consumer/global-advertising-consumers-trust-real-friends-and-virtual-strangers-the-most/" target="_blank" rel="noopener">http://blog.nielsen.com/nielsen-wire/consumer/global-advertising-consumers-trust-real-friends-and-virtual-strangers-the-most/</a> )。书中这部分有关于这次调查的简略介绍。实验证明了<strong>好友</strong>的推荐对于增加用户对推荐结果的信任度非常重要，并且该实验也从侧面说明了<strong>社交网络</strong>在推荐系统中可能具有重要的作用。</p><p>本章详细讨论了如何利用社交网络数据给用户进行个性化推荐。本章不仅讨论如何利用社交网络给用户推荐物品，而且将讨论如何利用社交网络给用户推荐好友。<br><a id="more"></a></p><h2 id="6-1-获取社交网络数据的途径"><a href="#6-1-获取社交网络数据的途径" class="headerlink" title="6.1 获取社交网络数据的途径"></a>6.1 获取社交网络数据的途径</h2><h3 id="6-1-1-电子邮件"><a href="#6-1-1-电子邮件" class="headerlink" title="6.1.1 电子邮件"></a>6.1.1 电子邮件</h3><p>谷歌在2010年的KDD会议上发表了一篇文章(参见Maayan Roth、Assaf Ben-David、David Deutscher、Guy Flysher、Ilan Horn、Ari Leichtberg、Naty Leiser、Yossi Matias、Ron Merom的“Suggesting Friends Using the Implicit Social Graph”（ACM 2010 Article，2010）)，其中就研究了如何通过Gmail系统中、一些不违反隐私协议的数据预测用户之间的社交关系，以便给用户推荐好友的问题。</p><p>其次，如果能够获得用户的邮箱，也可以通过邮箱后缀得到一定的社交关系信息。</p><h3 id="6-1-2-用户注册信息"><a href="#6-1-2-用户注册信息" class="headerlink" title="6.1.2 用户注册信息"></a>6.1.2 用户注册信息</h3><p>用户注册时输入的信息也是一种隐性社交网络数据，可以用来分析。</p><h3 id="6-1-3-用户的位置数据"><a href="#6-1-3-用户的位置数据" class="headerlink" title="6.1.3 用户的位置数据"></a>6.1.3 用户的位置数据</h3><p>可以通过得到的IP地址，GPS数据作为用户位置信息，进而分析出用户的同事、邻居等关系。</p><h3 id="6-1-4-讨论和讨论组"><a href="#6-1-4-讨论和讨论组" class="headerlink" title="6.1.4 讨论和讨论组"></a>6.1.4 讨论和讨论组</h3><p>类似于豆瓣上的小组。兴趣相近的人可能会加入一些相同的小组。</p><h3 id="6-1-5-即时聊天工具"><a href="#6-1-5-即时聊天工具" class="headerlink" title="6.1.5 即时聊天工具"></a>6.1.5 即时聊天工具</h3><p>通过即时聊天工具上的联系人列表和分组信息，知道用户的社交网络关系，并且能够通过统计用户之间聊天的频繁程度，度量出用户之间的熟悉程度。但与电子邮件一样，存在隐私问题。</p><h3 id="6-1-6-社交网络"><a href="#6-1-6-社交网络" class="headerlink" title="6.1.6 社交网络"></a>6.1.6 社交网络</h3><p>上述各种获取用户社交关系的途径，要么就是因为隐私问题很难获取，要么就是虽然容易获取，但却都是隐性社交关系数据，很难推断出用户之间的显性社交关系。<br>但以Facebook和Twitter为代表的新一代社交网络突破了这个瓶颈。</p><p>社交网站的另一个好处是自然地减轻了<strong>信息过载</strong>问题。在社交网站中，我们可以通过好友给自己过滤信息。比如，我们只关注那些和我们兴趣相似的好友，只阅读他们分享的信息，因此可以避免阅读很多和自己无关的信息。个性化推荐系统可以利用社交网站公开的用户社交网络和行为数据，辅助用户更好地完成信息过滤的任务，更好地找到和自己兴趣相似的好友，更快地找到自己感兴趣的内容。</p><h4 id="1-社会图谱和兴趣图谱"><a href="#1-社会图谱和兴趣图谱" class="headerlink" title="1.社会图谱和兴趣图谱"></a>1.社会图谱和兴趣图谱</h4><p>Facebook和Twitter作为社交网站中的两个代表，它们其实代表了不同的<strong>社交网络结构</strong>。在Facebook里，人们的好友一般都是自己在现实社会中认识的人(参见“Friends &amp; Frenemies: Why We Add and Remove Facebook Friends”，地址为<a href="http://blog.nielsen.com/nielsenwire/online_mobile/friends-frenemies-why-we-add-and-remove-facebook-friends/" target="_blank" rel="noopener">http://blog.nielsen.com/nielsenwire/online_mobile/friends-frenemies-why-we-add-and-remove-facebook-friends/</a> ，尼尔森的这个报告表明82%的用户会因为在现实社会中认识而在Facebook中加好友。)，并且Facebook中的好友关系是需要双方确认的。在Twitter里，人们的好友往往都是现实中自己不认识的， 而只是出于对对方言论的兴趣而建立好友关系， 好友关系也是单向的关注关系。 以Facebook为代表的社交网络称为<strong>社交图谱(social graph)</strong>，而以Twitter为代表的社交网络称为<strong>兴趣图谱(interest graph)</strong>。</p><p>关于这两种社交网络的分类早在19世纪就被社会学家研究过。19世纪，德国社会学家斐迪南·滕尼斯（Ferdinand Tönnies）认为社会群体分为两种，一种是通过人们之间的共同兴趣和信念形成的，他将这种社会群体称为Gemeinschaft，而Gemeinschaft这个词后来被翻译成英语就是community，即汉语中的社区。另一种社会群体则是由于人们之间的亲属关系，工作关系而形成的，他称之为Gesellschaft，英文翻译为society，即汉语中的“社会”。因此，斐迪南·滕尼斯说的Gemeinschaft就是兴趣图谱，而Gesellschaft就是社会图谱。</p><p>但是，每个社会化网站都不是单纯的社交图谱或者兴趣图谱。</p><h2 id="6-2-社交网络数据简介"><a href="#6-2-社交网络数据简介" class="headerlink" title="6.2 社交网络数据简介"></a>6.2 社交网络数据简介</h2><p>可以用图定义社交网络并表示用户之间的关系。用图$G(V,E,w)$定义一个社交网络，其中$V$是顶点集合，每个顶点代表一个用户，$E$是边集合，如果用户$v_a$和$v_b$有社交网络关系，那么就有一条边$e(v_a , v_b)$连接这两个用户，而$w(v_a , v_b)$定义了边的权重。业界有两种著名的社交网络。一种以Facebook为代表，它的朋友关系是需要双向确认的，因此用无向边连接有社交网络关系的用户。另一种以Twitter为代表，它的朋友关系是单向的，因此用有向边代表这种社交网络上的用户关系。</p><p>此外，对图$G$中的用户顶点$u$，定义$out(u)$为顶点$u$指向的顶点集合（如果套用微博中的术语，$out(u)$就是用户$u$关注的用户集合），定义$in(u)$为指向顶点$u$的顶点集合（也就是关注用户$u$的用户集合）。那么，在Facebook这种无向社交网络中显然有$out(u)$=$in(u)$。</p><p>一般来说，有3种不同的社交网络数据。</p><ul><li><strong>双向确认的社交网络数据</strong> 以Facebook为代表的社交网络。用户A和B之间形成好友关系需要通过双方的确认。此种社交网络一般可以通过无向图表示。</li><li><strong>单向关注的社交网络数据</strong> 以Twitter为代表的社交网络。用户A可以关注用户B而不需要得到用户B的允许。此种社交网络一般可以通过有向图表示。</li><li><strong>基于社区的社交网络数据</strong> 以豆瓣小组为代表的社交网络。用户之间没有明确的关系，但这种社交网络数据办函了用户属于不同社区的数据。</li></ul><h3 id="社交网络数据中的长尾分布"><a href="#社交网络数据中的长尾分布" class="headerlink" title="社交网络数据中的长尾分布"></a>社交网络数据中的长尾分布</h3><p>该节利用了Slashdot的社交网络数据集(数据集来自Stanford Large Network Dataset Collection，参见<a href="http://snap.stanford.edu/data/" target="_blank" rel="noopener">http://snap.stanford.edu/data/</a> )统计了用户<strong>入度(in degree)</strong>和<strong>出度(out degree)</strong>的分布，得到了两个结论：</p><ul><li>用户的入度近似长尾分布，说明在一个社交网络中影响力大的用户总是占少数。</li><li>用户的出度同样近似长尾分布，说明在一个社交网络中，关注很多人的用户占少数，而绝大多数用户只关注很少的人。</li></ul><h2 id="6-3-基于社交网络的推荐"><a href="#6-3-基于社交网络的推荐" class="headerlink" title="6.3 基于社交网络的推荐"></a>6.3 基于社交网络的推荐</h2><p>社会化推荐之所以受到很多网站的重视，是缘于如下<u>优点</u>。</p><ul><li><strong>好友推荐可以增加推荐的信任度</strong>    好友往往是用户最信任的。用户往往不一定信任计算机的智能，但会信任好朋友的推荐。</li><li><strong>社交网络可以解决冷启动问题</strong>   当新用户使用社交网络账号登录网站时，网站可以从社交网站中获取用户的好友列表，然后给用户推荐好友在网站上喜欢的物品。</li></ul><p>社会化推荐同样拥有<u>缺点</u>，其中最主要的就是很多时候并不一定能提高推荐算法的离线精度（准确率和召回率）。特别是在基于社交图谱数据的推荐系统中，因为用户的好友关系不是基于共同兴趣产生的，所以用户好友的兴趣往往和用户的兴趣并不一致。举个例子就是我们和父母在社交网络上虽然是好友，但兴趣差别很大。</p><p>2010年，ACM推荐系统大会的一个讨论组CAMRa曾经举办过一个关于社交网络的推荐系统比赛(参见<a href="http://www.dai-labor.de/camra2010/" target="_blank" rel="noopener">http://www.dai-labor.de/camra2010/</a> )。该比赛希望参赛者能够利用用户之间的好友关系给用户推荐电影，并且利用准确率相关的指标评测参赛者的推荐算法。对社会化推荐感兴趣的读者可以关注一下该会议的相关论文。</p><h3 id="6-3-1-基于邻域的社会化推荐算法"><a href="#6-3-1-基于邻域的社会化推荐算法" class="headerlink" title="6.3.1 基于邻域的社会化推荐算法"></a>6.3.1 基于邻域的社会化推荐算法</h3><p>如果给定一个社交网络和一份用户行为数据集。其中社交网络定义了用户之间的好友关系，而用户行为数据集定义了不同用户的历史行为和兴趣数据。那么最简单算法是给用户推荐好友喜欢的物品集合。即用户$u$对物品$i$的兴趣$p_{ui}$可以通过如下公式计算。<br>$$<br>p_{ui} = \sum_{v \in \text{out(u)}} r_{vi}<br>$$<br>$\text{out(u)}$是用户$u$的好友集合，如果用户$v$喜欢物品$i$，则$r_{vi} =1$，否则$r_{vi} = 0$。同时由于不同的好友和用户$u$的熟悉程度和兴趣相似度也是不同的。因此，应该在推荐算法中考虑好友和用户的熟悉程度以及兴趣相似度：<br>$$<br>p_{ui} = \sum_{v \in \text{out(u)}} w_{uv}r_{vi}<br>$$<br>$w_{uv}$由两部分相似度构成，一部分是用户$u$和用户$v$的熟悉程度，另一部分是用户$u$和用户$v$的兴趣相似度。用户$u$和用户$v$的<strong>熟悉程度(familiarity)</strong>描述了用户$u$和用户$v$在现实社会中的熟悉程度。一般来说，用户更加相信自己熟悉的好友的推荐，因此我们需要考虑用户之间的熟悉度。熟悉度可以用用户之间的共同好友比例来度量。也就是说如果用户$u$和用户$v$很熟悉，那么一般来说他们应该有很多共同的好友。<br>$$<br>familiarity(u,v) = \frac{\vert out(u) \cap out(v) \vert}{\vert out(u) \cup out(v) \vert}<br>$$<br>除了熟悉程度，还需要考虑用户之间的兴趣相似度，而兴趣相似度可以通过和<strong>UserCF</strong>类似的方法度量，即如果两个用户喜欢的物品集合重合度很高，两个用户的兴趣相似度很高。<br>$$<br>similarity(u,v) = \frac{N(u) \cap N(v)}{N(u) \cup N(v)}<br>$$<br>其中$N(u)$是用户$u$喜欢的物品集合。</p><p>下面的代码实现社会化推荐的逻辑。在代码中，<code>familiarity</code>存储了每个用户最熟悉的$K$个好友和他们的熟悉程度，<code>similarity</code>存储了和每个用户兴趣最相关的$K$好友和他们的兴趣相似度。<code>train</code>记录了每个用户的行为记录，其中<code>train[u]</code>记录了用户$u$喜欢的物品列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Recommend</span><span class="params">(uid, familiarity, similarity, train)</span>:</span></span><br><span class="line">    rank = dict() </span><br><span class="line">    interacted_items = train[uid] </span><br><span class="line">    <span class="keyword">for</span> fid,fw <span class="keyword">in</span> familiarity[uid]:</span><br><span class="line">        <span class="keyword">for</span> item,pw <span class="keyword">in</span> train[fid]:</span><br><span class="line">            <span class="comment"># if user has already know the item </span></span><br><span class="line">            <span class="comment"># do not recommend it </span></span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> interacted_items:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            addToDict(rank, item, fw * pw)</span><br><span class="line">    <span class="keyword">for</span> vid,sw <span class="keyword">in</span> similarity[uid]:</span><br><span class="line">        <span class="keyword">for</span> item,pw <span class="keyword">in</span> train[vid]:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> interacted_items: </span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            addToDict(rank, item, sw * pw)</span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure><h3 id="6-3-2-基于图的社会化推荐算法"><a href="#6-3-2-基于图的社会化推荐算法" class="headerlink" title="6.3.2 基于图的社会化推荐算法"></a>6.3.2 基于图的社会化推荐算法</h3><p>图模型的<u>优点</u>是可以将各种数据和关系都表示到图上去。在社交网站中存在两种关系，一种是<em>用户对物品的兴趣关系</em>，一种是<em>用户之间的社交网络关系</em>。本节主要讨论如何将这两种关系建立到图模型中，从而实现对用户的个性化推荐。</p><p>用户的社交网络可以表示为<strong>社交网络图</strong>，用户对物品的行为可以表示为<strong>用户物品二分图</strong>，而这两种图可以结合成一个图。该图上有用户顶点和物品顶点两种顶点。如果用户$u$对物品$i$产生过行为，那么两个节点之间就有边相连。如果用户$u$和用户$v$是好友，那么也会有一条边连接这两个用户。</p><p>在定义完图中的顶点和边后，需要定义边的权重。其中用户和用户之间边的权重可以定义为用户之间相似度的$\alpha$倍（包括熟悉程度和兴趣相似度），而用户和物品之间的权重可以定义为用户对物品喜欢程度的$\beta$倍。$\alpha$和$\beta$需要根据应用的需求确定。如果希望<strong>用户好友的行为</strong>对推荐结果产生比较大的影响，那么就可以选择比较大的$\alpha$。相反，如果希望<strong>用户的历史行为</strong>对推荐结果产生比较大的影响，就可以选择比较大的。</p><p>在定义完图中的顶点、边和边的权重后，就可以利用前面几章提到的<strong>PersonalRank图排序算法</strong>给每个用户生成推荐结果。</p><p>在社交网络中，除了常见的、用户和用户之间直接的社交网络关系，还有一种关系，即两个用户属于同一个社群。Quan Yuan等详细研究了这两种社交网络关系(参见Quan Yuan、Li Chen和Shiwan Zhao的“Factorization vs. regularization: fusing heterogeneous social relationships in top-n recommendation”（ACM 2011 Article，2011）)，他们将第一种社交网络关系称为<strong>friendship</strong>，而将第二种社交网络关系称为<strong>membership</strong>。如果要在前面提到的基于邻域的社会化推荐算法中考虑membership的社交关系，可以利用两个用户加入的社区重合度计算用户 相似度，然后给用户推荐和他相似的用户喜欢的物品。但是，如果利用图模型则更为容易，可以加入一种节点表示社群，而如果用户属于某一社群，图中就有一条边联系用户对应的节点和社群对应的节点。建立图模型后，就可以通过前面提到的基于图的推荐算法(例如PersonalRank)给用户推荐物品。</p><h3 id="6-3-3-实际系统中的社会化推荐算法"><a href="#6-3-3-实际系统中的社会化推荐算法" class="headerlink" title="6.3.3 实际系统中的社会化推荐算法"></a>6.3.3 实际系统中的社会化推荐算法</h3><p>6.3.1节提出的基于邻域的社会化推荐算法看似简单，但在实际系统中却是很难操作的，这主要是因为该算法需要拿到用户所有好友的历史行为数据，而这一操作在实际系统中是比较重的操作。因为大型网站中用户数目非常庞大，用户的历史行为记录也非常庞大，所以不太可能将用户的所有行为都缓存在内存中，只能在数据库前做一个热数据的缓存。</p><p>由于ItemCF算法只需要当前用户的历史行为数据和物品的相关表就可以生成推荐结果。对于物品数不是很多的网站，可以将物品相关表缓存在内存中，因此ItemCF算法很容易在实际环境下实现。</p><p>可以从几个方面改进基于邻域的社会化推荐算法，让它能够具有比较快的响应时间。改进的方向有两种，一种是治标不治本的方法。简单地说，就是可以<em>做两处截断</em>。第一处截断在拿用户好友集合时只拿出用户相似度最高的N个好友而非全部，从而给该用户做推荐时可以只查询N次用户历史行为接口。此外，在查询每个用户的历史行为时，只返回用户最近1个月的行为，这样就可以在用户行为缓存中缓存更多用户的历史行为数据，从而加快查询用户历史行为接口的速度。此外，还可以牺牲一定的实时性，降低缓存中用户行为列表过期的频率。</p><p>而第二种解决方案需要<em>重新设计数据库</em>。Twitter的解决方案是给每个用户维护一个消息队列<strong>(message queue)</strong>，当一个用户发表一条微博时，所有关注他的用户的消息队列中都会加入这条微博。这个实现的<u>优点</u>是用户获取<strong>信息墙</strong>时可以直接读消息队列，所以终端用户的读操作很快。不过这个实现也有<u>缺点</u>，当一个用户发表了一条微博，就会触发很多写操作，因为要更新所有关注他的用户的消息队列，特别是当一个人被很多人关注时，就会有大量的写操作。Twitter通过大量的缓存解决了这一问题。具体的细节可以参考InfoQ对Twitter架构的介绍(参见“Twitter, an Evolving Architecture”，地址为<a href="http://www.infoq.com/news/2009/06/Twitter-Architecture" target="_blank" rel="noopener">http://www.infoq.com/news/2009/06/Twitter-Architecture</a> )。</p><p>如果将Twitter的架构搬到社会化推荐系统中，就可以按照如下方式设计系统：</p><ol><li>首先，为每个用户维护一个消息队列，用于存储他的推荐列表；</li><li>当一个用户喜欢一个物品时，就将（物品ID、用户ID和时间）这条记录写入关注该用户的推荐列表消息队列中；</li><li>当用户访问推荐系统时，读出他的推荐列表消息队列，对于这个消息队列中的每个物品，重新计算该物品的权重。计算权重时需要考虑物品在队列中出现的次数，物品对应的用户和当前用户的熟悉程度、物品的时间戳。同时，计算出每个物品被哪些好友喜欢过，用这些好友作为物品的推荐解释。</li></ol><h3 id="6-3-4-社会化推荐系统和协同过滤推荐系统"><a href="#6-3-4-社会化推荐系统和协同过滤推荐系统" class="headerlink" title="6.3.4 社会化推荐系统和协同过滤推荐系统"></a>6.3.4 社会化推荐系统和协同过滤推荐系统</h3><p>关于社会化推荐系统的离线评测可以参考Georg Groh和Christian Ehmig的工作成果(参见“Recommendations in Taste Related Domains: Collaborative Filtering vs. Social Filtering”，2007年)。不过社会化推荐系统的效果往往很难通过离线实验评测，因为社会化推荐的<u>优势</u>不在于增加预测准确度，而是在于通过用户的好友增加用户对推荐结果的信任度，从而让用户单击那些很冷门的推荐结果。此外，很多社交网站（特别是基于社交图谱的社交网站）中具有好友关系的用户并不一定有相似的兴趣。因此，利用好友关系有时并不能增加离线评测的准确率和召回率。因此，很多研究人员利用用户调查和在线实验的方式评测社会化推荐系统。</p><p>对社会化推荐系统进行用户调查的代表性工作成果是Rashmi Sinha和Kirsten Swearingen对比社会化推荐系统和协同过滤推荐系统的论文(参见“Comparing Recommendations Made by Online Systems and Friends”，2001年 )。这一节简单介绍了他们的工作方法和结果，详细见书。</p><h3 id="6-3-5-信息流推荐"><a href="#6-3-5-信息流推荐" class="headerlink" title="6.3.5 信息流推荐"></a>6.3.5 信息流推荐</h3><p>信息墙已经是个性化的，但里面仍夹杂了很多垃圾信息。因此，<strong>信息流</strong>的个性化推荐要解决的问题就是如何进一步帮助用户从信息墙上挑选有用的信息。</p><p>目前最流行的信息流推荐算法是Facebook的<strong>EdgeRank</strong>，该算法<u>综合考虑</u>了信息流中每个会话的时间、长度与用户兴趣的相似度。EdgeRank算法比较神秘，没有相关的论文，不过TechCrunch曾经公开过它的<u>主要思想</u>(参见“EdgeRank: The Secret Sauce That Makes Facebook’s News Feed Tick”， 地址为<a href="http://techcrunch.com/2010/04/22/facebook-edgerank/" target="_blank" rel="noopener">http://techcrunch.com/2010/04/22/facebook-edgerank/</a> )。Facebook将其他用户对当前用户信息流中的会话产生过行为的行为称为<strong>edge</strong>，而一条会话的权重定义为：</p><p>$$<br>\sum_{\text{edge} \ e}u_e w_e d_e<br>$$</p><ul><li>$u_e$指产生行为的用户和当前用户的相似度，这里的相似度主要是在社交网络图中的熟悉度；</li><li>$w_e$指行为的权重，这里的行为包括创建、评论、like(喜欢)、打标签等，不同的行为有不同的权重。</li><li>$d_e$指时间衰减参数，越早的行为对权重的影响越低。</li></ul><p>从上面的描述中可以得出如下结论：如果一个会话被你熟悉的好友最近产生过重要的行为，它就会有比较高的权重。</p><p>不过，EdgeRank算法的个性化因素仅仅是好友的熟悉度，它并没有考虑帖子内容和用户兴趣的相似度。所以EdgeRank仅仅考虑了“我”周围用户的社会化兴趣，而没有重视“我”个人的个性化兴趣。为此，GroupLens的研究人员Jilin Chen深入研究了信息流推荐中社会兴趣和个性化兴趣之间的关系。 (参见Jilin Chen、Rowan Nairn和Ed H. Chi的“Speak Little and Well: Recommending Conversations in Online Social Streams”（ACM 2011 Article, 2011）)他们的排名算法考虑了如下因素。</p><ul><li><strong>会话的长度</strong>    越长的会话包括越多的信息。</li><li><strong>话题相关性</strong>    度量了会话中主要话题和用户兴趣之间的相关性。这里Jilin Chen用了简单的<strong>TF-IDF</strong>建立用户历史兴趣的关键词向量和当前会话的关键词向量，然后用这两个向量的相似度度量话题相关性。</li><li><strong>用户熟悉程度</strong>    主要度量了会话中涉及的用户（比如会话的创建者、讨论者等）和当前用户的熟悉程度。对于如何度量用户的熟悉程度下一节将详细介绍。计算熟悉度的主要思想是考虑用户之间的共同好友数等。</li></ul><p>为了验证算法的性能，Jilin Chen同样也设计了一个用户调查。首先，他通过问卷将用户分成两种类型。第一种类型的用户<em>使用Twitter的目的是寻找信息</em>，也就是说他们将Twitter看做一种信息源和新闻媒体。而第二种用户使用Twitter的目的是<em>了解好友的最新动态以及和好朋友聊天</em>。然后，他让参试者对如下5种算法的推荐结果给出1~5分的评分，其中1分表示不喜欢，5分表示最喜欢。</p><ul><li><strong>Random</strong>    给用户随机推荐会话</li><li><strong>Length</strong>   给用户推荐比较长的会话</li><li><strong>Topic</strong>     给用户推荐和他兴趣相关的会话。</li><li><strong>Tie</strong>    给用户推荐和他熟悉的好友参与的会话。</li><li><strong>Topic+Tie</strong>     综合考虑会话和用户的兴趣相关度以及用户好友参与会话的程度。</li></ul><p>通过收集用户反馈，Jilin Chen发现对于所有用户不同算法的平均得分是：<br>Topic+Tie &gt; Tie &gt; Topic &gt; Length &gt; Random<br>而对于主要目的是寻找信息的用户，不同算法的得分是：<br>Topic+Tie ≥ Topic &gt; Length &gt; Tie &gt; Random<br>对于主要目的是交友的用户，不同算法的得分是：<br>Topic+Tie &gt; Tie &gt; Topic &gt; Length &gt; Random</p><p>实验结果说明，综合考虑用户的社会兴趣和个人兴趣对于提高用户满意度是有帮助的。因此，当我们在一个社交网站中设计推荐系统时，可以综合考虑这两个因素，找到最合适的<strong>融合参数</strong>来融合用户的社会兴趣和个人兴趣，从而给用户提供最令他们满意的推荐结果。</p><h2 id="6-4-给用户推荐好友"><a href="#6-4-给用户推荐好友" class="headerlink" title="6.4 给用户推荐好友"></a>6.4 给用户推荐好友</h2><p>好友推荐系统的<u>目的</u>是根据用户现有的好友、 用户的行为记录给用户推荐新的好友，从而增加整个社交网络的稠密程度和社交网站用户的活跃度。</p><p>好友推荐算法在社交网络上被称为<strong>链接预测(link prediction)</strong>。关于链接预测算法研究的代表文章是Jon Kleinberg的“Link Prediction in Social Network”。该文对各种用户好友关系的预测方法进行了系统地研究和对比。本节介绍其中一些比较直观和简单的算法。</p><h3 id="6-4-1-基于内容的匹配"><a href="#6-4-1-基于内容的匹配" class="headerlink" title="6.4.1 基于内容的匹配"></a>6.4.1 基于内容的匹配</h3><p>给用户推荐和他们有相似内容属性的用户作为好友。</p><p>常用属性如下：</p><ul><li>用户人口统计学属性，包括年龄、性别、职业、毕业学校和工作单位等。</li><li>用户的兴趣，包括用户喜欢的物品和发布过的言论等。</li><li>用户的位置信息，包括用户的住址、IP地址和邮编等。</li></ul><p>利用内容信息计算用户的相似度和前面介绍的利用内容信息计算物品的相似度类似。</p><h3 id="6-4-2-基于共同兴趣的好友推荐"><a href="#6-4-2-基于共同兴趣的好友推荐" class="headerlink" title="6.4.2 基于共同兴趣的好友推荐"></a>6.4.2 基于共同兴趣的好友推荐</h3><p>在Twitter和微博为代表的以<strong>兴趣图谱</strong>为主的社交网络中，用户往往不关心对于一个人是否在现实社会中认识，而只关心是否和他们有共同的兴趣爱好。因此，在这种网站中需要给用户推荐和他有共同兴趣的其他用户作为好友。</p><p>在第3章介绍<strong>基于用户的协同过滤算法(UserCF)</strong>时已经详细介绍了如何计算用户之间的兴趣相似度，其<u>主要思想</u>就是如果用户喜欢相同的物品，则说明他们具有相似的兴趣。</p><p>此外，也可以根据用户在社交网络中的发言提取用户的兴趣标签，来计算用户的兴趣相似度。关于如何分析用户发言的内容、提取文本的关键词、计算文本的相似度，可以参考第4章。</p><h3 id="6-4-3-基于社交网络图的好友推荐"><a href="#6-4-3-基于社交网络图的好友推荐" class="headerlink" title="6.4.3 基于社交网络图的好友推荐"></a>6.4.3 基于社交网络图的好友推荐</h3><p>最简单的好友推荐算法是给用户推荐好友的好友。</p><p>下面介绍3中基于社交网络的好友推荐算法。<br>对于用户$u$和用户$v$，可以用他们共同好友比例计算他们的相似度：<br>$$<br>w_{out}(u,v) = \frac{\vert out(u) \cap out(v) \vert}{\sqrt {\vert out(u) \vert \vert out(v) \vert}}<br>$$</p><p>下面的代码实现了这种相似度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FriendSuggestion</span><span class="params">(user, G, GT)</span>:</span></span><br><span class="line">    suggestions = dict() </span><br><span class="line">    friends = G[user] </span><br><span class="line">    <span class="keyword">for</span> fid <span class="keyword">in</span> G[user]:</span><br><span class="line">        <span class="keyword">for</span> ffid <span class="keyword">in</span> GT[fid]:</span><br><span class="line">            <span class="keyword">if</span> ffid <span class="keyword">in</span> friends:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            <span class="keyword">if</span> ffid <span class="keyword">not</span> <span class="keyword">in</span> suggestions:</span><br><span class="line">                suggestions[ffid] = <span class="number">0</span> </span><br><span class="line">            suggestions[ffid] += <span class="number">1</span> </span><br><span class="line">    suggestions = &#123;x: y / math.sqrt(len(G[user]) * len(G[x]) <span class="keyword">for</span> x,y <span class="keyword">in</span> suggestions&#125;</span><br></pre></td></tr></table></figure><p>$w_{out}(u,v)$公式中$out(u)$是在社交网络图中用户$u$指向的其他好友的集合。同理$in(u)$是在社交网络图中指向用户$u$的用户的集合。在无向社交网络图中，$out(u)$和$in(u)$是相同的集合。但在有向社交网络中，两个集合就不同了，因此可以通过$in(u)$定义另一种相似度：<br>$$<br>w_{in}(u,v) = \frac{\vert in(u) \cap in(v) \vert}{\sqrt{\vert in(u) \vert \vert in(v) \vert}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FriendSuggestion</span><span class="params">(user, G, GT)</span>:</span></span><br><span class="line">    suggestions = dict() </span><br><span class="line">    <span class="keyword">for</span> fid <span class="keyword">in</span> GT[user]:</span><br><span class="line">        <span class="keyword">for</span> ffid <span class="keyword">in</span> G[fid]:</span><br><span class="line">            <span class="keyword">if</span> ffid <span class="keyword">in</span> friends:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            <span class="keyword">if</span> ffid <span class="keyword">not</span> <span class="keyword">in</span> suggestions:</span><br><span class="line">                suggestions[ffid] = <span class="number">0</span> </span><br><span class="line">            suggestions[ffid] += <span class="number">1</span> </span><br><span class="line">    suggestions = &#123;x: y / math.sqrt(len(GT[user]) * len(GT[x]) <span class="keyword">for</span> x,y <span class="keyword">in</span> suggestions&#125;</span><br></pre></td></tr></table></figure><p>这两种相似度的定义有着不同的含义，用微博中的关注来解释这两种相似度。如果用户$u$关注了用户$v$，那么$v$就属于$out(u)$，而$u$就属于$in(v)$。因此，$w_{out} (u , v )$越大表示用户$u$和$v$关注的用户集合重合度越大，而$w_{in }(u, v) $越大表示关注用户$u$和关注用户$v$的用户的集合重合度越大。</p><p>前面两种相似度都是对称的，也就是也就是$w_{in} (u, v)  = w_{in} (v, u )$，$w_{out} (u , v ) = w_{out} (v, u ) $。同时，我们还可以定义第三种有向的相似度：<br>$$<br>w_{out,in}(u,v) = \frac{\vert out(u) \cap in(v) \vert}{out(u)}<br>$$<br>这个相似度的含义是用户$u$关注的用户中，有多大比例也关注了用户v。但是，这个相似度有一个<u>缺点</u>，就是在该相似度的定义下所有人都和名人有很大的相似度。这是因为这个相似度在分母的部分没有考虑$|in(v)|$的大小。因此，可以用如下公式改进上面的相似度：<br>$$<br>w_{out,in}’(u,v) = \frac{\vert out(u) \cap in(v) \vert}{\sqrt{\vert out(u) \vert \vert in(v) \vert}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FriendSuggestion</span><span class="params">(user, G, GT)</span>:</span></span><br><span class="line">    suggestions = dict() </span><br><span class="line">    <span class="keyword">for</span> fid <span class="keyword">in</span> GT[user]:</span><br><span class="line">        <span class="keyword">for</span> ffid <span class="keyword">in</span> G[fid]:</span><br><span class="line">            <span class="keyword">if</span> ffid <span class="keyword">in</span> friends:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            <span class="keyword">if</span> ffid <span class="keyword">not</span> <span class="keyword">in</span> suggestions:</span><br><span class="line">                suggestions[ffid] = <span class="number">0</span> </span><br><span class="line">    suggestions[ffid] += <span class="number">1</span> suggestions = &#123;x: y / math.sqrt(len(GT[user]) * len(GT[x]) <span class="keyword">for</span> x,y <span class="keyword">in</span> suggestions&#125;</span><br></pre></td></tr></table></figure><p>前面讨论的这些相似度都是基于一些简单计算公式给出的。这些相似度的计算无论时间复杂度还是空间复杂度都不是很高，非常适合在线应用使用。</p><h4 id="离线实验"><a href="#离线实验" class="headerlink" title="离线实验"></a>离线实验</h4><p>本节通过一些离线实验评测本节提出的几种相似度，评测哪种相似度能更好地预测用户之间的好友关系。实验详情见书。最终的结论是在实际系统中没有哪一种相似度公式绝对合适，只有在自己的数据集上对不不同的算法，才能找到最适合自己数据集的好友推荐算法。</p><h3 id="6-4-4-基于用户调查的好友推荐算法对比"><a href="#6-4-4-基于用户调查的好友推荐算法对比" class="headerlink" title="6.4.4 基于用户调查的好友推荐算法对比"></a>6.4.4 基于用户调查的好友推荐算法对比</h3><p>对于前面3节提出的几种不同的好友推荐算法，上一节提到的GroupLen的Jilin Chen也进行了研究。他通过用户调查对比了不同算法的用户满意度(参见Jilin Chen、Werner Geyer、Casey Dugan Michael Muller、Ido Guy的“‘Make New Friends, but Keep the Old’ ——Recommending People on Social Networking Site”（CHI 2009）)。实验介绍和结果见书。</p><h2 id="6-5-扩展阅读"><a href="#6-5-扩展阅读" class="headerlink" title="6.5 扩展阅读"></a>6.5 扩展阅读</h2><p>社交网络分析的研究已经有很悠久的历史了。其中关于社交网络最让人耳熟能详的结论就是<strong>六度原理</strong>。六度原理讲的是社会中任意两个人都可以通过不超过6个人的路径相互认识，如果转化为图的术语，就是社交网络图的直径为6。不过喜欢刨根问底的读者一定好奇六度原理的正确性。六度原理在均匀随机图上已经得到了完美证明，对此感兴趣的读者可以参考Random Graph一书。很多对社交网络的研究都是基于随机图理论的，因此深入研究社交网络必须掌握随机图理论的相关知识。</p><p>社交网络研究中有两个最著名的问题。第一个是<em>如何度量人的重要性，也就是社交网络顶点的中心度(centrality)</em>，第二个问题是<em>如何度量社交网络中人和人之间的关系，也就是链接预测</em>。这两个问题的研究都有着深刻的实际意义，因此得到了业界和学术界的广泛关注。对这两个问题感兴趣的读者可以参考社交网络分析方面的书(比如（Social Network Analysis: Methods and Applications）和（Social Network Analysis: A Handbook）)。</p><p>对于基于社交网络的推荐算法，因为数据集的限制，最早的研究都是基于Epinion的用户信任网络的。Ma Hao在Epinion数据集上提出了很多<strong>基于矩阵分解</strong>的社会化推荐算法用来解决评分预测问题(参见Hao Ma、Haixuan Yang、Michael R. Lyu和Irwin King的“SoRec: Social Recommendation Using Probabilistic Matrix Factorization”（ACM 2008 Article , 2008）)，其<u>主要思想</u>是在矩阵分解模型中加入正则化项，让具有社交关系的用户的隐语义向量具有比较高的相似度。</p><p>ACM推荐系统大会在2010年曾经举办过一个社会化推荐比赛(即CAMRa201: Challenge on Context-aware Movie Recommendation )，该比赛将社交网络看做一种上下文，希望参赛者能够利用社交网络信息提高推荐系统的性能。关注社交化推荐的读者可以关注一下该比赛最后发出的论文集。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第6章-利用社交网络数据&quot;&gt;&lt;a href=&quot;#第6章-利用社交网络数据&quot; class=&quot;headerlink&quot; title=&quot;第6章 利用社交网络数据&quot;&gt;&lt;/a&gt;第6章 利用社交网络数据&lt;/h1&gt;&lt;p&gt;美国著名的第三方调查机构尼尔森调查了影响用户相信某个推荐的因素(参见“Global Advertising Consumers Trust Real Friends and Virtual Strangers the Most”，&lt;a href=&quot;http://blog.nielsen.com/nielsen-wire/consumer/global-advertising-consumers-trust-real-friends-and-virtual-strangers-the-most/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://blog.nielsen.com/nielsen-wire/consumer/global-advertising-consumers-trust-real-friends-and-virtual-strangers-the-most/&lt;/a&gt; )。书中这部分有关于这次调查的简略介绍。实验证明了&lt;strong&gt;好友&lt;/strong&gt;的推荐对于增加用户对推荐结果的信任度非常重要，并且该实验也从侧面说明了&lt;strong&gt;社交网络&lt;/strong&gt;在推荐系统中可能具有重要的作用。&lt;/p&gt;
&lt;p&gt;本章详细讨论了如何利用社交网络数据给用户进行个性化推荐。本章不仅讨论如何利用社交网络给用户推荐物品，而且将讨论如何利用社交网络给用户推荐好友。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实践读书笔记（五）</title>
    <link href="https://rilzob.com/2018/11/28/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>https://rilzob.com/2018/11/28/推荐系统实践读书笔记（五）/</id>
    <published>2018-11-28T01:23:33.944Z</published>
    <updated>2019-03-24T03:31:27.756Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第5章-利用上下文信息"><a href="#第5章-利用上下文信息" class="headerlink" title="第5章 利用上下文信息"></a>第5章 利用上下文信息</h1><p>上下文信息对于提高推荐系统的各项评测指标是十分重要的，因此不能忽略上下文信息。准确了解用户的<strong>上下文信息(context)</strong>，并将该信息应用于推荐算法是设计好的推荐系统的<u>关键步骤</u>。<br>关于上下文推荐的研究， 可以参考Alexander Tuzhilin(个人主页为<a href="http://people.stern.nyu.edu/atuzhili/" target="_blank" rel="noopener">http://people.stern.nyu.edu/atuzhili/</a> )教授的一篇综述“Context Aware Recommender Systems”。</p><p>本章主要讨论了<strong>时间上下文</strong>，并简单介绍一下<strong>地点上下文</strong>，讨论如何将时间信息和地点信息建模到推荐算法中，从而让推荐系统能够准确预测用户在某个特定时刻及特定地点的兴趣。本章仍然研究TopN推荐，即如何给用户生成一个长度为$N$的推荐列表，而该列表包含了用户在某一时刻或者某个地方最可能喜欢的物品。<br><a id="more"></a></p><h2 id="5-1-时间上下文信息"><a href="#5-1-时间上下文信息" class="headerlink" title="5.1 时间上下文信息"></a>5.1 时间上下文信息</h2><p>本节重点讨论了上下文信息中最重要的时间上下文信息。本节首先介绍了各种不同的时间效应，然后研究如何将这些时间效应建模到推荐系统的模型中，最后通过实际数据集对比不同模型的效果。</p><h3 id="5-1-1-时间效应简介"><a href="#5-1-1-时间效应简介" class="headerlink" title="5.1.1 时间效应简介"></a>5.1.1 时间效应简介</h3><p>一般认为，时间信息对用户兴趣的影响表现在以下几个方面。</p><ul><li><strong>用户兴趣是变化的</strong>  这里提到的用户兴趣变化是因为用户自身原因发生的变化，并且考虑用户最近的兴趣只能针对渐变的用户兴趣，而对突变的用户兴趣很难起作用。</li><li><strong>物品也是有生命周期的</strong>  不同系统的物品具有不同的生命周期。</li><li><strong>季节效应</strong>  季节效应主要反映了时间本身对用户兴趣的影响。</li></ul><h3 id="5-1-2-时间效应举例"><a href="#5-1-2-时间效应举例" class="headerlink" title="5.1.2 时间效应举例"></a>5.1.2 时间效应举例</h3><p>这节通过Google Insights工具提供的某个搜索词的搜索频率曲线对时间效应进行一些分析，并且罗列了一些用户兴趣变化及节日效应的例子。</p><h3 id="5-1-3-系统时间特性的分析"><a href="#5-1-3-系统时间特性的分析" class="headerlink" title="5.1.3 系统时间特性的分析"></a>5.1.3 系统时间特性的分析</h3><p>在给定时间信息后，推荐系统从一个<strong>静态系统</strong>变成了一个<strong>时变的系统</strong>，而用户行为数据也变成了<strong>时间序列</strong>。研究一个时变系统，需要首先研究这个系统的时间特性。</p><p>本节通过研究时变 用户行为数据集来研究不同类型网站的时间特性。包含时间信息的用户行为数据集由一系列<strong>三元组</strong>构成，其中每个三元组$(u,i,t)$代表了用户$u$在时刻$t$对物品$i$产生过行为。在给定数据集后，本节通过统计如下信息研究系统的时间特性。</p><ul><li><strong>数据集每天独立用户数的增长情况</strong></li><li><strong>系统的物品变化情况</strong></li><li><strong>用户访问情况</strong></li></ul><h4 id="1-数据集的选择"><a href="#1-数据集的选择" class="headerlink" title="1.数据集的选择"></a>1.数据集的选择</h4><p>本节利用Delicious数据集进行离线实验以评测不同算法的预测精度，书中这部分有一些关于Delicious数据集的介绍。</p><h4 id="2-物品的生存周期和系统的时效性"><a href="#2-物品的生存周期和系统的时效性" class="headerlink" title="2.物品的生存周期和系统的时效性"></a>2.物品的生存周期和系统的时效性</h4><p>不同类型网站的物品具有不同的生命周期。可以使用如下指标度量网站中物品的生命周期。</p><ul><li><strong>物品平均在线天数</strong>    如果一个物品在某天被至少一个用户产生过行为，就定义该物品在这一天在线。因此，就可以通过物品的平均在线天数度量一类物品的生存周期。</li><li><strong>相隔T天系统物品流行度向量的平均相似度</strong>    取系统中相邻T天的两天，分别计算这两天的物品流行度，从而得到两个流行度向量。然后，计算这两个向量的余弦相似度，如果相似度大，说明系统的物品在相隔T天的时间内没有发生大的变化，从而说明系统的时效性不强，物品的平均在线时间较长。反之，相似度小则说明时效性很强。</li></ul><h3 id="5-1-4-推荐系统的实时性"><a href="#5-1-4-推荐系统的实时性" class="headerlink" title="5.1.4 推荐系统的实时性"></a>5.1.4 推荐系统的实时性</h3><p>用户兴趣是不断变化的，其变化体现在用户不断增加的新行为中。一个实时的推荐系统需要能够实时响应用户新的行为，让推荐列表不断变化，从而满足用户不断变化的兴趣。</p><p>实现推荐系统的实时性除了对用户行为的存取有实时性要求，还要求推荐算法本身具有实时性，而推荐算法本身的实时性意味着：</p><ul><li>实时推荐系统不能每天都给所有用户离线计算推荐结果，然后在线展示昨天计算出来的结果。所以，要求在每个用户访问推荐系统时，都根据用户这个时间点前的行为实时计算推荐列表。</li><li>推荐算法需要平衡考虑用户的近期行为和长期行为，即要让推荐列表反应出用户近期行为所体现的兴趣变化，又不能让推荐列表完全受用户近期行为的影响，要保证推荐列表对用户兴趣预测的延续性。</li></ul><h3 id="5-1-5-推荐算法的时间多样性"><a href="#5-1-5-推荐算法的时间多样性" class="headerlink" title="5.1.5 推荐算法的时间多样性"></a>5.1.5 推荐算法的时间多样性</h3><p>为了避免每天给用户的推荐结果相近，将推荐系统每天推荐结果的变化程度被定义为推荐系统的时间多样性。时间多样性高的推荐系统中用户会经常看到不同的推荐结果。</p><p>那么推荐系统的时间多样性和用户满意度之间是否存在关系呢？时间多样性高是否就能提高用户的满意度？为了解答这些问题，英国研究人员进行了一次实验(参见Neal Lathia、Stephen Hailes、Licia Capra和Xavier Amatriain的“Temporal Diversity in Recommender Systems”（SIGIR 2010）)，他们设计了3种推荐系统，证明了时间多样性对推荐系统的正面意义，书上有关于这个实验的简略介绍。</p><p>之后的问题就是如何在不损失精度的情况下提高推荐结果的时间多样性。<br>提高推荐结果的时间多样性需要分两步解决：首先，<em>需要保证推荐系统能够在用户有了新的行为后及时调整推荐结果，使推荐结果满足用户最近的兴趣</em>；其次，<em>需要保证推荐系统在用户没有新的行为时也能够经常变化一下结果，具有一定的时间多样性</em>。<br>对于第一步，又可以分成两种情况进行分析。第一是<em>从推荐系统的实时性角度分析</em>。有些推荐系统会每天离线生成针对所有用户的推荐结果，然后在线直接将这些结果展示给用户。这种类型的系统显然无法做到在用户有了新行为后及时调整推荐结果。第二，<em>即使是实时推荐系统，由于使用的算法不同，也具有不同的时间多样性</em>。对于不同算法的时间多样性，Neal Lathia博士在博士论文中进行了深入探讨(参见Neal Lathia的“Evaluating Collaborative Filtering Over Time”， 论文链接为<a href="http://www.cs.ucl.ac.uk/staff/n.lathia/thesis.html" target="_blank" rel="noopener">http://www.cs.ucl.ac.uk/staff/n.lathia/thesis.html</a> )。</p><p>紧接着需要思考的问题就是如果用户没有行为，如何保证给用户的推荐结果具有一定的时间多样性呢？一般的思路有以下几种。</p><ul><li>在生成推荐结果时加入一定的随机性。比如从推荐列表前20个结果中随机挑选10个结果 展示给用户，或者按照推荐物品的权重采样10个结果展示给用户。</li><li>记录用户每天看到的推荐结果，然后在每天给用户进行推荐时，对他前几天看到过很多次的推荐结果进行适当地降权。</li><li>每天给用户使用不同的推荐算法。</li></ul><p>当然，时间多样性也不是绝对的。推荐系统需要首先保证推荐的精度，在此基础上适当地考虑时间多样性。在实际应用中需要通过多次的实验才能知道什么程度的时间多样性对系统是最好的。</p><h3 id="5-1-6-时间上下文推荐算法"><a href="#5-1-6-时间上下文推荐算法" class="headerlink" title="5.1.6 时间上下文推荐算法"></a>5.1.6 时间上下文推荐算法</h3><p>上一节介绍了很多时间效应，本节主要讨论如何将这些时间效应应用到系统中。建模时间信息有很多方法，本节分别介绍了不同的方法，并通过实验对比这些方法。</p><h4 id="1-最近热门"><a href="#1-最近热门" class="headerlink" title="1.最近热门"></a>1.最近热门</h4><p>在没有时间信息的数据集中，可以给用户推荐历史上最热门的物品。在获得用户行为的时间信息后，就可以给用户推荐<strong>最近</strong>最热门的物品了。给定时间$T$，物品$i$最近的流行度$n_i(T)$可以定义为：<br>$$<br>n_i(T) = \sum_{(u,i,t) \in \text{Train}, t&lt;T} \frac{1}{1 + \alpha(T-t)}<br>$$<br>$\alpha$是时间衰减参数。<br>下面的Python代码实现了上面的计算公式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RecentPopularity</span><span class="params">(records, alpha, T)</span>:</span></span><br><span class="line">    ret = dict() </span><br><span class="line">    <span class="keyword">for</span> user,item,tm <span class="keyword">in</span> records:</span><br><span class="line">        <span class="keyword">if</span> tm &gt;= T:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        addToDict(ret, item, <span class="number">1</span> / (<span class="number">1.0</span> + alpha * (T - tm))) </span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure></p><h4 id="2-时间上下文相关的ItemCF算法"><a href="#2-时间上下文相关的ItemCF算法" class="headerlink" title="2.时间上下文相关的ItemCF算法"></a>2.时间上下文相关的ItemCF算法</h4><p>基于物品（item-based）的个性化推荐算法是商用推荐系统中应用最广泛的，从前面几章的讨论可以看到，该算法由两个核心部分构成：</p><ul><li>利用用户行为离线计算物品之间的相似度；</li><li>根据用户的历史行为和物品相似度矩阵，给用户做在线个性化推荐。</li></ul><p>时间信息在上面两个核心部分中都有重要的应用，这体现在两种时间效应上。</p><ul><li><strong>物品相似度</strong>   用户在相隔很短的时间内喜欢的物品具有更高相似度。</li><li><strong>在线推荐</strong>    用户近期行为相比用户很久之前的行为，更能体现用户现在的兴趣。因此在预测用户现在的兴趣时，应该加重用户近期行为的权重，优先给用户推荐那些和他近期喜欢的物品相似的物品。</li></ul><p>首先回顾一下前面提到的基于物品的协同过滤算法，它通过如下公式计算物品的相似度：<br>$$<br>sim(i,j) = \frac{\sum_{u \in N(i) \cap N(j)}1}{\sqrt{\vert N(i) \vert \vert N(j)\vert}}<br>$$<br>而在给用户$u$做推荐时，用户$u$对物品$i$的兴趣$p(u,i)$通过如下公式计算：<br>$$<br>p(u,i) = \sum_{j \in N(u)} sim(i,j)<br>$$<br>在得到时间信息（用户对物品产生行为的时间）后，可以通过如下公式改进相似度计算：<br>$$<br>sim(i,j) = \frac{\sum_{u \in N(i)\cap N(j)} f(\vert t_{ui} - t_{uj} \vert)}{\sqrt{\vert N(i) \vert \vert N(j) \vert}}<br>$$<br>注意，上面的公式在分子中引入了和时间有关的<strong>衰减项</strong>$f(\vert t_{ui} - t_{uj} \vert)$，其中$t_{ui}$是用户$u$对物品$i$产生行为的时间。$f$函数的含义是，用户对物品$i$和物品$j$产生行为的时间越远，则$f(\vert t_{ui} - t_{uj} \vert)$越小。实际上有很多数学衰减函数，本节使用如下衰减函数：<br>$$<br>f(\vert t_{ui} - t_{uj} \vert)=\frac{1}{1 + \alpha \vert t_{ui} - t_{uj} \vert}<br>$$<br>$\alpha$是时间衰减参数，它的取指在不同系统中不同。如果一个系统用户兴趣变化很快，就应该 取比较大的$\alpha$，反之需要取比较小的$\alpha$。</p><p>改进后ItemCF的相似度可以通过如下代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ItemSimilarity</span><span class="params">(train, alpha)</span>:</span></span><br><span class="line">    <span class="comment">#calculate co-rated users between items </span></span><br><span class="line">    C = dict() </span><br><span class="line">    N = dict() </span><br><span class="line">    <span class="keyword">for</span> u, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> i,tui <span class="keyword">in</span> items.items():</span><br><span class="line">            N[i] += <span class="number">1</span> </span><br><span class="line">            <span class="keyword">for</span> j,tuj <span class="keyword">in</span> items.items():</span><br><span class="line">                <span class="keyword">if</span> i == j:</span><br><span class="line">                   <span class="keyword">continue</span> </span><br><span class="line">                C[i][j] += <span class="number">1</span> / (<span class="number">1</span> + alpha * abs(tui - tuj))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#calculate finial similarity matrix W </span></span><br><span class="line">    W = dict() </span><br><span class="line">    <span class="keyword">for</span> i,related_items <span class="keyword">in</span> C.items():</span><br><span class="line">        <span class="keyword">for</span> j, cij <span class="keyword">in</span> related_items.items(): </span><br><span class="line">            W[u][v] = cij / math.sqrt(N[i] * N[j]) </span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure><p>除了考虑时间信息对<strong>相关表</strong>的影响，也应该考虑时间信息对<strong>预测公式</strong>的影响。一般来说， 用户现在的行为应该和用户最近的行为关系更大。因此，可以通过如下方式<u>修正</u>预测公式：<br>$$<br>p(u,i) = \sum_{j \in N(u) \cap S(i,k)} sim(i,j) \frac{1}{1+ \beta \vert t_0 - t_{uj} \vert}<br>$$<br>其中，$t_0$是当前时间。上面的公式表明，$t_{uj}$越靠近$t_0$，和物品$j$相似的物品就会在用户$u$的推荐列表中获得越高的排名。$\beta$是时间衰减参数，需要根据不同的数据集选择合适的值。上面的推荐算法可以通过如下代码实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Recommendation</span><span class="params">(train, user_id, W, K, t0)</span>:</span></span><br><span class="line">    rank = dict() ru = train[user_id] </span><br><span class="line">    <span class="keyword">for</span> i,pi <span class="keyword">in</span> ru.items():</span><br><span class="line">        <span class="keyword">for</span> j, wj <span class="keyword">in</span> sorted(W[i].items(), \ key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:K]: </span><br><span class="line">            <span class="keyword">if</span> j,tuj <span class="keyword">in</span> ru.items():</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            rank[j] += pi * wj / (<span class="number">1</span> + alpha * (t0 - tuj)) </span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure><h4 id="3-时间上下文相关的UserCF算法"><a href="#3-时间上下文相关的UserCF算法" class="headerlink" title="3.时间上下文相关的UserCF算法"></a>3.时间上下文相关的UserCF算法</h4><p>和ItemCF算法一样，UserCF算法同样可以利用时间信息提高预测的准确率。首先，回顾一下前面关于UserCF算法的<u>基本思想</u>：给用户推荐和他兴趣相似的其他用户喜欢的物品。从这个基本思想出发，可以在以下两个方面利用时间信息改进UserCF算法。</p><ul><li><strong>用户兴趣相似度</strong>    如果两个用户同时喜欢相同的物品，那么 这两个用户应该有更大的兴趣相似度。</li><li><strong>相似兴趣用户的最近行为</strong>   应该给用户推荐和他兴趣相似的用户最近喜欢的物品。</li></ul><p>回顾一下UserCF的推荐公式。UserCF通过如下公式计算用户$u$和用户$v$的兴趣相似度：<br>$$<br>w_{uv} = \frac{\vert N(u) \cap N(v) \vert}{\sqrt{\vert N(u) \vert \cup \vert N(u) \vert}}<br>$$<br>其中$N(u)$是用户$u$喜欢的物品集合，$N(v)$是用户$v$喜欢的物品集合。可以利用如下方式考虑时间信息：<br>$$<br>w_{uv} = \frac{\sum_{i \in N(u) \cap N(v)} \frac{1}{1 +\alpha \vert t_{ui} - t_{vi} \vert}}{\sqrt{\vert N(u) \vert \cup \vert N(u) \vert}}<br>$$<br>上面公式的分子对于用户$u$和用户$v$共同喜欢的物品$i$增加了一个<strong>时间衰减因子</strong>。用户$u$和用户$v$对物品$i$产生行为的时间越远，那么这两个用户的兴趣相似度就会越小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UserSimilarity</span><span class="params">(train)</span>:</span></span><br><span class="line">    <span class="comment"># build inverse table for item_users </span></span><br><span class="line">    item_users = dict() </span><br><span class="line">    <span class="keyword">for</span> u, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> i,tui <span class="keyword">in</span> items.items():</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> item_users: </span><br><span class="line">                item_users[i] = dict() </span><br><span class="line">            item_users[i][u] = tui</span><br><span class="line">            </span><br><span class="line">    <span class="comment">#calculate co-rated items between users </span></span><br><span class="line">    C = dict() </span><br><span class="line">    N = dict() </span><br><span class="line">    <span class="keyword">for</span> i, users <span class="keyword">in</span> item_users.items():</span><br><span class="line">        <span class="keyword">for</span> u,tui <span class="keyword">in</span> users.items():</span><br><span class="line">            N[u] += <span class="number">1</span> </span><br><span class="line">            <span class="keyword">for</span> v,tvi <span class="keyword">in</span> users.items():</span><br><span class="line">                <span class="keyword">if</span> u == v:</span><br><span class="line">                    <span class="keyword">continue</span> </span><br><span class="line">                C[u][v] += <span class="number">1</span> / (<span class="number">1</span> + alpha * abs(tui - tvi))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#calculate finial similarity matrix W </span></span><br><span class="line">    W = dict() </span><br><span class="line">    <span class="keyword">for</span> u, related_users <span class="keyword">in</span> C.items():</span><br><span class="line">        <span class="keyword">for</span> v, cuv <span class="keyword">in</span> related_users.items(): </span><br><span class="line">            W[u][v] = cuv / math.sqrt(N[u] * N[v]) </span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure><p>在得到用户相似度后，UserCF通过如下公式预测用户对物品的兴趣：<br>$$<br>p(u,i) = \sum_{v \in S(u,K)} w_{uv} r_{vi}<br>$$<br>其中，$S(u,K)$包含了和用户$u$兴趣最接近的$K$个用户。如果用户$v$对物品$i$产生过行为，那么$r_{vi}=1$，否则$r_{vi} = 0$。</p><p>如果考虑和用户$u$兴趣相似用户的最近兴趣，可以设计如下公式：<br>$$<br>p(u,i) = \sum_{v \in S(u,K)} w_{uv} r_{vi} \frac{1}{1 + \alpha(t_0 + t_{vi})}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Recommend</span><span class="params">(user, T, train, W)</span>:</span></span><br><span class="line">    rank = dict() </span><br><span class="line">    interacted_items = train[user] </span><br><span class="line">    <span class="keyword">for</span> v, wuv <span class="keyword">in</span> sorted(W[u].items, key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:K]:</span><br><span class="line">        <span class="keyword">for</span> i, tvi <span class="keyword">in</span> train[v].items:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> interacted_items:</span><br><span class="line">                <span class="comment">#we should filter items user interacted before </span></span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            rank[i] += wuv / (<span class="number">1</span> + alpha * (T - tvi)) </span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure><h3 id="5-1-7-时间段图模型"><a href="#5-1-7-时间段图模型" class="headerlink" title="5.1.7 时间段图模型"></a>5.1.7 时间段图模型</h3><p>本书的作者在KDD会议上曾经提出过一个时间段图模型(参见Liang Xiang、Quan Yuan、Shiwan Zhao、Li Chen、Xiatian Zhang、Qing Yang和Jimeng Sun的“Temporal recommendation on graphs via long- and short-term preference fusion”（ACM 2010 Article，2010）)，试图解决如何将时间信息建模到图模型中的方法，最终取得了不错的效果。<br>时间段图模型$G (U , S_U , I , S_I , E , w,\sigma )$也是一个二分图。$U$是用户节点集合，$S_U$是用户时间段节点集合。一个用户时间段节点$v_{ut} \in S_U$会和用户$u$在时刻$t$喜欢的物品通过边相连。$I$是物品节点集合，$S_I$是物品时间段节点集合。一个物品时间段节点 $v_{it} \in S_I$会和所有在时刻$t$喜欢物品$i$的用户通过边相连。$E$是边集合，它包含了3种边：(1)如果用户$u$对物品$i$有行为，那么存在边$e(v_u,v_i) \in E$；(2)如果用户$u$在$t$时刻对物品$i$有行为，那么就存在两条边$e(v_{ut},v_i )$, $e(v_u, v_{it} ) \in E$。$w(e)$定义了边的权重，$\sigma (e)$定义了顶点的权重。</p><p>定义完图的结构后，最简单的想法是可以利用前面提到的<strong>PersonalRank算法</strong>给用户进行个性化推荐。但是因为这个算法需要在全图上进行迭代计算，所以时间复杂度比较高。因此作者提出了一种称为<strong>路径融合算法</strong>的方法，通过该算法来度量图上两个顶点的相关性。<br>一般来说，图上两个相关性比较高的顶点一般具有如下特征：</p><ul><li>两个顶点之间有很多路径相连；</li><li>两个顶点之间的路径比较短；</li><li>两个顶点之间的路径不经过出度比较大的顶点。</li></ul><p>从这3条原则出发，路径融合算法首先提取出两个顶点之间长度小于一个阈值的所有路径，然后根据每条路径经过的顶点给每条路径赋予一定的权重，最后将两个顶点之间所有路径的权重之和作为两个顶点的相关度。</p><p>假设$P={v_1,v_2,···,v_N}$是连接顶点$v_1$和$v_n$的一条路径，这条路径的权重$\Gamma(P)$取决于这条路径经过的所有顶点和边：<br>$$<br>\Gamma(p) = \sigma(v_n) \prod_{i=1}^{n-1} \frac{\sigma (v_i) \cdot w(v_I,v_{i+1})}{\vert out(v_i) \vert^\rho}<br>$$<br>这里$out(v)$是顶点$v$指向的顶点集合，$|out(v)|$是顶点$v$的出度，$\sigma(v_i) \in (0,1]$定义了顶点的权重，$w(v_i,v_{i+1})$定义了边$e(v_i,v_{i+1})$的权重。上面的定义符合上面3条原则的后两条。首先，因为$\frac{\sigma(v_i) \cdot w(v_i,v_{i+1})}{\vert out(v_i)\vert^\rho}$，所以路径越长$n$越大，$\Gamma(P)$就越小。同时，如果路径经过了出度大的顶点v’，那么因为$|out(v’)|$比较大，所以$\Gamma(p)$也会比较小。</p><p>在定义了一条路径的权重后，就可以定义顶点之间的相关度。对于顶点$v$和$v’$，令$p(v, v’, K )$为这两个顶点间距离小于K的所有路径，那么这两个顶点之间的相关度可以定义为：<br>$$<br>d(v,v’) = \sum_{P \in P(v,v’,K)} \Gamma(P)<br>$$<br>对于时间段图模型，所有边的权重都定义为1，而顶点的权重$\sigma(v)$定义如下：<br>$$<br>\sigma(v) =<br>\begin{cases}<br>1- \alpha &amp;(v \in U) \\<br>\alpha &amp;(v \in S_U) \\<br>1 - \beta &amp; (v \in I) \\<br>\beta &amp; (v \in S_I)<br>\end{cases}<br>$$<br>$\alpha,\beta \in [0,1]$是两个参数，控制了不同顶点的权重。</p><p>路径融合算法可以基于图上的<strong>广度优先搜索算法</strong>实现，下面的Python代码简单实现了路径融合算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PathFusion</span><span class="params">(user, time,G,alpha)</span></span></span><br><span class="line">    Q = [] </span><br><span class="line">    V = set() </span><br><span class="line">    depth = dict() </span><br><span class="line">    rank = dict() </span><br><span class="line">    depth[<span class="string">'u:'</span> + user] = <span class="number">0</span> </span><br><span class="line">    depth[<span class="string">'ut:'</span> + user + <span class="string">'_'</span> + time] = <span class="number">0</span> </span><br><span class="line">    rank [<span class="string">'u:'</span> + user] = alpha </span><br><span class="line">    rank [<span class="string">'ut:'</span> + user + <span class="string">'_'</span> + time] = <span class="number">1</span> - alpha</span><br><span class="line">    Q.append(<span class="string">'u:'</span> + user)</span><br><span class="line">    Q.append(<span class="string">'ut:'</span> + user + <span class="string">'_'</span> + time) </span><br><span class="line">    <span class="keyword">while</span> len(Q) &gt; <span class="number">0</span>:</span><br><span class="line">        v = Q.pop() </span><br><span class="line">        <span class="keyword">if</span> v <span class="keyword">in</span> V:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> depth[v] &gt; <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> v2,w <span class="keyword">in</span> G[v].items():</span><br><span class="line">            <span class="keyword">if</span> v2 <span class="keyword">not</span> <span class="keyword">in</span> V:</span><br><span class="line">                depth[v2] = depth[v] + <span class="number">1</span> </span><br><span class="line">                Q.append(v2) </span><br><span class="line">            rank[v2] = rank[v] * w </span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure><h3 id="5-1-8-离线实验"><a href="#5-1-8-离线实验" class="headerlink" title="5.1.8 离线实验"></a>5.1.8 离线实验</h3><p>为了证明时间上下文信息对推荐系统至关重要，本节利用了离线实验对比使用时间信息后不同推荐算法的离线性能，感兴趣的看一下书。</p><h2 id="5-2-地点上下文信息"><a href="#5-2-地点上下文信息" class="headerlink" title="5.2 地点上下文信息"></a>5.2 地点上下文信息</h2><p>除了时间，地点作为一种重要的空间特征，也是一种重要的上下文信息。不同地区的用户兴趣有所不同，用户到了不同的地方，兴趣也会有所不同。</p><p>西班牙电信的研究人员曾经设计过一个基于位置的电影推荐系统，并且提供了详细的技术报告(参见“Geolocated Recommendations”，地址为<a href="http://xavier.amatriain.net/pubs/GeolocatedRecommendations.pdf" target="_blank" rel="noopener">http://xavier.amatriain.net/pubs/GeolocatedRecommendations.pdf</a> )。该报告详细地介绍了如何在iPhone上开发一个推荐系统，如何在电影推荐中融入用户的位置信息，感兴趣的读者可以仔细阅读他们的报告。</p><h3 id="基于位置的推荐算法"><a href="#基于位置的推荐算法" class="headerlink" title="基于位置的推荐算法"></a>基于位置的推荐算法</h3><p>明尼苏达大学的研究人员提出过一个称为<strong>LARS（Location Aware Recommender System,位置感知推荐系统）</strong>的和用户地点相关的推荐系统。该系统首先将物品分成两类，一类是<em>有空间属性的</em>，比如餐馆、商店、旅游景点等，另一类是<em>无空间属性的物品</em>，比如图书和电影等。同时，它将用户也分成两类，一类是<em>有空间属性的</em>，比如给出了用户现在的地址（国家、城市、邮编等）， 另一类用户<em>并没有相关的空间属性信息</em>。它使用的数据集有3种不同的形式。</p><ul><li>（用户，用户位置，物品，评分），每一条记录代表了某一个地点的用户对物品的评分。它们使用的是MovieLens数据集。该数据集给出了用户的邮编，从而可以知道用户的大致地址。</li><li>（用户，物品，物品位置，评分），每一条记录代表了用户对某个地方的物品的评分。LARS使用了FourSquare的数据集，该数据集包含用户对不同地方的餐馆、景点、商店的评分</li><li>（用户，用户位置，物品，物品位置，评分），每一条记录代表了某个位置的用户对某个位置的物品的评分。</li></ul><p>LARS通过研究前两种数据集，发现了用户兴趣和地点相关的两种特征。</p><ul><li><strong>兴趣本地化</strong>    不同地方的用户兴趣存在着很大的差别。不同国家和地区用户的兴趣存在着一定的差异性。</li><li><strong>活动本地化</strong>    一个用户往往在附近的地区活动。通过分析Foursqure的数据，研究人员发现45%的用户其活动范围半径不超过10英里，而75%的用户活动半径不超过50英里。</li></ul><p>对于第一种数据集，LARS的<u>基本思想</u>是将数据集根据用户的位置划分成很多子集。因为位置信息是一个树状结构，比如国家、省、市、县的结构。因此，数据集也会划分成一个<strong>树状结构</strong>。然后，给定每一个用户的位置，可以将他分配到某一个<strong>叶子节点</strong>中，而该叶子节点包含了所有和他同一个位置的用户的行为数据集。然后，LARS就利用这个叶子节点上的用户行为数据，通过ItemCF给用户进行推荐。</p><p>不过这样做的<u>缺点</u>是，每个叶子节点上的用户数量可能很少，因此他们的行为数据可能过于稀疏，从而无法训练出一个好的推荐算法。为此，我们可以从根节点出发，在到叶子节点的过程中，利用每个中间节点上的数据训练出一个推荐模型，然后给用户生成推荐列表。而最终的推荐结果是这一系列推荐列表的加权。文章的作者将这种算法成为<strong>金字塔模型</strong>，而金字塔的<strong>深度</strong>影响了推荐系统的性能，因而深度是这个算法的一个重要指标。下文用<strong>LARS-U</strong>代表该算法，书中有关于该算法的简单例子。</p><p>对于第二种数据集，每条用户行为表示为四元组（用户、物品、物品位置、评分），表示了用户对某个位置的物品给了某种评分。对于这种数据集，LARS会首先忽略物品的位置信息，利用ItemCF算法计算用户$u$对物品$i$的兴趣$P(u,i)$，但最终物品$i$在用户$u$的推荐列表中的权重定义为：<br>$$<br>RecScore(u,i) = P(u,i) - TravelPenalty(u,i)<br>$$<br>在该公式中，$TravelPenalty(u,i)$表示了物品$i$的位置对用户$u$的代价。 计算 $TravelPenalty(u,i)$的<u>基本思想</u>是对于物品$i$与用户$u$之前评分的所有物品的位置计算距离的平均值 （或者最小值）。关于如何度量地图上两点的距离，最简单的是基于<strong>欧式距离</strong>(参见Gísli R. Hjaltason和Hanan Samet的“Distance browsing in spatial databases”（ACM 1999 Article，1999）)。当然，欧式距离有明显的<u>缺点</u>，因为人们是不可能沿着地图上的直线距离从一点走到另一点的。比较好的度量方式是利用交通网络数据，将人们实际需要走的最短距离作为距离度量(参见Jie Bao、Chi-Yin Chow、Mohamed F. Mokbel和Wei-Shinn Ku的“Efficient Evaluation of k-Range Nearest Neighbor Queries in Road Networks”（MDM，2012）)。</p><p>为了避免计算用户对所有物品的$TravelPenalty$，LARS在计算用户$u$对物品$i$的兴趣度$RecScore(u,i)$时，首先对用户每一个曾经评过分的物品（一般是餐馆、商店、景点），找到和他距离小于一个阈值$d$的所有其他物品，然后将这些物品的集合作为候选集，然后再利用上面的公式计算最终的$RecScore$。</p><p>对于第三种数据集，LARS一文没有做深入讨论。不过，从第三种数据集的定义可以看到， 它相对于第二种数据集增加了用户当前位置这一信息。而在给定了这一信息后，应该保证推荐的物品应该距离用户当前位置比较近，在此基础上再通过用户的历史行为给用户推荐离他近且他会感兴趣的物品。</p><p>为了证明兴趣本地化和活动本地化两种效应，论文作者在FourSquare和MovieLens两个数据集上进行了离线实验。论文作者使用TopN推荐的Precision作为评测指标。</p><p>作者首先在FourSquare数据集上对比了ItemCF算法和考虑了TravelPenalty之后的算法（简称为<strong>LARS-T</strong>）。结果证明考虑TravelPenality确实能够提高TopN推荐的离线准确率，LARS-T算法明显优于ItemCF算法。</p><p>然后，作者在FourSquare数据集和MovieLens数据集上对比了普通的ItemCF算法和考虑用户位置的金字塔模型后的LARS-U算法。同时，作者对比了不同深度对LARS-U算法的影响。实验表明，选择合适的深度对LARS-U算法很重要，不过在绝大多数深度的选择下，LARS-U算法在两个数据集上都优于普通的ItemCF算法。</p><h2 id="5-3-扩展阅读"><a href="#5-3-扩展阅读" class="headerlink" title="5.3 扩展阅读"></a>5.3 扩展阅读</h2><p>时间上下文信息在Netflix Prize中得到了广泛关注，很多参赛者都研究了如何利用这一信息。这方面最著名的文章无疑是Koren的“collaborative filtering with temporal dynamics”，该文系统地总结了各种使用时间信息的方式，包括考虑用户近期行为的影响，考虑时间的周期性等。</p><p>英国剑桥大学的Neal Lathia在读博士期间对时间上下文信息以及推荐系统的时间效应进行了深入研究。他在“Temporal Diversity in Recommender Systems”一文中深入分析了时间多样性对推荐系统的影响。他的博士论文“Evaluating Collaborative Filtering Over Time”论述了各种不同推荐算法是如何随时间演化的。</p><p>如果要系统地研究与上下文推荐相关的工作， 可以参考Alexander Tuzhili 教授的工作（<a href="http://pages.stern.nyu.edu/~atuzhili/" target="_blank" rel="noopener">http://pages.stern.nyu.edu/~atuzhili/</a> ），他在最近几年和学生对上下文推荐问题进行了深入研究。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第5章-利用上下文信息&quot;&gt;&lt;a href=&quot;#第5章-利用上下文信息&quot; class=&quot;headerlink&quot; title=&quot;第5章 利用上下文信息&quot;&gt;&lt;/a&gt;第5章 利用上下文信息&lt;/h1&gt;&lt;p&gt;上下文信息对于提高推荐系统的各项评测指标是十分重要的，因此不能忽略上下文信息。准确了解用户的&lt;strong&gt;上下文信息(context)&lt;/strong&gt;，并将该信息应用于推荐算法是设计好的推荐系统的&lt;u&gt;关键步骤&lt;/u&gt;。&lt;br&gt;关于上下文推荐的研究， 可以参考Alexander Tuzhilin(个人主页为&lt;a href=&quot;http://people.stern.nyu.edu/atuzhili/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://people.stern.nyu.edu/atuzhili/&lt;/a&gt; )教授的一篇综述“Context Aware Recommender Systems”。&lt;/p&gt;
&lt;p&gt;本章主要讨论了&lt;strong&gt;时间上下文&lt;/strong&gt;，并简单介绍一下&lt;strong&gt;地点上下文&lt;/strong&gt;，讨论如何将时间信息和地点信息建模到推荐算法中，从而让推荐系统能够准确预测用户在某个特定时刻及特定地点的兴趣。本章仍然研究TopN推荐，即如何给用户生成一个长度为$N$的推荐列表，而该列表包含了用户在某一时刻或者某个地方最可能喜欢的物品。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实践读书笔记（四）</title>
    <link href="https://rilzob.com/2018/11/27/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>https://rilzob.com/2018/11/27/推荐系统实践读书笔记（四）/</id>
    <published>2018-11-27T09:54:35.242Z</published>
    <updated>2019-03-24T03:31:04.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第4章-利用用户标签数据"><a href="#第4章-利用用户标签数据" class="headerlink" title="第4章 利用用户标签数据"></a>第4章 利用用户标签数据</h1><p>GroupLens在一篇文章中(文章名是“Tagsplanations : Explaining Recommendations using Tags”)表示目前流行的推荐系统基本上通过3种方式联系用户兴趣和物品。</p><ul><li><strong>基于物品的算法</strong> 比如UserCF</li><li><strong>基于用户的算法</strong> 比如ItemCF</li><li><strong>基于特征的算法</strong> 比如隐语义模型</li></ul><p><img src="/2018/11/27/推荐系统实践读书笔记（四）/推荐系统联系用户和物品的几种途径.png" alt="推荐系统联系用户和物品的几种途径"></p><p>本章将讨论一种重要的特征表现方式——<strong>标签</strong>。<br>根据维基百科的定义(参见<a href="http://en.wikipedia.org/wiki/Tag_(metadata)" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Tag_(metadata)</a> )，标签是一种无层次化结构的、用来描述信息的关键词，它可以用来描述物品的语义。根据给物品打标签的人的不同，标签应用一般分为两种：一种是<em>让作者或者专家给物品打标签</em>；另一种是<em>让普通用户给物品打标签</em>，也就是<strong>UGC(User Generated Content，用户生成的内容)</strong>的标签应用。UGC的标签系统是一种表示用户兴趣和物品语义的重要方式。当一个用户对一个物品打上一个标签，这个标签一方面描述了用户的兴趣，另一方面则表示了物品的语义，从而将用户和物品联系了起来。本章主要讨论UGC的标签应用，研究用户给物品打标签的行为，探讨如何通过分析这种行为给用户进行个性化推荐。<br><a id="more"></a></p><h2 id="4-1-UGC标签系统的代表应用"><a href="#4-1-UGC标签系统的代表应用" class="headerlink" title="4.1 UGC标签系统的代表应用"></a>4.1 UGC标签系统的代表应用</h2><h3 id="4-1-1-Delicious"><a href="#4-1-1-Delicious" class="headerlink" title="4.1.1 Delicious"></a>4.1.1 Delicious</h3><p>Delicious允许用户给互联网上的每个网页打标签，从而通过标签重新组织整个互联网。</p><h3 id="4-1-2-CiteULike"><a href="#4-1-2-CiteULike" class="headerlink" title="4.1.2 CiteULike"></a>4.1.2 CiteULike</h3><p>CiteULike是一个著名的论文书签网站，它允许研究人员提交或者收藏自己感兴趣的论文并且给论文打标签，从而帮助用户更好地发现和自己研究领域相关的优秀论文。</p><h3 id="4-1-3-Last-fm"><a href="#4-1-3-Last-fm" class="headerlink" title="4.1.3 Last.fm"></a>4.1.3 Last.fm</h3><p>Last.fm在前面已经介绍过，是一家著名的音乐网站，它通过分析用户的听歌行为预测用户对音乐的兴趣，从而给用户推荐个性化的音乐。</p><h3 id="4-1-4-豆瓣"><a href="#4-1-4-豆瓣" class="headerlink" title="4.1.4 豆瓣"></a>4.1.4 豆瓣</h3><p>豆瓣允许用户对图书和电影打标签，借此获得图书和电影的内容信息和语义，并用这种信息改善推荐效果。</p><h3 id="4-1-5-Hulu"><a href="#4-1-5-Hulu" class="headerlink" title="4.1.5 Hulu"></a>4.1.5 Hulu</h3><p>Hulu是美国著名的视频网站。视频作为一种最为复杂的多媒体，获取它的内容信息是最困难的，因此Hulu也引入了用户标签系统来让用户对电视剧和电影进行标记。</p><p>总结以上标签系统的各种应用，标签系统的<u>最大优势</u>在于可以发挥群体的智能，获得对物品内容信息比较准确的关键词描述，而准确的内容信息是提升个性化推荐系统性能的重要资源。</p><p>关于标签系统的作用， GroupLen的Shilads Wieland Sen在MoveLens电影推荐系统上做了更为深入的、基于问卷调查的研究。在博士论文(博士论文为“Nurturing Tagging Communities”)中，他探讨了标签系统的不同作用，以及每种作用能够影响多大的人群，如下所示。</p><ul><li><strong>表达</strong> 标签系统帮助我表达对物品的看法。（30%的用户同意。）</li><li><strong>组织</strong> 打标签帮助我组织我喜欢的电影。（23%的用户同意。）</li><li><strong>学习</strong> 打标签帮助我增加对电影的了解。（27%的用户同意。）</li><li><strong>发现</strong> 标签系统使我更容易发现喜欢的电影。（19%的用户同意。）</li><li><strong>决策</strong> 标签系统帮助我判定是否看某一部电影。（14%的用户同意。）</li></ul><p>上面的研究表明，标签系统确实能够帮助用户发现可能喜欢的电影。</p><h2 id="4-2-标签系统中的推荐问题"><a href="#4-2-标签系统中的推荐问题" class="headerlink" title="4.2 标签系统中的推荐问题"></a>4.2 标签系统中的推荐问题</h2><p>标签系统中的推荐问题主要有以下两个。</p><ul><li>如何利用用户打标签的行为为其推荐物品（基于标签的推荐）？</li><li>如何在用户给物品打标签时为其推荐适合该物品的标签（标签推荐）？</li></ul><p>为了研究上面的问题，首先需要解答下面3个问题。</p><ul><li>用户为什么要打标签？</li><li>用户怎么打标签？</li><li>用户打什么样的标签？</li></ul><h3 id="4-2-1-用户为什么进行标注"><a href="#4-2-1-用户为什么进行标注" class="headerlink" title="4.2.1 用户为什么进行标注"></a>4.2.1 用户为什么进行标注</h3><p>Morgan Ames研究图片分享网站中用户标注的动机问题，并从两个维度进行探讨(参见Morgan Ames和 Mor Naaman的“Why we tag: motivations for annotation in mobile and online media”（ CHI 2007，2007）)。首先是<em>社会维度</em>，有些用户标注是给内容上传者使用的（便于上传者组织自己的信息），而有些用户标注是给广大用户使用的（便于帮助其他用户找到信息）。另一个维度是<em>功能维度</em>，有些标注用于更好地组织内容，方便用户将来的查找，而另一些标注用于传达某种信息，比如照片的拍摄时间和地点等。</p><h3 id="4-2-2-用户如何打标签"><a href="#4-2-2-用户如何打标签" class="headerlink" title="4.2.2 用户如何打标签"></a>4.2.2 用户如何打标签</h3><p>该节通过研究Delicious数据集总结用户标注行为中的一些统计规律，即标签的流行度分布同用户活跃度、物品流行度分布一致都遵循<strong>长尾分布(Power Law分布)</strong>，实验具体过程见书。</p><h3 id="4-2-3-用户打什么样的标签"><a href="#4-2-3-用户打什么样的标签" class="headerlink" title="4.2.3 用户打什么样的标签"></a>4.2.3 用户打什么样的标签</h3><p>用户在对物品打标签时，可能并非如我们希望地提供能够准确描述物品内容属性的关键词，而是各种奇怪的标签。</p><h4 id="Delicious上的标签分类"><a href="#Delicious上的标签分类" class="headerlink" title="Delicious上的标签分类"></a>Delicious上的标签分类</h4><p>Scott A. Golder 总结了Delicious上的标签，将它们分为如下几类。</p><ul><li><strong>表明物体是什么</strong></li><li><strong>表明物品的种类</strong></li><li><strong>表明谁拥有物品</strong></li><li><strong>表达用户的观点</strong></li><li><strong>用户相关的标签</strong></li><li><strong>用户的任务</strong></li></ul><h4 id="Hulu上的标签分类"><a href="#Hulu上的标签分类" class="headerlink" title="Hulu上的标签分类"></a>Hulu上的标签分类</h4><ul><li><strong>类型(Genre)</strong> 主要表示这个电视剧的类别。</li><li><strong>时间(Time)</strong> 主要包括电视剧的发布的时间，有时也包括电视剧中事件发生的时间。</li><li><strong>人物(People)</strong> 主要包括电视剧的导演、演员和剧中重要人物等。</li><li><strong>地点(Place)</strong> 剧情发生的地点，或者视频拍摄的地点等。</li><li><strong>语言(Language)</strong> 这部电视剧使用的语言。</li><li><strong>奖项(Awards)</strong> 这部电视剧获得的相关奖项。</li><li><strong>其他(Details)</strong> 包含不能归类到上面各类中的其他所有标签。</li></ul><h2 id="4-3-基于标签的推荐系统"><a href="#4-3-基于标签的推荐系统" class="headerlink" title="4.3 基于标签的推荐系统"></a>4.3 基于标签的推荐系统</h2><p>一个用户标签行为的数据集一般由一个三元组的集合表示，其中记录$(u, i, b)$表示用户$u$给物品$i$打上了标签$b$。当然，用户的真实标签行为数据远远比三元组表示的要复杂，比如用户打标签的时间、用户的属性数据、物品的属性数据等。但是本章为了集中讨论标签数据，只考虑上面定义的三元组形式的数据，即用户的每一次打标签行为都用一个三元组（用户、物品、标签）表示。</p><p>本章将采用两个不同的数据集评测基于标签的物品推荐算法。一个是<em>Delicious数据集</em>，另一个是<em>CiteULike数据集</em>。Delicious数据集中包含用户对网页的标签记录。它每一行由4部分组成，即时间、用户ID、网页URL、标签。CiteULike数据集包含用户对论文的标签记录，它每行也由4部分组成，即物品ID、用户ID、时间、标签。</p><h3 id="4-3-1-实验设置"><a href="#4-3-1-实验设置" class="headerlink" title="4.3.1 实验设置"></a>4.3.1 实验设置</h3><p>本节将数据集随机分成10份。这里分割的键值是用户和物品，不包括标签。也就是说，用户对物品的多个标签记录要么都被分进训练集，要么都被分进测试集，不会一部分在训练集，另一部分在测试集中。然后，我们挑选1份作为测试集，剩下的9份作为训练集，通过学习训练集中的用户标签数据预测测试集上用户会给什么物品打标签。对于用户$u$，令$R(u)$为给用户$u$的长度为$N$的推荐列表，里面包含我们认为用户会打标签的物品。令$T(u)$是测试集中用户$u$实际上打过标签的物品集合。然后，利用<strong>准确率(precision)</strong>和<strong>召回率(recall)</strong>评测个性化推荐算法的精度。<br>将上面的实验进行10次，每次选择不同的测试集，然后将每次实验的准确率和召回率的平均值作为最终的评测结果。</p><p>为了全面评测个性化推荐的性能，实验同时评测了推荐结果的<strong>覆盖率(coverage)</strong>、<strong>多样性(diversity)</strong>和<strong>新颖度</strong>。</p><p>关于多样性，在第1章中讨论过，多样性的定义取决于相似度的定义。在本章中，实验用物品标签向量的余弦相似度度量物品之间的相似度。对于每个物品$i$，<code>item_tags[i]</code>存储了物品$i$的标签向量，其中<code>item_tags[i][b]</code>是对物品$i$打标签$b$的次数，那么物品$i$和$j$的余弦相似度可以通过如下程序计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CosineSim</span><span class="params">(item_tags, i, j)</span>:</span></span><br><span class="line">    ret = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> b,wib <span class="keyword">in</span> item_tags[i].items():</span><br><span class="line">        <span class="keyword">if</span> b <span class="keyword">in</span> item_tags[j]:</span><br><span class="line">            ret += wib * item_tags[j][b] </span><br><span class="line">    ni = <span class="number">0</span> </span><br><span class="line">    nj = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> item_tags[i].items():</span><br><span class="line">        ni += w * w </span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> item_tags[j].items():</span><br><span class="line">        nj += w * w </span><br><span class="line">    <span class="keyword">if</span> ret == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> </span><br><span class="line">    <span class="keyword">return</span> ret / math.sqrt(ni * nj)</span><br></pre></td></tr></table></figure><p>在得到物品之间的相似度度量后，通过如下公式计算一个推荐列表的多样性。<br>$$<br>Diversity = 1 - \frac {\sum_{i \in R(u)} \sum_{j \in R(u),j \neq i} \text{Sim(item_tags[i],item_tags[j])}}<br>{\begin{pmatrix}<br>\vert R(u) \vert \\<br>2  \\<br>\end{pmatrix}}<br>$$</p><p>如果用程序实现，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Diversity</span><span class="params">(item_tags, recommend_items)</span>:</span></span><br><span class="line">    ret = <span class="number">0</span> </span><br><span class="line">    n = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> recommend_items.keys():</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> recommend_items.keys():</span><br><span class="line">            <span class="keyword">if</span> i == j:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            ret += CosineSim(item_tags, i, j) </span><br><span class="line">            n += <span class="number">1</span> </span><br><span class="line">    <span class="keyword">return</span> ret / (n * <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure></p><p>推荐系统的多样性为所有用户推荐列表多样性的平均值。</p><p>至于推荐结果的新颖性，可以简单地用推荐结果的<strong>平均热门程度(AveragePopularity)</strong>度量。 对于物品$i$，定义它的流行度<code>item_pop(i)</code>为给这个物品打过标签的用户数。而对推荐系统，定义它的平均热门度如下：<br>$$<br>Average Popularity = \frac {\sum_u \sum_{i \in R(u)} \log(1 + \text{item_pop(i))}}{\sum_u \sum_{i \in R(u)}1}<br>$$</p><h3 id="4-3-2-一个最简单的算法"><a href="#4-3-2-一个最简单的算法" class="headerlink" title="4.3.2 一个最简单的算法"></a>4.3.2 一个最简单的算法</h3><p>拿到了用户标签行为数据后，最简单的个性化推荐算法描述如下：</p><ul><li>统计每个用户最常用的标签</li><li>对于每个标签，统计被打过这个标签次数最多的物品。</li><li>对于一个用户，首先找到他常用的标签，然后找到具有这些标签的最热门物品推荐给这个用户。</li></ul><p>对于上面的算法，用户 $u$ 对物品 $i$ 的兴趣公式如下：<br>$$<br>p(u,i) = \sum_b n_{u,b}n_{b,i}<br>$$</p><p>$B(u)$是用户$u$打过的标签集合，$B(i)$是物品$i$被打过的标签集合，$n_{u,b}$是用户$u$打过标签$b$的次数，$n_{b,i}$是物品$i$被打过标签$b$的次数。本章用<strong>SimpleTagBased</strong>标记这个算法。</p><p>在Python中，遵循如下规定：</p><ul><li>用<code>records</code>存储标签数据的三元组，其中<code>records[i] = [user, item, tag]</code>；</li><li>用<code>user_tags</code>存储$n_{u,b}$，其中<code>user_tags[u][b]</code> = $n_{u,b}$；</li><li>用<code>tags_items</code>存储$n_{b,i}$，其中<code>tags_item[b][i]</code> = $n_{b,i}$ ；</li></ul><p>如下程序可以从<code>records</code>中统计出<code>user_tags</code>和<code>tag_items</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InitStat</span><span class="params">(records)</span>:</span></span><br><span class="line">    user_tags = dict() </span><br><span class="line">    tag_items = dict() </span><br><span class="line">    user_items = dict() </span><br><span class="line">    <span class="keyword">for</span> user, item, tag <span class="keyword">in</span> records.items():</span><br><span class="line">        addValueToMat(user_tags, user, tag, <span class="number">1</span>)</span><br><span class="line">        addValueToMat(tag_items, tag, item, <span class="number">1</span>)</span><br><span class="line">    addValueToMat(user_items, user, item, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>统计出<code>user_tags</code>和<code>tag_items</code>之后，可以通过如下程序对用户进行个性化推荐：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Recommend</span><span class="params">(user)</span>:</span></span><br><span class="line">    recommend_items = dict() </span><br><span class="line">    tagged_items = user_items[user] </span><br><span class="line">    <span class="keyword">for</span> tag, wut <span class="keyword">in</span> user_tags[user].items():</span><br><span class="line">        <span class="keyword">for</span> item, wti <span class="keyword">in</span> tag_items[tag].items():</span><br><span class="line">            <span class="comment">#if items have been tagged, do not recommend them </span></span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> tagged_items:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> recommend_items: </span><br><span class="line">                recommend_items[item] = wut * wti </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                recommend_items[item] += wut * wti </span><br><span class="line">    <span class="keyword">return</span> recommend_items</span><br></pre></td></tr></table></figure></p><h3 id="4-3-3-算法的改进"><a href="#4-3-3-算法的改进" class="headerlink" title="4.3.3 算法的改进"></a>4.3.3 算法的改进</h3><h4 id="1-TF-IDF"><a href="#1-TF-IDF" class="headerlink" title="1.TF-IDF"></a>1.TF-IDF</h4><p>前面这个公式倾向于给热门标签对应的热门物品很大的权重，因此会造成推荐热门的物品给用户，从而降低推荐结果的新颖性。另外，这个公式利用用户的标签向量对用户兴趣建模，其中每个标签都是用户使用过的标签，而标签的权重是用户使用该标签的次数。这种建模方法的<u>缺点</u>是给热门标签过大的权重，从而不能反应用户个性化的兴趣。这里我们可以借鉴<strong>TF-IDF</strong>的思想， 对这一公式进行改进：<br>$$<br>p(u,i) = \sum_b \frac{n_{u,b}}{\log(1+ n_b^{(u)})} n_{b,i}<br>$$<br>$n_b^{(u)}$记录了标签$b$被多少个不同的用户使用过。这个算法记为<strong>TagBasedTFIDF</strong>。<br>通过TagBasedTFIDF在Delicious和CiteULike两个数据集上的离线实验结果，可知该算法在所有指标上相比SimpleTagBased算法都有提高。</p><p>同理，也可以借鉴TF-IDF的思想对热门物品进行惩罚，从而得到如下公式：<br>$$<br>p(u,i) = \sum_b \frac{n_{u,b}}{\log(1+n_b^{(u)})}\frac{n_{b,i}}{\log(1+n_i^{(u)})}<br>$$<br>$n_i^{(u)}$记录了物品$i$被多少个不同的用户打过标签。这个算法记为<strong>TagBasedTFIDF++</strong>。<br>同样通过离线实验证明和TagBasedTFIDF算法相比，除了多样性有所下降，其他指标都有明显提高。这一结果表明，适当惩罚热门标签和热门物品，在增进推荐结果个性化的同时并不会降低推荐结果的离线精度。</p><h4 id="2-数据稀疏性"><a href="#2-数据稀疏性" class="headerlink" title="2.数据稀疏性"></a>2.数据稀疏性</h4><p>对于新用户或者新物品，$B(u) \cap B(i)$中的标签数量会很少。为了提高推荐的准确率，可能要对标签集合做扩展。<br>进行标签扩展有很多方法，其中常用的有<strong>话题模型(topic model)</strong>，不过这里遵循简单的原则介绍一种<strong>基于邻域</strong>的方法。<br>标签扩展的<u>本质</u>是对每个标签找到和它相似的标签，也就是计算标签之间的相似度。最简单的相似度可以是<strong>同义词</strong>。如果有一个同义词词典，就可以根据这个词典进行标签扩展。如果没有这个词典，可以从数据中统计出标签的相似度。<br>如果认为同一个物品上的不同标签具有某种相似度，那么当两个标签同时出现在很多物品的标签集合中时，就可以认为这两个标签具有较大的相似度。对于标签$b$，令$N(b)$为有标签$b$的物品的集合，$n_{b,i}$为给物品$i$打上标签$b$的用户数，可以通过如下余弦相似度公式计算标签$b$和标签$b’$的相似度：<br>$$<br>sim(b,b’) = \frac{\sum_{i \in N(b) \cap N(b’)} n_{b,i} n_{b’,i}}{\sqrt{\sum_{i \in N(b)}n_{b,i}^2 \sum_{i \in N(b’)} n_{b’,i}^2} }<br>$$<br>实验表明，进行标签扩展确实能够提高基于标签的物品推荐的准确率和召回率，但可能会稍微降低推荐结果的覆盖率和新颖度。</p><h4 id="3-标签清理"><a href="#3-标签清理" class="headerlink" title="3.标签清理"></a>3.标签清理</h4><p>不是所有标签都能反应用户的兴趣。同时，标签系统里经常出现词形不同、词义相同的标签。</p><p>标签清理的另一个重要意义在于将标签作为推荐解释。如果要把标签呈现给用户，将其作为给用户推荐某一个物品的解释，对标签的质量要求就很高。首先，这些标签不能包含没有意义的停止词或者表示情绪的词，其次这些推荐解释里不能包含很多意义相同的词语。</p><p>一般来说有如下标签清理方法：</p><ul><li>去除词频很高的停止词；</li><li>去除因词根不同造成的同义词，比如recommender system和recommendation system；</li><li>去除因分隔符造成的同义词，比如collaborative_filtering和collaborative-filtering；</li></ul><p>为了控制标签的质量，很多网站也采用了让用户进行反馈的思想，即让用户告诉系统某个标签是否合适。MovieLens在实验系统中就采用了这种方法。关于这方面的研究可以参考GroupLens 的Shilad Wieland Sen同学的博士论文(参见Shilad Wieland Sen的“ Nurturing Tagging Communities”) 。</p><h3 id="4-3-4-基于图的推荐算法"><a href="#4-3-4-基于图的推荐算法" class="headerlink" title="4.3.4 基于图的推荐算法"></a>4.3.4 基于图的推荐算法</h3><p>前面讨论的简单算法很容易懂，也容易实现，但<u>缺点</u>是不够系统化和理论化。因此考虑利用<strong>图模型</strong>做基于标签数据的个性化推荐。</p><p>首先，需要将用户打标签的行为表示到一张图上。图是由顶点、边和边上的权重组成的。而在用户标签数据集上，有3种不同的元素，即<em>用户</em>、<em>物品</em>和<em>标签</em>。因此，需要定义3种不同的顶点，即<em>用户顶点</em>、<em>物品顶点</em>和<em>标签顶点</em>。然后，如果我们得到一个表示用户$u$给物品$i$打了标签$b$的用户标签行为$(u,i,b)$，那么最自然的想法就是在图中增加3条边，首先需要在用户u对应的顶点$v(u)$和物品i对应的顶点$v(i)$之间增加一条边（如果这两个顶点已经有边相连，那么就应该将边的权重加1），同理，在$v(u)$和$v(b)$之间需要增加一条边，$v(i)$和$v(b)$之间也需要边相连接。</p><p>在定义出用户—物品—标签图后，可以用第2章提到的<strong>PersonalRank算法</strong>计算所有物品节点相对于当前用户节点在图上的相关性，然后按照相关性从大到小的排序，给用户推荐排名最高的$N$个物品。</p><h4 id="用图模型解释前面的简单算法"><a href="#用图模型解释前面的简单算法" class="headerlink" title="用图模型解释前面的简单算法"></a>用图模型解释前面的简单算法</h4><p>基于图模型重新思考前面的简单算法。</p><p>在那个算法中，用户对物品的兴趣公式如下：<br>$$<br>P(i|u) = \sum_b P(i|b)P(b|u)<br>$$<br>这个公式假定用户对物品的兴趣通过标签传递，因此这个公式可以通过一个比本节前面介绍的图更简单的图建模（记为<strong>SimpleTagGraph</strong>）。给定用户标签行为记录$(u,i,b)$，SimpleTagGraph会增加两条有向边，一条由用户节点$v(u)$指向标签节点$v(b)$，另一条由标签节点$v(b)$指向物品节点$v(i)$。从这个定义可以看到，SimpleTagGraph相对于前面提到用户—物品—标签图少了用户节点和物品节点之间的边。<br>在构建了SimpleTagGraph后， 利用前面的PersonalRank算法，令$K = 1$，并给出不同边权重的定义，就等价于前面提出的简单推荐算法。</p><h3 id="4-3-5-基于标签的推荐解释"><a href="#4-3-5-基于标签的推荐解释" class="headerlink" title="4.3.5 基于标签的推荐解释"></a>4.3.5 基于标签的推荐解释</h3><p>书中这部分首先介绍了豆瓣基于标签的推荐系统，其中豆瓣将推荐结果的可解释性拆分成两部分这个方法值得借鉴，详细内容见书。</p><p>GroupLens的研究人员Jesse Vig对基于标签的解释进行了深入研究(参见Jesse Vig、 Shilad Wieland Sen和 John Riedl的“Tagsplanations: Explaining Recommendations Using Tags”（ACM 2009 Article，2009）)。 和4.3.2节提出的算法类似，Jesse Vig将用户和物品之间的关系变成了<strong>用户对标签的兴趣(tag preference)</strong>和<strong>标签与物品的相关度(tag relevance)</strong>，然后作者用同一种推荐算法给用户推荐物品，但设计了4种标签解释的展示界面。</p><ul><li><strong>RelSort</strong>   对推荐物品做解释时使用的是用户以前使用过且物品上有的标签，给出了用户对标签的兴趣和标签与物品的相关度，但标签按照和物品的相关度排序。</li><li><strong>PreSort</strong>   对推荐物品做解释时使用的是用户以前使用过且物品上有的标签，给出了用户对标签的兴趣和标签与物品的相关度，但标签按照用户的兴趣程度排序。</li><li><strong>RelOnly</strong>   对推荐物品做解释时使用的是用户以前使用过且物品上有的标签，给出了标签与物品的相关度，且标签按照和物品的相关度排序。</li><li><strong>PreOnly</strong>   对推荐物品做解释时使用的是用户以前使用过且物品上有的标签，给出了用户对标签的兴趣程度，且标签按照用户的兴趣程度排序。</li></ul><p>然后，作者对用户设计了3种调查问卷。首先是关于推荐解释的调查问卷，作者问了如下3个问题：</p><ul><li>推荐解释帮助我理解这部电影为什么会被推荐给我：对于这个问题用户认为 <em>RelSort&gt; PrefOnly&gt;=PrefSort&gt;RelOnly</em>。</li><li>推荐解释帮助我判定是否喜欢推荐的电影：对于这个问题用户认为 <em>RelSort&gt;PrefSort&gt; PrefOnly&gt;RelOnly</em>。</li><li>推荐解释帮助我判定观看这部电影是否符合我现在的兴趣：对于这个问题用户认为<em>RelSort&gt;PrefSort&gt;RelOnly &gt;PrefOnly</em>。</li></ul><p>然后，作者调查了用户对不同类型标签的看法。作者将标签分为<strong>主观类</strong>（比如对电影的看法）和<strong>客观类</strong>（比如对电影内容的描述）。作者对每种类型的标签同样问了上面3个问题。</p><ul><li>这个标签帮助我理解这部电影为什么会被推荐给我：用户认为客观类标签优于主观类标签。</li><li>这个标签帮助我判定是否喜欢推荐的电影：用户认为客观类标签优于主观类标签。</li><li>这个标签帮助我判定观看这部电影是否符合我现在的兴趣：用户认为客观类标签优于主观类标签。</li></ul><p>从上面的结果可以发现，客观事实类的标签优于主观感受类标签。<br>最后，作者询问了用户对4种不同推荐解释界面的总体满意度，结果显示PrefOnly &gt; RelSort &gt; PrefSort &gt; RelOnly。</p><p><u>总结</u>问卷调查的结果，作者得出了以下结论：</p><ul><li>用户对标签的兴趣对帮助用户理解为什么给他推荐某个物品更有帮助；</li><li>用户对标签的兴趣和物品标签相关度对于帮助用户判定自己是否喜欢被推荐物品具有同样的作用；</li><li>物品标签相关度对于帮助用户判定被推荐物品是否符合他当前的兴趣更有帮助；</li><li>客观事实类标签相比主观感受类标签对用户更有作用。</li></ul><h2 id="4-4-给用户推荐标签"><a href="#4-4-给用户推荐标签" class="headerlink" title="4.4 给用户推荐标签"></a>4.4 给用户推荐标签</h2><h3 id="4-4-1-为什么要给用户推荐标签"><a href="#4-4-1-为什么要给用户推荐标签" class="headerlink" title="4.4.1 为什么要给用户推荐标签"></a>4.4.1 为什么要给用户推荐标签</h3><p>一般认为，给用户推荐标签有以下好处。</p><ul><li><strong>方便用户输入标签</strong> 从键盘输入标签会增加用户打标签的难度，推荐标签则可以降低难度，从而提高用户打标签的参与度。</li><li><strong>提高标签质量</strong> 同一个语义不同的用户可能用不同的词语来表示。这些同义词会使标签的词表变得很庞大，而且会使计算相似度不太准确。而使用推荐标签时，可以对词表进行选择，首先保证词表不出现太多的同义词，同时保证出现的词都是一些比较热门的、有代表性的词。</li></ul><h3 id="4-4-2-如何给用户推荐标签"><a href="#4-4-2-如何给用户推荐标签" class="headerlink" title="4.4.2 如何给用户推荐标签"></a>4.4.2 如何给用户推荐标签</h3><p>推荐标签的方法中比较简单的有4种。<br>第0种方法(最简单的方法)就是给用户$u$推荐整个系统里最热门的标签（这里将这个算法称为<strong>PopularTags</strong>）<br>令<code>tags[b]</code>为标签$b$的热门程度，算法的实验方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RecommendPopularTags</span><span class="params">(user,item, tags, N)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sorted(tags.items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:N]</span><br></pre></td></tr></table></figure><p>第1种方法就是给用户$u$推荐物品$i$上最热门的标签（这里将这个算法称为<strong>ItemPopularTags</strong>）。<br>令<code>item_tags[i][b]</code>为物品$i$被打上标签$b$的次数，这个算法的实现具体如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RecommendItemPopularTags</span><span class="params">(user,item, item_tags, N)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sorted(item_tags[item].items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:N]</span><br></pre></td></tr></table></figure><p>第2种方法是给用户$u$推荐他自己经常使用的标签（这里将这个算法称为<strong>UserPopularTags</strong>）。<br>令<code>user_tags[u][b]</code>为用户$u$使用标签$b$的次数，这个算法的实现如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RecommendUserPopularTags</span><span class="params">(user,item, user_tags, N)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sorted(user_tags[user].items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:N]</span><br></pre></td></tr></table></figure></p><p>第3种算法是前面两种的融合（这里记为<strong>HybridPopularTags</strong>），该方法通过一个系数(线性融合系数)将上面的推荐结果线性加权，然后生成最终的推荐结果。这个算法的实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RecommendHybridPopularTags</span><span class="params">(user,item, user_tags, item_tags, alpha, N)</span>:</span></span><br><span class="line">    max_user_tag_weight = max(user_tags[user].values()) </span><br><span class="line">    <span class="keyword">for</span> tag, weight <span class="keyword">in</span> user_tags[user].items():</span><br><span class="line">        ret[tag] = (<span class="number">1</span> – alpha) * weight / max_user_tag_weight</span><br><span class="line"></span><br><span class="line">    max_item_tag_weight = max(item_tags[item].values()) </span><br><span class="line">    <span class="keyword">for</span> tag, weight <span class="keyword">in</span> item_tags[item].items():</span><br><span class="line">        <span class="keyword">if</span> tag <span class="keyword">not</span> <span class="keyword">in</span> ret:</span><br><span class="line">            ret[tag] = alpha * weight / max_item_tag_weight</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ret[tag] += alpha * weight / max_item_tag_weight </span><br><span class="line">    <span class="keyword">return</span> sorted(ret[user].items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:N]</span><br></pre></td></tr></table></figure><p>注意在上面的实现中，在将两个列表线性相加时都将两个列表按最大值做了<strong>归一化</strong>，这样的<u>好处</u>是便于控制两个列表对最终结果的影响，而不至于因为物品非常热门而淹没用户对推荐结果的影响，或者因为用户非常活跃而淹没物品对推荐结果的影响。</p><h3 id="4-4-3-实验设置"><a href="#4-4-3-实验设置" class="headerlink" title="4.4.3 实验设置"></a>4.4.3 实验设置</h3><p>和前面的实验一样，用同样的方法将数据集按照9∶1分成<em>训练集</em>和<em>测试集</em>，然后通过训练集学习用户标注的模型。需要注意的是，这里切分数据集不再是以<code>user</code>、<code>item</code>为主键，而是以<code>user</code>、<code>item</code>、<code>tag</code>为主键。为了更好的理解如何切分数据集，请参考下面的Python代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span><span class="params">(records, train, test)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> user,item, tag <span class="keyword">in</span> records:</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">1</span>,<span class="number">10</span>) == <span class="number">1</span>: </span><br><span class="line">            test.append([user,item,tag])</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            train.append([user,item,tag]) </span><br><span class="line">    <span class="keyword">return</span> [train, test]</span><br></pre></td></tr></table></figure></p><p>对于测试集中的每一个用户物品对$(u,i)$，我们都可以推荐$N$个标签给用户$u$作参考。令$R(u,i)$为给用户$u$推荐的应该在物品$i$上打的标签集合，令$T(u,i)$为用户$u$实际给物品$i$打的标签的集合，然后可以利用准确率和召回率评测标签推荐的精度。</p><p>实验结果表明，ItemPopularTags具有最好的准确率和召回率。<br>在$α$=0.8($\alpha​$是线性融合稀疏)的时候，HybridPopularTags取得了最好的准确度。而且这个精度超过了单独的ItemPopularTags和UserPopularTags算法的精度。考虑到近70%的精度已经很高了，因此很多应用在给用户推荐标签时会直接给出用户最常用的标 签，以及物品最经常被打的标签。</p><p>不过，前面提到的基于统计用户常用标签和物品常用标签的算法有一个<u>缺点</u>，就是对新用户或者不热门的物品很难有推荐结果。解决这一问题有两个思路。<br>第一个思路是<em>从物品的内容数据中抽取关键词作为标签</em>。这方面的研究很多，特别是在<strong>上下文广告领域</strong>(参见Wen-tau Yih、Joshua Goodman和 Vitor R. Carvalho的“Finding Advertising Keywords on Web Pages”（ACM 2006 Article，2006）)。本书3.4节也介绍了生成关键词向量的一些方法。<br>第二个思路是<em>针对有结果，但结果不太多的情况</em>。可以做一些关键词扩展，加入一些与现有结果相关的标签。实现标签扩展的关键就是计算标签之间的相似度。关于这一点， 4.3.3节已经进行了深入探讨。</p><h3 id="4-4-4-基于图的标签推荐算法"><a href="#4-4-4-基于图的标签推荐算法" class="headerlink" title="4.4.4 基于图的标签推荐算法"></a>4.4.4 基于图的标签推荐算法</h3><p>图模型同样可以用于标签推荐。在根据用户打标签的行为生成图之后，可以利用PersonalRank算法进行排名。但这次遇到的问题和之前不同。这次的问题是，当用户$u$遇到物品$i$时，会给物品$i$打什么样的标签。因此，可以重新定义顶点的<strong>启动概率</strong>，如下所示：<br>$$<br>r_{v(k)} =<br>\begin{cases}<br>\alpha &amp;  {(v(k)= v(u))} \\<br>1 - \alpha &amp; (v(k) = v(i)) \\<br>0 &amp; \text{(其他)}<br>\end{cases}<br>$$<br>只有用户$u$和物品$i$对应的顶点有非0的启动概率，而其他顶点的启动概率都为0。 在上面的定义中，$v(u)$和$v(i)$的启动概率并不相同，$v(u)$的启动概率是$\alpha$，而$v(i)$的启动概率是$1-\alpha$。 参数$\alpha$可以通过离线实验选择。</p><h2 id="4-5-扩展阅读"><a href="#4-5-扩展阅读" class="headerlink" title="4.5 扩展阅读"></a>4.5 扩展阅读</h2><p>本章主要讨论了UGC标签在推荐系统中的应用。标签作为描述语义的重要媒介，无论是对于描述用户兴趣还是表示物品的内容都有很重要的意义。标签在推荐系统中的应用主要集中在两个问题上，一个是<em>如何利用用户打标签的行为给用户推荐物品</em>，另一个是<em>如何给用户推荐标签</em>。本章在深入分析用户标签行为的基础上对这两个问题进行了深入探讨。</p><p>关于标签的问题，最近几年在学术界获得了广泛关注。ECML/PKDD在2008年曾经推出过基于标签的推荐系统比赛(1)。 在这些研究中涌现了很多新的方法， 比如<em>张量分解(2)(tensor factorization)</em>、<em>基于LDA的算法(3)</em>、<em>基于图的算法(4)</em>等。不过这些算法很多具有较高的复杂度，在实际系统中应用起来还有很多实际的困难需要解决。<br>GroupLens的研究人员给MovieLens系统做了很多标签方面的工作。Shilad Sen在论文(5)中研究了如何利用标签联系用户和物品并给用户进行个性化电影推荐。Jesse Vig在论文(6)中研究了如何利用标签进行推荐解释，他将用户和物品之间的关系转化为用户对标签的兴趣（tag preference） 以及标签和物品的相关度（tag relevance）两种因素。同时他们研究了如何对标签进行清理(7)，以及如何选择合适的标签进行解释。</p><blockquote><p>(1):比赛介绍见 <a href="http://www.kde.cs.uni-kassel.de/ws/rsdc08/program.html。" target="_blank" rel="noopener">http://www.kde.cs.uni-kassel.de/ws/rsdc08/program.html。</a><br>(2):参见Panagiotis Symeonidis、Alexandros Nanopoulos和Yannis Manolopoulos的“Tag recommendations based on tensor dimensionality reduction”（ACM 2008 Article，2008）。<br>(3):参见Ralf Krestel、 Peter Fankhauser和Wolfgang Nejdl的“Latent dirichlet allocation for tag recommendation”（ACM 2009 Article，2009）。<br>(4):参见Andreas Hotho、 Robert Jäschke、 Christoph Schmitz和Gerd Stumme的“Folkrank: A ranking algorithm for folksonomies”（Proc. FGIR 2006，2006）。<br>(5):参见Shilad Wieland Sen、 Jesse Vig和John Riedl的“Tagommenders: Connecting Users to Items through Tags”（ACM 2009 Article，2009）。<br>(6):参见Jesse Vig、Shilad Wieland Sen和John Riedl的“Tagsplanations: Explaining Recommendations Using Tags”（ACM 2009 Article，2009）。<br>(7):参见Shilad Wieland Sen、F. Maxwell Harper、Adam LaPitz和John Riedl的“The quest for quality tags”（ACM 2007 Article，2007）。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第4章-利用用户标签数据&quot;&gt;&lt;a href=&quot;#第4章-利用用户标签数据&quot; class=&quot;headerlink&quot; title=&quot;第4章 利用用户标签数据&quot;&gt;&lt;/a&gt;第4章 利用用户标签数据&lt;/h1&gt;&lt;p&gt;GroupLens在一篇文章中(文章名是“Tagsplanations : Explaining Recommendations using Tags”)表示目前流行的推荐系统基本上通过3种方式联系用户兴趣和物品。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基于物品的算法&lt;/strong&gt; 比如UserCF&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于用户的算法&lt;/strong&gt; 比如ItemCF&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于特征的算法&lt;/strong&gt; 比如隐语义模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/2018/11/27/推荐系统实践读书笔记（四）/推荐系统联系用户和物品的几种途径.png&quot; alt=&quot;推荐系统联系用户和物品的几种途径&quot;&gt;&lt;/p&gt;
&lt;p&gt;本章将讨论一种重要的特征表现方式——&lt;strong&gt;标签&lt;/strong&gt;。&lt;br&gt;根据维基百科的定义(参见&lt;a href=&quot;http://en.wikipedia.org/wiki/Tag_(metadata)&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://en.wikipedia.org/wiki/Tag_(metadata)&lt;/a&gt; )，标签是一种无层次化结构的、用来描述信息的关键词，它可以用来描述物品的语义。根据给物品打标签的人的不同，标签应用一般分为两种：一种是&lt;em&gt;让作者或者专家给物品打标签&lt;/em&gt;；另一种是&lt;em&gt;让普通用户给物品打标签&lt;/em&gt;，也就是&lt;strong&gt;UGC(User Generated Content，用户生成的内容)&lt;/strong&gt;的标签应用。UGC的标签系统是一种表示用户兴趣和物品语义的重要方式。当一个用户对一个物品打上一个标签，这个标签一方面描述了用户的兴趣，另一方面则表示了物品的语义，从而将用户和物品联系了起来。本章主要讨论UGC的标签应用，研究用户给物品打标签的行为，探讨如何通过分析这种行为给用户进行个性化推荐。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实践读书笔记（三）</title>
    <link href="https://rilzob.com/2018/11/26/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>https://rilzob.com/2018/11/26/推荐系统实践读书笔记（三）/</id>
    <published>2018-11-26T06:35:08.655Z</published>
    <updated>2019-03-24T03:30:45.188Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第3章-推荐系统冷启动问题"><a href="#第3章-推荐系统冷启动问题" class="headerlink" title="第3章 推荐系统冷启动问题"></a>第3章 推荐系统冷启动问题</h1><p>如何在没有大量用户数据的情况下设计个性化推荐系统并且让用户对推荐结果满意从而愿意使用推荐系统，就是<strong>冷启动</strong>的问题。<br>本章简单介绍一下冷启动问题的分类，以及如何解决不同种类的冷启动问题。<br><a id="more"></a></p><h2 id="3-1-冷启动问题简介"><a href="#3-1-冷启动问题简介" class="headerlink" title="3.1 冷启动问题简介"></a>3.1 冷启动问题简介</h2><p>冷启动问题(cold start)主要分3类。</p><ul><li><strong>用户冷启动</strong> 用户冷启动主要解决如何给新用户做个性化推荐的问题。</li><li><strong>物品冷启动</strong> 物品冷启动主要解决如何将新的物品推荐给可能对它感兴趣的用户这一问题。</li><li><strong>系统冷启动</strong> 系统冷启动主要解决如何在一个新开发的网站上（还没有用户，也没有用户行为，只有一些物品的信息）设计个性化推荐系统，从而在网站刚发布时就让用户体验到个性化推荐服务这一问题。</li></ul><p>对于这3种不同的冷启动问题，有不同的解决方案。一般来说，可以参考如下解决方案。</p><ul><li><em>提供非个性化的推荐。</em> 非个性化推荐的最简单例子就是热门排行榜。</li><li><em>利用用户注册时提供的年龄、性别等数据做粗粒度的个性化。</em></li><li><em>利用用户的社交网络账号登录（需要用户授权），导入用户在社交网站上的好友信息，然后给用户推荐其好友喜欢的物品。</em></li><li><em>要求用户在登录时对一些物品进行反馈，收集用户对这些物品的兴趣信息，然后给用户推荐那些和这些物品相似的物品。</em></li><li><em>对于新加入的物品，可以利用内容信息，将它们推荐给喜欢过和它们相似的物品的用户。</em></li><li><em>在系统冷启动时，可以引入专家的知识，通过一定的高效方式迅速建立起物品的相关度表。</em></li></ul><h2 id="3-2-利用用户注册信息"><a href="#3-2-利用用户注册信息" class="headerlink" title="3.2 利用用户注册信息"></a>3.2 利用用户注册信息</h2><p>用户的注册信息分3种。</p><ul><li><strong>人口统计学信息</strong> 包括用户的年龄、性别、职业、民族、学历和居住地。</li><li><strong>用户兴趣的描述</strong> 有一些网站会让用户用文字描述他们的兴趣。</li><li><strong>从其他网站导入的用户站外行为数据</strong> 比如用户通过豆瓣、新浪微博的账号登录，就可以在得到用户同意的情况下获取用户在豆瓣或者新浪微博的一些行为数据和社交网络数据。</li></ul><p><strong>基于人口统计学特征的推荐系统</strong>其典型代表是Bruce Krulwich开发的Lifestyle Finder(参见论文Bruce Krulwich的“Lifestyle finder : intelligent user profiling using large scale demographic data”（1997）) 。书上这部分有关于该算法的简略介绍和评测，但并没有涉及该算法是具体如何根据人口统计学属性进行分类的，详情见书。</p><p>基于注册信息的<u>个性化推荐流程</u>基本如下：</p><ol><li>获取用户的注册信息；</li><li>根据用户的注册信息对用户分类；</li><li>给用户推荐他所属分类中用户喜欢的物品。</li></ol><p><strong>基于用户注册信息的推荐算法</strong>其<u>核心问题</u>是计算每种特征的用户喜欢的物品。也就是说，对于每种特征$f$，计算具有这种特征的用户对各个物品的喜好程度$p(f, i)$。<br>$p(f,i)$可以简单地定义为物品$i$在具有$f$的特征的用户中的热门程度：$$p(f,i)=\vert N(i) \cap U(f) \vert$$其中$N(i)$是喜欢物品$i$的用户集合，$U(f)$是具有特征$f$的用户集合。</p><p>上面这种定义可以比较准确地预测具有某种特征的用户是否喜欢某个物品。但是，在这种定义下，往往热门的物品会在各种特征的用户中都具有比较高的权重。也就是说具有比较高的$\vert N (i) \vert$的物品会在每一类用户中都有比较高的$p(f ,i)$。给用户推荐热门物品并不是推荐系统的主要任务，推荐系统应该帮助用户发现他们不容易发现的物品。因此，可以将$p( f , i )$定义为喜欢物品$i$的用户中具有特征$f$的比例：$$p(f,i) =  \frac {\vert N(i) \cap U(f) \vert}{\vert N(i) \vert + \alpha}$$这里分母中使用参数$\alpha$的目的是解决<strong>数据稀疏问题</strong>。比如有一个物品只被1个用户喜欢过， 而这个用户刚好就有特征$f$，那么就有$p(f,i)$。但是，这种情况并没有统计意义，因此为分母加上一个比较大的数，可以避免这样的物品产生比较大的权重。</p><p>有两个推荐系统数据集包含了人口统计学信息， 一个是 BookCrossing 数据集。另一个是Lastfm数据集。</p><blockquote><p>BookCrossing数据集:<br>参见<a href="http://www.informatik.uni-freiburg.de/~cziegler/BX/" target="_blank" rel="noopener">http://www.informatik.uni-freiburg.de/~cziegler/BX/</a><br>Lastfm数据集：<br>参见<a href="http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-360K.html" target="_blank" rel="noopener">http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-360K.html</a></p></blockquote><p>作者在这两个数据集上做实验验证，证明基于人口统计学特征的推荐系统准确率、召回率和覆盖率确实更高，而且利用的用户人口统计学特征越多，越能准确地预测用户兴趣，详情见书。</p><h2 id="3-3-选择合适的物品启动用户的兴趣"><a href="#3-3-选择合适的物品启动用户的兴趣" class="headerlink" title="3.3 选择合适的物品启动用户的兴趣"></a>3.3 选择合适的物品启动用户的兴趣</h2><p>对于通过让用户对物品进行评分来收集用户兴趣，从而对用户进行冷启动的系统，它们需要解决的<u>首要问题</u>就是如何选择物品让用户进行反馈。</p><p>一般来说，能够用来启动用户兴趣的物品需要具有以下特点。</p><ul><li><strong>比较热门</strong> 因为用户需要知道这个物品是什么，才能对它们作出准确反馈。</li><li><strong>具有代表性和区分性</strong> 启动用户兴趣的物品不能是大众化或老少咸宜的，因为这样的物品对用户的兴趣没有区分性。</li><li><strong>启动物品集合需要有多样性</strong> 冷启动时，由于不清楚用户的兴趣，并且用户用户兴趣的可能性非常多，为了匹配多样的兴趣，为此需要提供具有很高覆盖率的启动物品集合，这些物品能覆盖几乎所有主流的用户兴趣。</li></ul><p>上面这些因素都是选择启动物品时需要考虑的，但还需要考虑的是如何设计一个选择启动物品集合的系统？Nadav Golbandi在论文中(“Adaptive Bootstrapping of Recommender Systems Using Decision Trees”，下载地址为 <a href="http://research.yahoo.com/pub/3502" target="_blank" rel="noopener">http://research.yahoo.com/pub/3502</a>) 探讨了这个问题，提出可以用一个<strong>决策树</strong>解决这个问题。<br>首先，给定一群用户，Nadav Golbandi用这群用户对物品评分的<strong>方差</strong>度量这群用户兴趣的一致程度。如果方差很大，说明这一群用户的兴趣不太一致，反之则说明这群用户的兴趣比较一致。 令$\sigma_u \in U’$为用户集合$U’$中所有评分的方差，Nadav Golbandi的<u>基本思想</u>是通过如下方式度量一个物品的<strong>区分度</strong>$D(i)$:$$D(i) = \sigma_{u \in N^+(i)} + \sigma_{u \in N^-(i)} + \sigma_{u \in  \bar N(i)}$$其中，$N^+(i)$是喜欢物品i的用户集合，$N^-(i)$是不喜欢物品i的用户集合，$\bar N(i)$是没有对物品$i$评分的用户集合。$ \sigma_{u \in N^+(i)}$是喜欢物品i的用户对其他物品评分的方差，$\sigma_{u \in N^-(i)}$是不喜欢物品$i$的用户对其他物品评分的方差，$\sigma_{u \in  \bar N(i)}$是没有对物品$i$评分的用户对其他物品评分的方差。也就是说，对于物品$i$，Nadav Golbandi将用户分成3类——喜欢物品i的用户、不喜欢物品i的用户和不知道物品i的用户（即没有给i评分的用户）。如果这3类用户集合内的用户对其他的物品兴趣很不一致，说明物品i具有较高的区分度。<br>Nadav Golbandi的<u>算法</u>首先会从所有用户中找到具有最高区分度的物品i，然后将用户分成3类。然后在每类用户中再找到最具区分度的物品，然后将每一类用户又各自分为3类，也就是将总用户分成9类，然后这样继续下去，最终可以通过对一系列物品的看法将用户进行分类。而在冷启动时，我们从根节点开始询问用户对该节点物品的看法，然后根据用户的选择将用户放到不同的分枝，直到进入最后的叶子节点，此时我们就已经对用户的兴趣有了比较清楚的了解，从而可以开始对用户进行比较准确地个性化推荐。</p><h2 id="3-4-利用物品的内容信息"><a href="#3-4-利用物品的内容信息" class="headerlink" title="3.4 利用物品的内容信息"></a>3.4 利用物品的内容信息</h2><p>物品冷启动需要解决的问题是如何将新加入的物品推荐给对它感兴趣的用户。</p><p>第2章介绍了两种主要的推荐算法——<strong>UserCF</strong>和<strong>ItemCF算法</strong>。首先需要指出的是，UserCF算法对物品冷启动问题并不非常敏感。因为，UserCF在给用户进行推荐时，会首先找到和用户兴趣相似的一群用户，然后给用户推荐这一群用户喜欢的物品。在很多网站中，推荐列表并不是给用户展示内容的唯一列表，那么当一个新物品加入时，总会有用户从某些途径看到这些物品，对这些物品产生反馈。那么，当一个用户对某个物品产生反馈后，和他历史兴趣相似的其他用户的推荐列表中就有可能出现这一物品，从而更多的人就会对这个物品产生反馈，导致更多的人的推荐列表中会出现这一物品，因此该物品就能不断地扩散开来，从而逐步展示到对它感兴趣用户的推荐列表中。<br>但是，有些网站中推荐列表可能是用户获取信息的主要途径，比如豆瓣网络电台。那么对于UserCF算法就需要解决第一推动力的问题，即第一个用户从哪儿发现新的物品。只要有一小部分人能够发现并喜欢新的物品，UserCF算法就能将这些物品扩散到更多的用户中。解决第一推动力最简单的方法是将新的物品<strong>随机</strong>展示给用户，但这样显然不太个性化，因此可以考虑利用物品的内容信息，将新物品先投放给曾经喜欢过和它内容相似的其他物品的用户。<br>对于ItemCF算法来说，物品冷启动就是一个严重的问题了。因为ItemCF算法的原理是给用户推荐和他之前喜欢的物品相似的物品。ItemCF算法会每隔一段时间利用用户行为计算<strong>物品相似度表</strong>（一般一天计算一次），在线服务时ItemCF算法会将之前计算好的<strong>物品相关度矩阵</strong>放在内存中。因此，当新物品加入时，内存中的物品相关表中不会存在这个物品，从而ItemCF算法无法推荐新的物品。解决这一问题的办法是频繁更新物品相似度表，但基于用户行为计算物品相似度是非常耗时的事情，主要原因是用户行为日志非常庞大。而且，新物品如果不展示给用户，用户就无法对它产生行为，通过行为日志计算是计算不出包含新物品的相关矩阵的。为此，我们只能利用物品的内容信息计算物品相关表，并且频繁地更新相关表（比如半小时计算一次）。</p><p>一般来说，物品的内容可以通过<strong>向量空间模型</strong>(参见维基百科Vector Space Model词条)表示，该模型会将物品表示成一个<strong>关键词向量</strong>。如果物品的内容是一些诸如导演、演员等实体的话，可以直接将这些实体作为关键词。但如果内容是文本的形式，则需要引入一些理解自然语言的技术抽取关键词。图3-11展示了从文本生成关键词向量的主要步骤。对于中文，首先要对文本进行<strong>分词</strong>，将字流变成词流，然后从词流中检测出<strong>命名实体</strong>（如人名、地名、组织名等），这些实体和一些其他重要的词将组成关键词集合， 最后对关键词进行排名，计算每个关键词的<strong>权重</strong>，从而生成关键词向量。<br><img src="/2018/11/26/推荐系统实践读书笔记（三）/关键词向量的生成过程.png" alt="关键词向量的生成过程"><br>对于物品$d$，它的内容表示成一个关键词向量如下：$$d_i = \{ (e_1,w_1),(e_2,w_2), ···\}$$其中$e_i$是关键词，$w_i$是关键词对应的权重。如果物品是文本，我们可以用信息检索领域著名的<strong>TF-IDF公式</strong>计算词的权重：$$w_i = \frac {TF(e_i)}{\log DF(e_i)}​$$</p><p>向量空间模型的<u>优点</u>是简单，<u>缺点</u>是丢失了一些信息，比如关键词之间的关系信息。不过在绝大多数应用中，向量空间模型对于文本的分类、聚类、相似度计算已经可以给出令人满意的结果。</p><p>在给定物品内容的关键词向量后，物品的内容相似度可以通过向量之间的余弦相似度计算：$$w_{ij} = \frac {d_i \cdot d_j}{\sqrt {\Vert d_i \Vert \Vert d_j \Vert}}$$<br>在具体计算物品之间的内容相似度时，最简单的方法当然是对两两物品都利用上面的余弦相似度公式计算相似度，如下代码简单实现了这种方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalculateSimilarity</span><span class="params">(D)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> di <span class="keyword">in</span> D:</span><br><span class="line">        <span class="keyword">for</span> dj <span class="keyword">in</span> D:</span><br><span class="line">            w[i][j] = CosineSimilarity(di, dj) </span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure></p><p><code>D</code>是文档集合。</p><p>但这种算法的时间复杂度很高。假设有$N$个物品，每个物品平均由$m$个实体表示，那么这个算法的复杂度是$O( N^2m)$。 在实际应用中，可以首先通过建立关键词—物品的<strong>倒排表</strong>加速这一计算过程，关于这一方法已经在前面介绍UserCF和ItemCF算法时详细介绍过了，所以这里直接给出计算的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalculateSimilarity</span><span class="params">(entity-items)</span></span></span><br><span class="line">    w = dict() </span><br><span class="line">    ni = dict() </span><br><span class="line">    <span class="keyword">for</span> e,items <span class="keyword">in</span> entity_items.items():</span><br><span class="line">        <span class="keyword">for</span> i,wie <span class="keyword">in</span> items.items(): </span><br><span class="line">            addToVec(ni, i, wie * wie) </span><br><span class="line">            <span class="keyword">for</span> j,wje <span class="keyword">in</span> items.items(): </span><br><span class="line">                addToMat(w, i, j, wie, wje) </span><br><span class="line">    <span class="keyword">for</span> i, relate_items <span class="keyword">in</span> w.items():</span><br><span class="line">        relate_items = &#123;x:y/math.sqrt(ni[i] * ni[x]) <span class="keyword">for</span> x,y <span class="keyword">in</span> relate_items.items()&#125;</span><br></pre></td></tr></table></figure></p><p>得到物品的相似度之后，可以利用上一章提到的ItemCF算法的思想，给用户推荐和他历史上喜欢的物品内容相似的物品。</p><p>既然内容相似度计算简单，能频繁更新，而且能够解决物品冷启动问题，那么为什么还需要协同过滤的算法?书中这部分在MovieLens和GitHub两个数据集上进行了实验，并加以说明，详情见书。</p><h3 id="话题模型"><a href="#话题模型" class="headerlink" title="话题模型"></a>话题模型</h3><p>向量空间模型在内容数据丰富时可以获得比较好的效果。以文本为例，如果是计算长文本的相似度，用向量空间模型利用关键词计算相似度已经可以获得很高的精确度。但是，如果文本很短，关键词很少，向量空间模型就很难计算出准确的相似度。举个例子，假设有两篇论文，它们的标题分别是“推荐系统的动态特性”和“基于时间的协同过滤算法研究”。如果读者对推荐系统很熟悉，可以知道这两篇文章的研究方向是类似的，但是它们标题中没有一样的关键词。其实，它们的关键词虽然不同，但却是相似的。“动态”和“基于时间”含义相似，“协同过滤”是“推荐系统”的一种算法。换句话说，这两篇文章的关键词虽然不同，但关键词所属的话题是相同的。在这种情况下，首先需要知道文章的<strong>话题分布</strong>，然后才能准确地计算文章的相似度。如何建立文章、话题和关键词的关系是<strong>话题模型（topic model）</strong>研究的重点。</p><p>代表性的话题模型有<strong>LDA</strong>。关于LDA的详细理论介绍可以参考DM Blei的论文“Latent Dirichlet Allocation” (参见David M. Blei、 Andrew Y. Ng、 Michael I. Jordan的“ Latent dirichlet allocation”（Journal of Machine Learning Research 3， 2003）)。<br>任何模型都有一个假设，LDA作为一种<strong>生成模型</strong>，对一篇文档产生的过程进行了建模。话题模型的<u>基本思想</u>是，一个人在写一篇文档的时候，会首先想这篇文章要讨论哪些话题，然后思考这些话题应该用什么词描述，从而最终用词写成一篇文章。因此，文章和词之间是通过话题联系的。<br>LDA中有3种元素，即文档、话题和词语。每一篇文档都会表现为词的集合，这称为<strong>词袋模型(bag of words)</strong>。每个词在一篇文章中属于一个话题。令$D$为文档集合，$D[i]$是第$i$篇文档。$w[i][j]$是第$i$篇文档中的第$j$个词。$z[i][j]$是第$i$篇文档中第$j$个词属于的话题。</p><p>LDA的<u>计算过程</u>包括<strong>初始化</strong>和<strong>迭代</strong>两部分。首先要对$z$进行初始化，而初始化的方法很简单，假设一共有$K$个话题， 那么对第$i$篇文章中的第$j$个词， 可以随机给它赋予一个话题。同时，用$NWZ(w,z)$记录词$w$被赋予话题$z$的次数，$NZD(z,d)$记录文档$d$中被赋予话题$z$的词的个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">foreach document i <span class="keyword">in</span> range(<span class="number">0</span>,|D|):</span><br><span class="line">    foreach word j <span class="keyword">in</span> range(<span class="number">0</span>, |D(i)|):</span><br><span class="line">        z[i][j] = rand() % K </span><br><span class="line">        NZD[z[i][j], D[i]]++ </span><br><span class="line">        NWZ[w[i][j], z[i][j]]++ </span><br><span class="line">        NZ[z[i][j]]++</span><br></pre></td></tr></table></figure><p>在初始化之后，要通过迭代使话题的分布收敛到一个合理的分布上去。伪代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> converged:</span><br><span class="line">    foreach document i <span class="keyword">in</span> range(<span class="number">0</span>, |D|):</span><br><span class="line">        foreach word j <span class="keyword">in</span> range(<span class="number">0</span>, |D(i)|): </span><br><span class="line">            NWZ[w[i][j], z[i][j]]--</span><br><span class="line">            NZ[z[i][j]]--</span><br><span class="line">            NZD[z[i][j], D[i]]--</span><br><span class="line">            z[i][j] = SampleTopic() </span><br><span class="line">            NWZ[w[i][j], z[i][j]]++ </span><br><span class="line">            NZ[z[i][j]]++ </span><br><span class="line">            NZD[z[i][j], D[i]]++</span><br></pre></td></tr></table></figure><p>LDA可以很好地将词组合成不同的话题。</p><p>在使用LDA计算物品的内容相似度时，可以先计算出物品在话题上的分布，然后利用两个物品的话题分布计算物品的相似度。比如，如果两个物品的话题分布相似，则认为两个物品具有较高的相似度，反之则认为两个物品的相似度较低。计算分布的相似度可以利用<strong>KL散度</strong>(参见<a href="http://en.wikipedia.org/wiki/Kullback-Leibler_divergence" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Kullback-Leibler_divergence</a>) ：$$D_{KL}(p||q) = \sum_i p(i)ln \frac{p(i)}{q(i)}$$其中$p$和$q$是两个分布，KL散度越大说明分布的相似度越低。</p><h2 id="3-5-发挥专家的作用"><a href="#3-5-发挥专家的作用" class="headerlink" title="3.5 发挥专家的作用"></a>3.5 发挥专家的作用</h2><p>书中这部分对个性化网络电台Pandora和电影推荐网站Jinni如何利用专家对物品进行标注，进而建立推荐系统作了介绍。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第3章-推荐系统冷启动问题&quot;&gt;&lt;a href=&quot;#第3章-推荐系统冷启动问题&quot; class=&quot;headerlink&quot; title=&quot;第3章 推荐系统冷启动问题&quot;&gt;&lt;/a&gt;第3章 推荐系统冷启动问题&lt;/h1&gt;&lt;p&gt;如何在没有大量用户数据的情况下设计个性化推荐系统并且让用户对推荐结果满意从而愿意使用推荐系统，就是&lt;strong&gt;冷启动&lt;/strong&gt;的问题。&lt;br&gt;本章简单介绍一下冷启动问题的分类，以及如何解决不同种类的冷启动问题。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实践读书笔记（二）</title>
    <link href="https://rilzob.com/2018/11/24/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>https://rilzob.com/2018/11/24/推荐系统实践读书笔记（二）/</id>
    <published>2018-11-24T06:41:12.976Z</published>
    <updated>2019-03-24T04:10:58.754Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第2章-利用用户行为数据"><a href="#第2章-利用用户行为数据" class="headerlink" title="第2章 利用用户行为数据"></a>第2章 利用用户行为数据</h1><p>用户行为数据中蕴涵着很多不是那么显而易见的规律，而个性化推荐算法的任务就是通过计算机去发现这些规律，从而为产品的设计提供指导，提高用户体验。<br>基于<strong>用户行为分析</strong>的推荐算法是个性化推荐系统的重要算法，学术界一般将这种类型的算法称为<strong>协同过滤算法</strong>。顾名思义，协同过滤就是指用户可以齐心协力，通过不断地和网站互动，使自己的推荐列表能够不断过滤掉自己不感兴趣的物品，从而越来越满足自己的需求。<br><a id="more"></a></p><h2 id="2-1-用户行为数据简介"><a href="#2-1-用户行为数据简介" class="headerlink" title="2.1 用户行为数据简介"></a>2.1 用户行为数据简介</h2><h3 id="用户行为数据的存在形式"><a href="#用户行为数据的存在形式" class="headerlink" title="用户行为数据的存在形式"></a>用户行为数据的存在形式</h3><p>用户行为数据在网站上最简单的存在形式就是<strong>日志</strong>。网站在运行过程中都产生大量<strong>原始日志(raw log)</strong>，并将其存储在文件系统中。很多互联网业务会把多种原始日志按照用户行为汇总成<strong>会话日志(session log)</strong>，其中每个会话表示一次用户行为和对应的服务。比如，在搜索引擎和搜索广告系统中，服务会为每次查询生成一个<strong>展示日志(impression log)</strong>，其中记录了查询和返回结果。如果用户点击了某个结果，这个点击信息会被服务器截获并存储在<strong>点击日志(click log)</strong>中。 一个并行程序会周期性地归并展示日志和点击日志，得到的会话日志中每个消息是一个用户提交的查询、得到的结果以及点击。类似地，推荐系统和电子商务网站也会汇总原始日志生成描述用户行为的会话日志。</p><h3 id="用户行为数据的分类"><a href="#用户行为数据的分类" class="headerlink" title="用户行为数据的分类"></a>用户行为数据的分类</h3><p>用户行为在个性化推荐系统中一般分两种——<strong>显性反馈行为(explicit feedback)</strong>和<strong>隐性反馈行为(implicit feedback)</strong>。显性反馈行为包括用户明确表示对物品喜好的行为，主要形式就是评分和喜欢/不喜欢。和显性反馈行为相对应的是隐性反馈行为。隐性反馈行为指的是那些不能明确反应用户喜好的行为。最具代表性的隐性反馈行为就是页面浏览行为。<br>按照反馈的明确性分，用户行为数据可以分为显性反馈和隐性反馈，但按照反馈的方向分，又可以分为<strong>正反馈</strong>和<strong>负反馈</strong>。正反馈指用户的行为倾向于指用户喜欢该物品，而负反馈指用户的行为倾向于指用户不喜欢该物品。在显性反馈中，很容易区分一个用户行为是正反馈还是负反馈，而在隐性反馈行为中，就相对比较难以确定。</p><h3 id="显性反馈数据和隐性反馈数据的区别"><a href="#显性反馈数据和隐性反馈数据的区别" class="headerlink" title="显性反馈数据和隐性反馈数据的区别"></a>显性反馈数据和隐性反馈数据的区别</h3><p>下图从几个方面比较了显性反馈数据和隐性反馈数据。<br><img src="/2018/11/24/推荐系统实践读书笔记（二）/显示反馈数据和隐性反馈数据的比较.png" alt="显示反馈数据和隐性反馈数据的比较"></p><h3 id="用户行为的表示方式"><a href="#用户行为的表示方式" class="headerlink" title="用户行为的表示方式"></a>用户行为的表示方式</h3><p>下图给出了一种表示方式，它将一个用户行为表示为六部分，即产生行为的用户和行为的对象。行为的种类、产生行为的上下文、行为的内容和权重。<br><img src="/2018/11/24/推荐系统实践读书笔记（二）/用户行为的统一表示.png" alt="用户行为的统一表示"><br>很多时候我们并不使用统一结构表示所有行为，而是针对不同的行为给出不同表示。有时可能会忽略一些信息(比如上下文)，但有些信息不能忽略(比如产生行为的用户和行为的对象就是所有行为都必须包含的)。</p><h3 id="数据集的分类"><a href="#数据集的分类" class="headerlink" title="数据集的分类"></a>数据集的分类</h3><p>不同的数据集针对不同的情况，根据所包含行为的不同将数据集进行分类，目前比较有代表性的数据集有如下几个：</p><ul><li><strong>无上下文信息的隐性反馈数据集</strong> 每一条行为记录仅仅包含用户ID和物品ID。Book-Crossing(参见“Book-Crossing Dataset”，地址为<a href="http://www.informatik.uni-freiburg.de/~cziegler/BX/" target="_blank" rel="noopener">http://www.informatik.uni-freiburg.de/~cziegler/BX/</a>) 就是这种类型的数据集。 </li><li><strong>无上下文信息的显性反馈数据集</strong> 每一条记录包含用户ID、物品ID和用户对物品的评分。</li><li><strong>有上下文信息的隐性反馈数据集</strong> 每一条记录包含用户ID、物品ID和用户对物品产生行为的时间戳。Lastfm数据集(参见<a href="http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html" target="_blank" rel="noopener">http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html</a>) 就是这种类型的数据集。</li><li><strong>有上下文信息的显性反馈数据集</strong> 每一条记录包含用户ID、物品ID、用户对物品的评分和评分行为发生的时间戳。Netflix Prize(参见<a href="http://netflixprize.com/" target="_blank" rel="noopener">http://netflixprize.com/</a>) 提供的就是这种类型的数据集。</li></ul><h2 id="2-2-用户行为分析"><a href="#2-2-用户行为分析" class="headerlink" title="2.2 用户行为分析"></a>2.2 用户行为分析</h2><p>在利用用户行为数据设计推荐算法之前，研究人员首先需要对用户行为数据进行分析，了解数据中蕴含的一般规律，这样才能对算法的设计起到指导作用。</p><h3 id="2-2-1-用户活跃度和物品流行度的分布"><a href="#2-2-1-用户活跃度和物品流行度的分布" class="headerlink" title="2.2.1 用户活跃度和物品流行度的分布"></a>2.2.1 用户活跃度和物品流行度的分布</h3><h4 id="长尾分布"><a href="#长尾分布" class="headerlink" title="长尾分布"></a>长尾分布</h4><p>很多关于互联网数据的研究发现，互联网上的很多数据分布都满足一种称为Power Law(参见“浅谈网络世界的Power Law现象”，地址为<a href="http://mmdays.com/2008/11/22/power_law_1/" target="_blank" rel="noopener">http://mmdays.com/2008/11/22/power_law_1/</a>) 的分布，这个分布在互联网领域也称<strong>长尾分布</strong>。$$f(x) = \alpha x^k$$<br>1932年，哈佛大学的语言学家Zipf在研究英文单词的词频时发现，如果将单词出现的频率按照由高到低排列，则每个单词出现的频率和它在热门排行榜中排名的常数次幂成反比。这个现象表明，在英文中大部分词的词频其实很低，只有很少的词被经常使用。<br>用户行为数据也蕴含着这种规律。令$f_u(k)$为对k个物品产生过行为的用户数，令$f_i(k)$为被k个用户产生过行为的物品数。那么，$f_u(k)$和$f_i(k)$都满足长尾分布。也就是说：$$f_u(k)= \alpha_u k^{\beta_u}$$ $$f_i(k)= \alpha_i k^{\beta_i}$$</p><h3 id="2-2-2-用户活跃度和物品流行度的关系"><a href="#2-2-2-用户活跃度和物品流行度的关系" class="headerlink" title="2.2.2 用户活跃度和物品流行度的关系"></a>2.2.2 用户活跃度和物品流行度的关系</h3><p>用户越活跃，越倾向于浏览冷门的物品。<br>仅仅基于用户行为数据设计的推荐算法一般称为协同过滤算法。 学术界对协同过滤算法进行了深入研究，提出了很多方法，比如<strong>基于邻域的方法（ neighborhood-based ）</strong>、<strong>隐语义模型（ latent factor model）</strong>、<strong>基于图的随机游走算法（random walk on graph）</strong>等。在这些方法中，最著名的、在业界得到最广泛应用的算法是基于邻域的方法， 而基于邻域的方法主要包含下面两种算法。</p><ul><li><strong>基于用户的协同过滤算法</strong></li><li><strong>基于物品的协同过滤算法</strong></li></ul><h2 id="2-3-实验设计和算法评测"><a href="#2-3-实验设计和算法评测" class="headerlink" title="2.3 实验设计和算法评测"></a>2.3 实验设计和算法评测</h2><h3 id="2-3-1-数据集"><a href="#2-3-1-数据集" class="headerlink" title="2.3.1 数据集"></a>2.3.1 数据集</h3><p>采用GroupLens提供的MovieLens数据集(数据集详细信息见 <a href="http://www.grouplens.org/node/73" target="_blank" rel="noopener">http://www.grouplens.org/node/73</a>) 介绍和评测各种算法，并且忽略了数据集中的评分记录。</p><h3 id="2-3-2-实验设计"><a href="#2-3-2-实验设计" class="headerlink" title="2.3.2 实验设计"></a>2.3.2 实验设计</h3><p>协同过滤算法的离线实验一般如下设计。首先，将用户行为数据集按照均匀分布随机分成M份，挑选一份作为测试集，将剩下的M-1份作为训练集。然后在训练集上建立用户兴趣模型，并在测试集上对用户行为进行预测，统计出相应的评测指标。为了保证评测指标并不是过拟合的结果，需要进行M次实验，并且每次都使用不同的测试集。然后将M次实验测出的评测指标的平均值作为最终的评测指标。</p><p>下面的代码描述了将数据集随机分成训练集和测试集的过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span><span class="params">(data, M, k, seed)</span>:</span></span><br><span class="line">    test = []</span><br><span class="line">    train = []</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>,M) == k:</span><br><span class="line">            test.append([user,item])</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            train.append([user,item])</span><br><span class="line">    <span class="keyword">return</span> train, test</span><br></pre></td></tr></table></figure><p>这里，每次实验选取不同的k（0≤k≤M-1）和相同的随机数种子seed，进行M次实验就可以得到M个不同的训练集和测试集，然后分别进行实验，用M次实验的平均值作为最后的评测指标。这样做主要是防止某次实验的结果是过拟合的结果（over fitting），但如果数据集够大，模型够简单，为了快速通过离线实验初步地选择算法，也可以只进行一次实验。</p><h3 id="2-3-3-评测指标"><a href="#2-3-3-评测指标" class="headerlink" title="2.3.3 评测指标"></a>2.3.3 评测指标</h3><h4 id="召回率和准确率"><a href="#召回率和准确率" class="headerlink" title="召回率和准确率"></a>召回率和准确率</h4><p>计算方法和第一章预测准确度中TopN推荐部分一样</p><h4 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h4><p>同样与之前介绍的一致。如下代码可以用来计算推荐算法的覆盖率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Coverage</span><span class="params">(train, test, N)</span>:</span></span><br><span class="line">    recommend_items = set() </span><br><span class="line">    all_items = set() </span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user].keys():</span><br><span class="line">            all_items.add(item)</span><br><span class="line">        rank = GetRecommendation(user, N)</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            recommend_items.add(item) </span><br><span class="line">    <span class="keyword">return</span> len(recommend_items) / (len(all_items) * <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><h4 id="新颖度"><a href="#新颖度" class="headerlink" title="新颖度"></a>新颖度</h4><p>使用推荐列表中物品的平均流行度度量推荐结果的新颖度。如果推荐出的物品都很热门，说明推荐的新颖度较低，否则说明推荐结果比较新颖。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Popularity</span><span class="params">(train, test, N)</span>:</span></span><br><span class="line">    item_popularity = dict() </span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items.keys() </span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_popularity:</span><br><span class="line">                item_popularity[item] = <span class="number">0</span> </span><br><span class="line">            item_popularity[item] += <span class="number">1</span> </span><br><span class="line">    ret = <span class="number">0</span> </span><br><span class="line">    n = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        rank = GetRecommendation(user, N)</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            ret += math.log(<span class="number">1</span> + item_popularity[item]) </span><br><span class="line">            n += <span class="number">1</span> </span><br><span class="line">    ret /= n * <span class="number">1.0</span> </span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p>计算平均流行度时对每个物品的流行度取对数，这是因为物品的流行度分布满足长尾分布，在取对数后，流行度的平均值更加稳定。</p><h2 id="2-4-基于近邻的算法"><a href="#2-4-基于近邻的算法" class="headerlink" title="2.4 基于近邻的算法"></a>2.4 基于近邻的算法</h2><p>基于邻域的算法分为两大类，一类是<strong>基于用户的协同过滤算法</strong>，另一类是<strong>基于物品的协同过滤算法</strong>。</p><h3 id="2-4-1-基于用户的协同过滤算法-UserCF算法"><a href="#2-4-1-基于用户的协同过滤算法-UserCF算法" class="headerlink" title="2.4.1 基于用户的协同过滤算法(UserCF算法)"></a>2.4.1 基于用户的协同过滤算法(UserCF算法)</h3><p>基于用户的协同过滤算法是推荐系统中最古老的算法。这个算法的诞生标志了推荐系统的诞生。该算法在1992年被提出，并应用于邮件过滤系统，1994年被GroupLens用于新闻过滤。在此之后直到2000年，该算法都是推荐系统领域最著名的算法。</p><h4 id="1-基础算法"><a href="#1-基础算法" class="headerlink" title="1.基础算法"></a>1.基础算法</h4><p>在一个在线个性化推荐系统中，当一个用户A需要个性化推荐时，可以先找到和他有相似兴趣的其他用户，然后把那些用户喜欢的、而用户A没有听说过的物品推荐给A。这种方法称为<em>基于用户的协同过滤算法</em>。<br>基于用户的协同过滤算法主要包括两个步骤。</p><ol><li>找到和目标用户兴趣相似的用户集合。</li><li>找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。</li></ol><p>步骤(1)的关键是计算两个用户的兴趣相似度。协同过滤算法主要利用行为的相似度计算兴趣的相似度。给定用户u和用户v，令$N(u)$表示用户u曾经有过正反馈的物品集合，令$N(v)$为用户v曾经有过正反馈的物品集合。可以通过如下的Jaccard公式简单计算用户u和用户v的兴趣相似度：$$w_{uv} = \frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}$$<br>或者通过余弦相似度计算：$$w_{uv} = \frac{|N(u) \cap N(v)|}{\sqrt{|N(u)||N(v)|}}$$<br>实现余弦相似度可以利用如下的伪码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UserSimilarity</span><span class="params">(train)</span>:</span></span><br><span class="line">    W = dict() </span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> train.keys():</span><br><span class="line">            <span class="keyword">if</span> u == v:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            W[u][v] = len(train[u] &amp; train[v]) </span><br><span class="line">            W[u][v] /= math.sqrt(len(train[u]) * len(train[v]) * <span class="number">1.0</span>) </span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure><p>该代码对两两用户都利用余弦相似度计算相似度。这种方法的时间复杂度是$O(|U|*|U|)$，这在用户数很大时非常耗时。由于实际上很多用户户相互之间并没有对同样的物品产生过行为，即$|N(u)\cap N(v)| = 0 $，因此会将很多时间浪费在计算用户之间相似度上。换个思路，首先计算出$|N(u)\cap N(v)|\neq 0$的用户对$(u, v)$，然后再对这种情况除以分母$\sqrt{|N(u)||N(v)|}$。<br>为此，首先建立物品到用户的倒排表，对于每个物品都保存对该物品产生过行为的用户列表。令稀疏矩阵$C[u][v]= N(u) \cap N(v)$ 。那么，假设用户u和用户v同时属于倒排表中K个物品对应的用户列表，就有$C[u][v]=K$。从而，可以扫描倒排表中每个物品对应的用户列表，将用户列表中的两两用户对应的$C[u][v]$加1，最终就可以得到所有用户之间不为0的$C[u][v]$。下面的代码实现了上面提到的算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UserSimilarity</span><span class="params">(train)</span>:</span></span><br><span class="line">    <span class="comment"># build inverse table for item_users </span></span><br><span class="line">    item_users = dict() </span><br><span class="line">    <span class="keyword">for</span> u, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> items.keys():</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> item_users: </span><br><span class="line">                item_users[i] = set() </span><br><span class="line">            item_users[i].add(u)</span><br><span class="line">            </span><br><span class="line">    <span class="comment">#calculate co-rated items between users </span></span><br><span class="line">    C = dict() </span><br><span class="line">    N = dict() </span><br><span class="line">    <span class="keyword">for</span> i, users <span class="keyword">in</span> item_users.items():</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> users:</span><br><span class="line">            N[u] += <span class="number">1</span> </span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> users: </span><br><span class="line">                <span class="keyword">if</span> u == v: </span><br><span class="line">                    <span class="keyword">continue</span> </span><br><span class="line">                C[u][v] += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">    <span class="comment">#calculate finial similarity matrix W </span></span><br><span class="line">    W = dict() </span><br><span class="line">    <span class="keyword">for</span> v, cuv <span class="keyword">in</span> related_users.items(): </span><br><span class="line">        W[u][v] = cuv / math.sqrt(N[u] * N[v]) </span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure><p>得到用户之间的兴趣相似度后，UserCF算法会给用户推荐和他兴趣最相似的K个用户喜欢的物品。如下的公式度量了UserCF算法中用户u对物品i的感兴趣程度：$$p(u,i)=\sum_{v \in S(u,K) \cap N(i)} w_{uv}r_{vi}$$<br>其中，$S(u,K)$包含和用户u兴趣最接近的K个用户，$N(i)$是对物品i有过行为的用户集合，$w_{uv}$是用户u和用户v的兴趣相似度，$r_{vi}$代表用户v对物品i的兴趣，因为使用的是单一行为的隐反馈数据，所以所有的$r_{vi}=1$。</p><p>如下代码实现了上面的UserCF推荐算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Recommend</span><span class="params">(user, train, W)</span>:</span></span><br><span class="line">    rank = dict() </span><br><span class="line">    interacted_items = train[user] </span><br><span class="line">    <span class="keyword">for</span> v, wuv <span class="keyword">in</span> sorted(W[u].items, key=itemgetter(<span class="number">1</span>), \</span><br><span class="line">        reverse=<span class="keyword">True</span>)[<span class="number">0</span>:K]:</span><br><span class="line">        <span class="keyword">for</span> i, rvi <span class="keyword">in</span> train[v].items:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> interacted_items:</span><br><span class="line">                <span class="comment"># we should filter items user interacted before</span></span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            rank[i] += wuv * rvi </span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure><p>参数K是UserCF的一个重要参数，它的调整对推荐算法的各种指标都会产生一定的影响。</p><ul><li><strong>准确率和召回率</strong> 选择合适的K对于获得高的推荐系统精度比较重要。推荐结果的精度对K也不是特别敏感，只要选在一定的区域内，就可以获得不错的精度。</li><li><strong>流行度</strong> K越大，参考的人越多，结果就越来越趋近于全局热门的物品。</li><li><strong>覆盖率</strong> K越大，则UserCF推荐结果的覆盖率越低。覆盖率的降低是因为流行度的增加，随着流行度增加，UserCF越来越倾向于推荐热门的物品，从而对长尾物品的推荐越来越少，因此造成了覆盖率的降低。</li></ul><h4 id="2-用户相似度计算的改进"><a href="#2-用户相似度计算的改进" class="headerlink" title="2.用户相似度计算的改进"></a>2.用户相似度计算的改进</h4><p>之前介绍的通过余弦相似度公式计算兴趣相似度，但是由于这个公式过于粗糙，于是需要改进该公式来提高UserCF的推荐性能。<br>研究表明，用户对冷门物品采取与热门物品同样的行为更能说明他们兴趣的相似度。因此，John S. Breese在论文(参见John S. Breese、 David Heckerman和 Carl Kadie的论文“ Empirical Analysis of Predictive Algorithms for Collaborative Filtering”（Morgan Kaufmann Publishers，1998）)中提出了如下公式，根据用户行为计算用户的兴趣相似度：$$w_{uv} = \frac {\sum_{i \in N(u) \cap N(v)}\frac{1}{\log 1 + |N(i)|}}{\sqrt{|N(u)||N(v)|}}$$<br>该公式通过$\frac{1}{\log 1 + |N(i)|}$惩罚了用户u和用户v共同兴趣列表中热门物品对他们相似度的影响。</p><p>将基于上述用户相似度公式的UserCF算法记为User-IIF算法。下面的代码实现了上述用户相似度公式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UserSimilarity</span><span class="params">(train)</span>:</span></span><br><span class="line">    <span class="comment"># build inverse table for item_users</span></span><br><span class="line">    item_users = dict() </span><br><span class="line">    <span class="keyword">for</span> u, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> items.keys():</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> item_users: </span><br><span class="line">                item_users[i] = set() </span><br><span class="line">            item_users[i].add(u)</span><br><span class="line">            </span><br><span class="line">    <span class="comment">#calculate co-rated items between users </span></span><br><span class="line">    C = dict() </span><br><span class="line">    N = dict() </span><br><span class="line">    <span class="keyword">for</span> i, users <span class="keyword">in</span> item_users.items():</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> users:</span><br><span class="line">            N[u] += <span class="number">1</span> </span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> users:</span><br><span class="line">                <span class="keyword">if</span> u == v:</span><br><span class="line">                    <span class="keyword">continue</span> </span><br><span class="line">                C[u][v] += <span class="number">1</span> / math.log(<span class="number">1</span> + len(users))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#calculate finial similarity matrix W </span></span><br><span class="line">    W = dict() </span><br><span class="line">    <span class="keyword">for</span> u, related_users <span class="keyword">in</span> C.items():</span><br><span class="line">        <span class="keyword">for</span> v, cuv <span class="keyword">in</span> related_users.items(): </span><br><span class="line">            W[u][v] = cuv / math.sqrt(N[u] * N[v]) </span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure><p>通过实验评测证明计算用户兴趣相似度时考虑物品的流行度对提升推荐效果的质量确实有帮助。</p><h4 id="3-实际在线系统中使用UserCF的例子"><a href="#3-实际在线系统中使用UserCF的例子" class="headerlink" title="3.实际在线系统中使用UserCF的例子"></a>3.实际在线系统中使用UserCF的例子</h4><p>相比我们后面要讨论的基于物品的协同过滤算法(ItemCF)，UserCF在目前的实际应用中使用并不多。其中最著名的使用者是Digg，书中这部分介绍了Digg的推荐系统设计思路。</p><h4 id="4-UserCF算法的缺点"><a href="#4-UserCF算法的缺点" class="headerlink" title="4.UserCF算法的缺点"></a>4.UserCF算法的缺点</h4><p>首先，随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系。其次，基于用户的协同过滤很难对推荐结果作出解释。</p><h3 id="2-4-2-基于物品的协同过滤算法-ItemCF"><a href="#2-4-2-基于物品的协同过滤算法-ItemCF" class="headerlink" title="2.4.2 基于物品的协同过滤算法(ItemCF)"></a>2.4.2 基于物品的协同过滤算法(ItemCF)</h3><p><em>基于物品的协同过滤（item-based collaborative filtering）算法</em>是目前业界应用最多的算法。无论是亚马逊网，还是Netflix、Hulu、YouTube，其推荐算法的基础都是该算法。</p><h4 id="1-基础算法-1"><a href="#1-基础算法-1" class="headerlink" title="1.基础算法"></a>1.基础算法</h4><p>ItemCF算法并不利用物品的内容属性计算物品之间的相似度，它主要通过分析用户的行为记录计算物品之间的相似度。该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B 。</p><p>基于物品的协同过滤算法主要分为两步：</p><ol><li>计算物品之间的相似度。</li><li>根据物品的相似度和用户的历史行为给用户生成推荐列表。</li></ol><p>为了避免推荐出热门的商品，用下面的公式定义物品的相似度：$$w_{ij}=\frac {|N(i)\cap N(j)|} {\sqrt{|N(i)||N(j)|}}$$<br>这个公式惩罚了物品j的权重，因此减轻了热门物品会和很多物品相似的可能性。</p><p>和UserCF算法类似，用ItemCF算法计算物品相似度时也可以首先建立用户—物品倒排表（即对每个用户建立一个包含他喜欢的物品的列表），然后对于每个用户，将他物品列表中的物品两两在共现矩阵C中加1。</p><p>详细代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ItemSimilarity</span><span class="params">(train)</span>:</span></span><br><span class="line">    <span class="comment">#calculate co-rated users between items </span></span><br><span class="line">    C = dict() </span><br><span class="line">    N = dict() </span><br><span class="line">    <span class="keyword">for</span> u, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> users:</span><br><span class="line">            N[i] += <span class="number">1</span> </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> users:</span><br><span class="line">                <span class="keyword">if</span> i == j:</span><br><span class="line">                    <span class="keyword">continue</span> </span><br><span class="line">                C[i][j] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#calculate finial similarity matrix W </span></span><br><span class="line">    W = dict() </span><br><span class="line">    <span class="keyword">for</span> i,related_items <span class="keyword">in</span> C.items():</span><br><span class="line">        <span class="keyword">for</span> j, cij <span class="keyword">in</span> related_items.items(): </span><br><span class="line">            W[u][v] = cij / math.sqrt(N[i] * N[j]) </span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure></p><p>在得到物品之间的相似度后，ItemCF通过如下公式计算用户u对一个物品j的兴趣：$$p_{uj}=\sum_{i\in N(u) \cap S(j,K)} {w_{ji}r_{ui}} $$<br>这里$N(u)$是用户喜欢的物品的集合，$S(j,K)$是和物品$j$最相似的$K$个物品的集合，$w_{ji}$是物品$j$和$i$的相似度，$r_{ui}$是用户$u$对物品$i$的兴趣。（对于隐反馈数据集，如果用户$u$对物品$i$有过行为，即可令$r_{ui}$ =1。）该公式的含义是，和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。</p><p>该公式的实现代码如下所示。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Recommendation</span><span class="params">(train, user_id, W, K)</span>:</span></span><br><span class="line">    rank = dict() </span><br><span class="line">    ru = train[user_id] </span><br><span class="line">    <span class="keyword">for</span> i,pi <span class="keyword">in</span> ru.items():</span><br><span class="line">        <span class="keyword">for</span> j, wj <span class="keyword">in</span> sorted(W[i].items(), / key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:K]: </span><br><span class="line">            <span class="keyword">if</span> j <span class="keyword">in</span> ru:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            rank[j] += pi * wj </span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure></p><p>ItemCF的一个优势就是可以提供推荐解释，即利用用户历史上喜欢的物品为现在的推荐结果进行解释。</p><p>如下代码实现了带解释的ItemCF算法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Recommendation</span><span class="params">(train, user_id, W, K)</span>:</span></span><br><span class="line">    rank = dict() </span><br><span class="line">    ru = train[user_id] </span><br><span class="line">    <span class="keyword">for</span> i,pi <span class="keyword">in</span> ru.items():</span><br><span class="line">        <span class="keyword">for</span> j, wj <span class="keyword">in</span> sorted(W[i].items(), / key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:K]: </span><br><span class="line">            <span class="keyword">if</span> j <span class="keyword">in</span> ru:</span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">            rank[j].weight += pi * wj </span><br><span class="line">            rank[j].reason[i] = pi * wj </span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure></p><p>参数K同样也是ItemCF算法中的一个重要参数。</p><ul><li><strong>精度(准确率和召回率)</strong> ItemCF推荐结果的精度也是不和K成正相关或者负相关的，因此选择合适的K对获得最高精度是非常重要的。</li><li><strong>流行度</strong> 和UserCF不同，参数K对ItemCF推荐结果流行度的影响也不是完全正相关的。随着K的增加，结果流行度会逐渐提高，但当K增加到一定程度，流行度就不会再有明显变化。</li><li><strong>覆盖率</strong> K增加会降低系统的覆盖率。</li></ul><h4 id="2-用户活跃度对物品相似度的影响"><a href="#2-用户活跃度对物品相似度的影响" class="headerlink" title="2.用户活跃度对物品相似度的影响"></a>2.用户活跃度对物品相似度的影响</h4><p>John S. Breese在论文中(参见John S. Breese、 David Heckerman和 Carl Kadie的“ Empirical Analysis of Predictive Algorithms for Collaborative Filtering”（Morgan Kaufmann Publishers ，1998）)提出了一个称为<strong>IUF(Inverse User Frequence)</strong>，即用户活跃度对数的倒数的参数，他认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加<em>IUF 参数</em>来修正物品相似度的计算公式：$$w_{ij}=\frac{\sum_{u\in N(i)\cap N(j)} \frac{1}{\log1 + |N(u)|}}{\sqrt{|N(i)||N(j)|}}$$</p><p>上面的公式只是对活跃用户做了一种软性惩罚，对于很多过于活跃的用户，为了避免相似度矩阵过于稠密，在实际计算中一般直接忽略他的兴趣列表，而不将其纳入到相似度计算的数据集中。</p><p>下面代码实现了改进后的算法，并将改进后的算法记为ItemCF-IUF：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ItemSimilarity</span><span class="params">(train)</span>:</span></span><br><span class="line">    <span class="comment">#calculate co-rated users between items </span></span><br><span class="line">    C = dict() </span><br><span class="line">    N = dict() </span><br><span class="line">    <span class="keyword">for</span> u, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> users:</span><br><span class="line">            N[i] += <span class="number">1</span> </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> users:</span><br><span class="line">                 <span class="keyword">if</span> i == j:</span><br><span class="line">                     <span class="keyword">continue</span> </span><br><span class="line">                 C[i][j] += <span class="number">1</span> / math.log(<span class="number">1</span> + len(items) * <span class="number">1.0</span>)</span><br><span class="line">                    </span><br><span class="line">    <span class="comment">#calculate finial similarity matrix W </span></span><br><span class="line">    W = dict() </span><br><span class="line">    <span class="keyword">for</span> i,related_items <span class="keyword">in</span> C.items():</span><br><span class="line">        <span class="keyword">for</span> j, cij <span class="keyword">in</span> related_items.items(): </span><br><span class="line">            W[u][v] = cij / math.sqrt(N[i] * N[j]) </span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure></p><p>通过离线算法评测该算法证明，ItemCF-IUF在准确率和召回率两个指标上和ItemCF相近，但ItemCF-IUF明显提高了推荐结果的覆盖率，降低了推荐结果的流行度。从这个意义上说，ItemCF-IUF确实改进了ItemCF的综合性能。</p><h4 id="3-物品相似度的归一化"><a href="#3-物品相似度的归一化" class="headerlink" title="3.物品相似度的归一化"></a>3.物品相似度的归一化</h4><p>Karypis在研究中发现如果将ItemCF的相似度矩阵按最大值归一化，可以提高推荐的准确率(参见George Karypis的论文“ Evaluation of Item-based Top-N Recommendation Algorithms”)。其研究表明，如果已经得到了物品相似度矩阵w，那么可以用如下公式得到归一化之后的相似度矩阵$w’$：$$w_{ij}’ = \frac{w_{ij}}{max_j{w_{ij}}}$$<br>归一化的好处不仅仅在于增加推荐的准确度，它还可以提高推荐的覆盖率和多样性。从实验结果可以看到，归一化确实能提高ItemCF的性能，其中各项指标都有了比较明显的提高。</p><h3 id="2-4-3-UserCF和ItemCF的综合比较"><a href="#2-4-3-UserCF和ItemCF的综合比较" class="headerlink" title="2.4.3 UserCF和ItemCF的综合比较"></a>2.4.3 UserCF和ItemCF的综合比较</h3><h4 id="UserCF和ItemCF的应用领域比较及原因"><a href="#UserCF和ItemCF的应用领域比较及原因" class="headerlink" title="UserCF和ItemCF的应用领域比较及原因"></a>UserCF和ItemCF的应用领域比较及原因</h4><p>UserCF多被用于新闻推荐比如Digg，而ItemCF则在电子商务和书籍电影推荐方面得到广泛应用比如Amazon和Netflix。<br>UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF 的推荐结果着重于维系用户的历史兴趣。换句话说，UserCF的推荐更社会化，反映了用户所在的小型兴趣群体中物品的热门程度，而ItemCF的推荐更加个性化，反映了用户自己的兴趣传承。<br>在新闻网站中，用户的兴趣不是特别细化，绝大多数用户都喜欢看热门的新闻。即使是个性化，也是比较粗粒度的。因此，个性化新闻推荐更加强调抓住新闻热点，热门程度和时效性是个性化新闻推荐的重点，而个性化相对于这两点略显次要。因此，UserCF可以给用户推荐和他有相似爱好的一群其他用户今天都在看的新闻，这样在抓住热点和时效性的同时， 保证了一定程度的个性化。 这是 Digg在新闻推荐中使用UserCF的最重要原因。<br>UserCF适合用于新闻推荐的另一个原因是从技术角度考量的。因为作为一种物品，新闻的更新非常快，每时每刻都有新内容出现，而ItemCF需要维护一张物品相关度的表，如果物品更新很快，那么这张表也需要很快更新，这在技术上很难实现。绝大多数物品相关度表都只能做到一天一次更新，这在新闻领域是不可以接受的。而UserCF只需要用户相似性表，虽然UserCF对于新用户也需要更新相似度表，但在新闻网站中，物品的更新速度远远快于新用户的加入速度，而且对于新用户，完全可以给他推荐最热门的新闻，因此UserCF显然是利大于弊。<br>但是，在图书、电子商务和电影网站，比如亚马逊、豆瓣、Netflix中，ItemCF则能极大地发挥优势。首先，在这些网站中，用户的兴趣是比较固定和持久的。而且这些网站中有一些活跃度很高的人，例如技术人员。一个技术人员可能都是在购买 技术方面的书，而且他们对书的热门程度并不是那么敏感，事实上越是资深的技术人员，他们看的书就越可能不热门。此外，这些系统中的用户大都不太需要流行度来辅助他们判断一个物品的 好坏，而是可以通过自己熟悉领域的知识自己判断物品的质量。因此，这些网站中个性化推荐的任务是帮助用户发现和他研究领域相关的物品。因此，ItemCF算法成为了这些网站的首选算法。 此外，这些网站的物品更新速度不会特别快，一天一次更新物品相似度矩阵对它们来说不会造成太大的损失，是可以接受的。<br>同时，从技术上考虑，UserCF需要维护一个用户相似度的矩阵，而ItemCF需要维护一个物品相似度矩阵。从存储的角度说，如果用户很多，那么维护用户兴趣相似度矩阵需要很大的空间， 同理，如果物品很多，那么维护物品相似度矩阵代价较大。<br>在早期的研究中，大部分研究人员都是让少量的用户对大量的物品进行评价，然后研究用户兴趣的模式。对于他们来说，因为用户很少，计算用户兴趣相似度是最快也是最简单的方法。但在实际的互联网中，用户数目往往非常庞大，而在图书、电子商务网站中，物品的数目则是比较少的。此外，物品的相似度相对于用户的兴趣一般比较稳定，因此使用ItemCF是比较好的选择。当然，新闻网站是个例外，在那儿，物品的相似度变化很快，物品数目庞大， 相反用户兴趣则相对固定（都是喜欢看热门的），所以新闻网站的个性化推荐使用UserCF算法的更多。</p><p><img src="/2018/11/24/推荐系统实践读书笔记（二）/UserCF和ItemCF优缺点的对比.png" alt="UserCF和ItemCF优缺点的对比"></p><h4 id="UserCF与ItemCF的性能比较及原因"><a href="#UserCF与ItemCF的性能比较及原因" class="headerlink" title="UserCF与ItemCF的性能比较及原因"></a>UserCF与ItemCF的性能比较及原因</h4><p>离线实验结果可见，ItemCF算法在各项指标上似乎都不如UserCF，特别是其推荐结果的覆盖率和新颖度都低于UserCF。<br>似乎与之前所说的不符合。首先要指出的是，离线实验的性能在选择推荐算法时并不起决定作用。首先应该满足产品的需求，比如如果需要提供推荐解释，那么可能得选择ItemCF算法。其次，需要看实现代价，比如若用户太多，很难计算用户相似度矩阵，这个时候可能不得不抛弃UserCF算法。最后，离线指标和点击率等在线指标不一定成正比。而且，这里对比的是最原始的UserCF和ItemCF算法，这两种算法都可以进行各种各样的改进。一般来说，这两种算法经过优化后，最终得到的离线性能是近似的。</p><h4 id="哈利波特问题"><a href="#哈利波特问题" class="headerlink" title="哈利波特问题"></a>哈利波特问题</h4><p>亚马逊网的研究人员在设计ItemCF算法之初发现ItemCF算法计算出的图书相关表存在一个问题，就是很多书都和《哈利波特》相关。  也就是说，购买任何一本书的人似乎都会购买《哈利波特》。后来他们研究发现，主要是因为《哈利波特》太热门了，确实是购买任何一本书的人几乎都会购买它。<br>回顾一下ItemCF计算物品相似度的经典公式：$$w_{ij}=\frac {\vert N(i)\cap N(j) \vert} {\sqrt{\vert N(i)||N(j)\vert}}$$<br>这个问题的原因是，如果j非常热门， 那么上面公式的分子$N (i ) \cap  N ( j )$就会越来越接近$N (i)$。 尽管上面的公式分母已经考虑到了$j$的流行度，但在实际应用中，热门的j仍然会获得比较大的相似度。</p><p>哈利波特问题有几种解决方案。<br>第一种是在分母上加大对热门物品的惩罚，比如采用如下公式：$$w_{ij} = \frac {\vert N(i) \cap N(j) \vert}{\vert  N(i)\vert^{1-\alpha} \vert  N(j)\vert^{\alpha}}$$<br>其中$\alpha \in [0.5,1]$。通过提高$\alpha$，就可以惩罚热门的j。</p><p>如果α＝0.5就是标准的ItemCF算法。从离线实验结果可以看到，α只有在取值为0.5时才会导致最高的准确率和召回率，而无论α＜0.5或者α＞0.5都不会带来这两个指标的提高。但是，如果看覆盖率和平均流行度就可以发现，α越大，覆盖率就越高，并且结果的平均热门程度会降低。因此，通过这种方法可以在适当牺牲准确率和召回率的情况下显著提升结果的覆盖率和新颖性（降低流行度即提高了新颖性）。<br>上述方法还不能彻底地解决哈利波特问题。每个用户一般都会在不同的领域喜欢一种物品。两个不同领域的最热门物品之间往往具有比较高的相似度。这个时候，仅仅靠用户行为数据是不能解决这个问题的，因为用户的行为表示这种物品之间应该相似度很高。此时，我们只能依靠引入物品的内容数据解决这个问题，比如对不同领域的物品降低权重等。这些就不是协同过滤讨论的范畴了。</p><h2 id="2-5-隐语义模型"><a href="#2-5-隐语义模型" class="headerlink" title="2.5 隐语义模型"></a>2.5 隐语义模型</h2><p>自从Netflix Prize比赛举办以来，LFM（latent factor model）隐语义模型逐渐成为推荐系统领域耳熟能详的名词。其实该算法最早在文本挖掘领域被提出，用于找到文本的隐含语义。</p><h3 id="2-5-1-基础算法"><a href="#2-5-1-基础算法" class="headerlink" title="2.5.1 基础算法"></a>2.5.1 基础算法</h3><p>隐语义模型的核心思想是通过特征(latent factor)联系用户兴趣和物品。<br>针对推荐问题除了UserCF、ItemCF算法，还有一种方法就是根据用户的兴趣进行分类。对于某个用户，首先得到他的兴趣分类，然后从分类中挑选他可能喜欢的物品。<br>这个基于兴趣分类的方法大概需要解决3个问题：</p><ul><li>如何给物品进行分类？</li><li>如何确定用户对哪些类的物品感兴趣，以及感兴趣的程度？</li><li>对于一个给定的类，选择哪些属于这个类的物品推荐给用户，以及如何确定这些物品在一个类中的权重？</li></ul><p>对于第一个问题的简单解决方案是找编辑给物品分类。但是，即使有很系统的分类体系，编辑给出的分类仍然具有以下缺点。</p><ul><li>编辑的意见不能代表各种用户的意见。编辑的分类大部分是从书的内容出 发，而不是从书的读者群出发。</li><li>编辑很难控制分类的粒度。</li><li>编辑很难给一个物品多个分类。有的书不仅属于一个类，而是可能属于很多的类。</li><li>编辑很难给出多维度的分类。</li><li>编辑很难决定一个物品在某一个分类中的权重。</li></ul><p>为了解决上面的问题，研究人员提出：为什么我们不从数据出发，自动地找到那些类，然后进行个性化推荐？于是，隐含语义分析技术（latent variable analysis）出现了。隐含语义分析技术因为采取基于用户行为统计的自动聚类，较好地解决了上面提出的5个问题。</p><ul><li>编辑的意见不能代表各种用户的意见，但隐含语义分析技术的分类来自对用户行为的统计，代表了用户对物品分类的看法。隐含语义分析技术和ItemCF在物品分类方面的思想类似，如果两个物品被很多用户同时喜欢，那么这两个物品就很有可能属于同一个类。</li><li>编辑很难控制分类的粒度，但隐含语义分析技术允许我们指定最终有多少个分类，这个数字越大，分类的粒度就会越细，反正分类粒度就越粗。</li><li>编辑很难给一个物品多个分类，但隐含语义分析技术会计算出物品属于每个类的权重，因此每个物品都不是硬性地被分到某一个类中。</li><li>编辑很难给出多维度的分类，但隐含语义分析技术给出的每个分类都不是同一个维度的，它是基于用户的共同兴趣计算出来的，如果用户的共同兴趣是某一个维度，那么LFM给出的类也是相同的维度。</li><li>编辑很难决定一个物品在某一个分类中的权重，但隐含语义分析技术可以通过统计用户行为决定物品在每个类中的权重，如果喜欢某个类的用户都会喜欢某个物品，那么这个物品在这个类中的权重就可能比较高。</li></ul><p>隐含语义分析技术从诞生到今天产生了很多著名的模型和方法，其中和该技术相关且耳熟能详的名词有pLSA、LDA、隐含类别模型（latent class model）、隐含主题模型（latent topic model）、矩阵分解（matrix factorization）。这些技术和方法在本质上是相通的，其中很多方法都可以用于个性化推荐系统。</p><p>LFM通过如下公式计算用户u对物品i的兴趣：$$Preference(u,i)=r_{ui}=p_u^Tq_i=\sum_{f=1}^F p_{u,k} q_{i,k}$$<br>这个公式中$p_{u,k}$和$q_{i,k}$是模型的参数，其中$p_{u,k}$度量了用户u的兴趣和第k个隐类的关系，而$q_{i,k}$度量了第k个隐类和物品i之间的关系。</p><p>要计算这两个参数，需要一个训练集，对于每个用户u，训练集里都包含了用户u喜欢的物品和不感兴趣的物品，通过学习这个数据集，就可以获得上面的模型参数。</p><p>LFM在显性反馈数据（也就是评分数据）上解决评分预测问题并达到了很好的精度。不过本章主要讨论的是隐性反馈数据集，这种数据集的特点是只有正样本（用户喜欢什么物品），而没有负样本（用户对什么物品不感兴趣）。<br>在隐性反馈数据集上应用LFM解决TopN推荐的第一个关键问题就是如何给每个用户生成负样本。 对于这个问题，Rong Pan在文章(参见“One-Class Collaborative Filtering”)中进行了深入探讨。他对比了如下几种方法。</p><ul><li>对于一个用户，用他所有没有过行为的物品作为负样本。</li><li>对于一个用户，从他没有过行为的物品中均匀采样出一些物品作为负样本。</li><li>对于一个用户，从他没有过行为的物品中采样出一些物品作为负样本，但采样时，保证每个用户的正负样本数目相当。</li><li>对于一个用户，从他没有过行为的物品中采样出一些物品作为负样本，但采样时，偏重采样不热门的物品。</li></ul><p>对于第一种方法，它的明显缺点是负样本太多，正负样本数目相差悬殊，因而计算复杂度很高，最终结果的精度也很差。对于另外3种方法，Rong Pan在文章中表示第三种好于第二种，而第二种好于第四种。</p><p>作者认为对负样本采样时应该遵循以下原则：</p><ul><li>对每个用户，要保证正负样本的平衡（数目相似）。</li><li>对每个用户采样负样本时，要选取那些很热门，而用户却没有行为的物品。</li></ul><p>下面的代码实现了负样本采样过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RandomSelectNegativeSample</span><span class="params">(self, items)</span>:</span></span><br><span class="line">    ret = dict() </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> items.keys():</span><br><span class="line">        ret[i] = <span class="number">1</span> </span><br><span class="line">    n = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(items) * <span class="number">3</span>): <span class="comment"># 将上限设为len(items) * 3，主要是保证正、负样本数量接近</span></span><br><span class="line">        item = items_pool[random.randint(<span class="number">0</span>, len(items_pool) - <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">in</span> ret:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        ret[item] = <span class="number">0</span></span><br><span class="line">        n + = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n &gt; len(items): </span><br><span class="line">            <span class="keyword">break</span> </span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p>在上面的代码中，<code>items_pool</code>维护了候选物品的列表，在这个列表中，物品i出现的次数和物品i的流行度成正比。<code>items</code>是一个dict，它维护了用户已经有过行为的物品的集合。因此，上面的代码按照物品的流行度采样出了那些热门的、但用户却没有过行为的物品。经过采样，可以得到一个用户—物品集$K = {(u,i)}$，其中如果$(u, i)$是正样本，则有$r_{ui}$ = 1，否则有$r_{ui}$ = 0 。然后， 需要优化如下的损失函数来找到最合适的参数p和q：$$C = \sum_{(u,i) \in K} (r_{ui} - \hat r_{ui})^2 =  \sum_{(u,i) \in K}(r_{ui} - \sum_{k=1}^K p_{u,k}q_{i,k})^2 + \lambda \Vert p_u \Vert^2 + \lambda \Vert q_i \Vert^2$$<br>这里$\lambda \Vert p_u \Vert^2 + \lambda \Vert q_i \Vert^2$用来防止过拟合的正则化项，$\lambda$可以通过实验获得。最小化上面的损失函数，可以利用随机梯度下降算法。该算法是最优化理论里最基础的优化算法，它首先通过求参数的偏导数找到最速下降方向，然后通过迭代法不断地优化参数。下面介绍优化方法的数学推导。</p><p>上面定义的损失函数里有两组参数$p_{u,k}$和$q_{i,k}$，随机梯度下降法需要首先对它们分别求偏导数，可以得到：$$\frac{\partial C}{\partial p_{uk}} = -2q_{ik} + 2\lambda p_{uk}$$ $$\frac{\partial C}{\partial q_{ik}} = -2p_{uk} + 2\lambda q_{ik}$$</p><p>然后，根据随机梯度下降法，需要将参数沿着最速下降方向向前推进，因此可以得到如下递推公式：$$p_{uk}=p_{uk} + \alpha(q_{ik}-\lambda p_{uk})$$ $$q_{ik} = q_{ik} + \alpha (p_{uk} - \lambda q_{ik})$$<br>其中，$\alpha$是学习速率(learning rate)，它的选取需要通过反复实验获得。</p><p>下面的Python代码实现了这一优化过程：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LatentFactorModel</span><span class="params">(user_items, F, N, alpha, lambda)</span>:</span></span><br><span class="line">    [P, Q] = InitModel(user_items, F) </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">0</span>,N):</span><br><span class="line">        <span class="keyword">for</span> user, items <span class="keyword">in</span> user_items.items():</span><br><span class="line">           samples = RandSelectNegativeSamples(items) </span><br><span class="line">            <span class="keyword">for</span> item, rui <span class="keyword">in</span> samples.items():</span><br><span class="line">                eui = rui - Predict(user, item) </span><br><span class="line">                <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>, F):</span><br><span class="line">                    P[user][f] += alpha * (eui * Q[item][f] - \ <span class="keyword">lambda</span> * P[user][f]) </span><br><span class="line">                    Q[item][f] += alpha * (eui * P[user][f] - \ <span class="keyword">lambda</span> * Q[item][f])</span><br><span class="line">        alpha *= <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Recommend</span><span class="params">(user, P, Q)</span>:</span></span><br><span class="line">    rank = dict() </span><br><span class="line">    <span class="keyword">for</span> f, puf <span class="keyword">in</span> P[user].items():</span><br><span class="line">        <span class="keyword">for</span> i, qfi <span class="keyword">in</span> Q[f].items():</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> rank: </span><br><span class="line">                rank[i] += puf * qfi </span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure></p><p>经过离线实验评测证明，LFM确实可以实现通过用户行为将物品聚类的功能。</p><p>在LFM中，重要的参数有4个：</p><ul><li>隐特征的个数F；</li><li>学习速率alpha；</li><li>正则化参数lambda；</li><li>负样本/正样本比例 ratio。</li></ul><p>通过实验发现， ratio 参数对LFM的性能影响最大。随着负样本数目的增加， LFM 的准确率和召回率有明显提高。 不过当ratio&gt;10以后，准确率和召回率基本就比较稳定了。同时，随着负样本数目的增加，覆盖率不断降低，而推荐结果的流行度不断增加，说明ratio参数控制了推荐算法发掘长尾的能力。当数据集非常稀疏时，LFM的性能会明显下降，甚至不如UserCF和ItemCF的性能。</p><h3 id="2-5-2-基于LFM的实际系统的例子"><a href="#2-5-2-基于LFM的实际系统的例子" class="headerlink" title="2.5.2 基于LFM的实际系统的例子"></a>2.5.2 基于LFM的实际系统的例子</h3><p>雅虎的研究人员公布过一个使用LFM进行雅虎首页个性化设计的方案(参见Bee-Chung Chen、Deepak Agarwal、Pradheep Elango和Raghu Ramakrishnan的“Latent Factor Models for Web Recommender Systems”)。</p><p>雅虎的研究人员以CTR作为优化目标，利用LFM来预测用户是否会单击一个链接。为此， 他们将用户历史上对首页上链接的行为记录作为训练集。其中，如果用户u单击过链接i，那么就定义$(u, i)$是正样本，即$r_{ui}$ = 1。如果链接i展示给用户u，但用户u从来没有单击过，那么就定义$(u, i)$是负样本，即$r_{ui}$ = -1。然后，雅虎的研究人员利用前文提到的LFM预测用户是否会单击链接：$$\hat r_{ui} = p_u^T \cdot q_i$$</p><p>LFM模型在实际使用中有一个困难，那就是它很难实现实时的推荐。经典的LFM模型每次训练时都需要扫描所有的用户行为记录，这样才能计算出用户隐类向量$p_u$和物品隐类向量$q_i$。而且LFM的训练需要在用户行为记录上反复迭代才能获得比较好的性能。因此，LFM的每次训练都很耗时，一般在实际应用中只能每天训练一次，并且计算出所有用户的推荐结果。从而LFM模型不能因为用户行为的变化实时地调整推荐结果来满足用户最近的行为。为了解决传统LFM不能实时化，而产品需要实时性的矛盾，雅虎的研究人员提出了一个解决方案。</p><p>他们的解决方案分为两个部分。首先，他们利用新闻链接的内容属性（关键词、类别等）得到链接i的内容特征向量$y_i$。其次，他们会实时地收集用户对链接的行为，并且用这些数据得到链接i的隐特征向量$q_i$。然后，他们会利用如下公式预测用户u是否会单击链接i：$$r_{ui} = x^T_u \cdot y_i + p_u^T \cdot q_i$$</p><p>其中，$y_i$是根据物品的内容属性直接生成的，$x_{uk}$是用户u对内容特征k的兴趣程度，用户向量$x_{u}$可以根据历史行为记录获得，而且每天只需要计算一次。而$p_u$、$q_i$是根据实时拿到的用户最近几小时的行为训练LFM获得的。因此，对于一个新加入的物品i，可以通过$ x^T_u \cdot y_i$估计用户u对物品i的兴趣，然后经过几个小时后，就可以通过$p_u^T \cdot q_i$得到更加准确的预测值。</p><h3 id="2-5-3-LFM和基于邻域的方法比较"><a href="#2-5-3-LFM和基于邻域的方法比较" class="headerlink" title="2.5.3 LFM和基于邻域的方法比较"></a>2.5.3 LFM和基于邻域的方法比较</h3><p>LFM是一种基于机器学习的方法，具有比较好的理论基础。这个方法和基于邻域的方法（比如UserCF、ItemCF）相比，各有优缺点。下面将从不同的方面对比LFM和基于邻域的方法。</p><ul><li><strong>理论基础</strong> LFM具有比较好的理论基础，它是一种学习方法，通过优化一个设定的指标建立最优的模型。基于邻域的方法更多的是一种基于统计的方法，并没有学习过程。</li><li><strong>离线计算的空间复杂度</strong> 基于邻域的方法需要维护一张离线的相关表。在离线计算相关表的过程中，如果用户/物品数很多，将会占据很大的内存。假设有M个用户和N个物品，在计算相关表的过程中，我们可能会获得一张比较稠密的临时相关表（尽管最终我们对每个物品只保留K个最相关的物品，但在中间计算过程中稠密的相关表是不可避免的），那么假设是用户相关表，则需要$O(M * M)$的空间，而对于物品相关表，则需要 $O(N * N)$的空间。而LFM在建模过程中，如果是F个隐类，那么它需要的存储空间是$O(F * (M+N))$，这在M和N很大时可以很好地节省离线计算的内存。</li><li><strong>离线计算的时间复杂度</strong> 假设有M个用户、N个物品、K条用户对物品的行为记录。那么，UserCF计算用户相关表的时间复杂度是$O(N * (K/N)^2)$，而ItemCF计算物品相关表的时间复杂度是$O(M * (K / M)^2)$。而对于LFM，如果用F个隐类，迭代S次，那么它的计算复杂度是$O(K * F * S)$。那么，如果$K / N &gt; F * S$，则代表UserCF的时间复杂度低于LFM ，如果$K / M&gt;F * S$，则说明ItemCF的时间复杂度低于LFM。在一般情况下，LFM的时间复杂度要稍微高于UserCF和ItemCF，这主要是因为该算法需要多次迭代。但总体上，这两种算法在时间复杂度上没有质的差别。</li><li><strong>在线实时推荐</strong> UserCF和ItemCF在线服务算法需要将相关表缓存在内存中，然后可以在线进行实时的预测。以ItemCF算法为例，一旦用户喜欢了新的物品，就可以通过查询内存中的相关表将和该物品相似的其他物品推荐给用户。因此，一旦用户有了新的行为， 而且该行为被实时地记录到后台的数据库系统中，他的推荐列表就会发生变化。而从LFM的预测公式可以看到，LFM在给用户生成推荐列表时，需要计算用户对所有物品的兴趣权重，然后排名，返回权重最大的N个物品。那么，在物品数很多时，这一过程的时间复杂度非常高，可达$O(M * N * F)$。因此，LFM不太适合用于物品数非常庞大的系统，如果要用，我们也需要一个比较快的算法给用户先计算一个比较小的候选列表，然后再用LFM重新排名。另一方面，LFM在生成一个用户推荐列表时速度太慢，因此不能在线实时计算，而需要离线将所有用户的推荐结果事先计算好存储在数据库中。因此，LFM不能进行在线实时推荐，也就是说，当用户有了新的行为后，他的推荐列表不会发生变化。</li><li><strong>推荐解释</strong> ItemCF算法支持很好的推荐解释，它可以利用用户的历史行为解释推荐结果。但LFM无法提供这样的解释，它计算出的隐类虽然在语义上确实代表了一类兴趣和物品，却很难用自然语言描述并生成解释展现给用户。</li></ul><h2 id="2-6-基于图的模型"><a href="#2-6-基于图的模型" class="headerlink" title="2.6 基于图的模型"></a>2.6 基于图的模型</h2><h3 id="2-6-1-用户行为数据的二分图表示"><a href="#2-6-1-用户行为数据的二分图表示" class="headerlink" title="2.6.1 用户行为数据的二分图表示"></a>2.6.1 用户行为数据的二分图表示</h3><p>在研究基于图的模型之前，首先需要将用户行为数据表示成图的形式。本章讨论的用户行为数据是由一系列二元组组成的，其中每个二元组$(u, i)$表示用户u对物品i产生过行为。<br>令$G(V,E)$表示用户物品二分图，其中$V = V_U \cup V_I$由用户顶点集合$V_U$和物品顶点集合$V_I$组成。对于数据集中每一个二元组$(u, i)$，图中都有一套对应的边$e(v_u,v_i)$，其中$v_u \in V_U$是用户u对应的顶点，$v_i \in V_I$是物品i对应的顶点。</p><h3 id="2-6-2-基于图的推荐算法"><a href="#2-6-2-基于图的推荐算法" class="headerlink" title="2.6.2 基于图的推荐算法"></a>2.6.2 基于图的推荐算法</h3><p>如果将个性化推荐算法放到二分图模型上，那么给用户u推荐物品的任务就可以转化为度量用户顶点$v_u$和与$v_u$没有边直接相连的物品节点在图上的相关性，相关性越高的物品在推荐列表中的权重就越高。</p><p>度量图中两个顶点之间相关性的方法很多，但一般来说图中顶点的相关性主要取决于下面3个因素：</p><ul><li>两个顶点之间的路径数；</li><li>两个顶点之间路径的长度；</li><li>两个顶点之间的路径经过的顶点。</li></ul><p>而相关性高的一对顶点一般具有如下特征：</p><ul><li>两个顶点之间有很多路径相连；</li><li>连接两个顶点之间的路径长度都比较短；</li><li>连接两个顶点之间的路径不会经过出度比较大的顶点。</li></ul><p>基于上面3个主要因素，研究人员设计了很多计算图中顶点之间相关性的方法(参见Fouss Francois、Pirotte Alain、Renders Jean-Michel和Saerens Marco的“Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation”(IEEE Transactions on Knowl edge and Data Eng ineering, 2007))。本节将介绍一种基于随机游走的PersonalRank算法(参见Taher H .Haveliwala的“Topic-Sensitive PageRank”（WWW 2002, 2002）)。</p><p>假设要给用户u进行个性化推荐，可以从用户u对应的节点$v_u$开始在用户物品二分图上进行随机游走。游走到任何一个节点时，首先按照概率α决定是继续游走，还是停止这次游走并从$v_u$节点开始重新游走。如果决定继续游走，那么就从当前节点指向的节点中按照均匀分布随机选择一个节点作为游走下次经过的节点。这样，经过很多次随机游走后，每个物品节点被访问到的概率会收敛到一个数。最终的推荐列表中物品的权重就是物品节点的访问概率。</p><p>如果将上面的描述表示成公式，可以得到如下公式：<br>$$<br>PR(v) =<br>\begin{cases}<br>\alpha \sum_{v’ \in in(v)} \frac{PR(v’)}{\vert out(v’) \vert}  &amp;  (v \neq v_u) \\<br>(1- alpha) + \alpha \sum_{v’ \in in(v)} \frac{PR(v’)}{\vert out(v’) \vert} &amp; (v = v_u)<br>\end{cases}<br>$$</p><p>下面的代码简单实现了上面的公式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PersonalRank</span><span class="params">(G, alpha, root)</span>:</span></span><br><span class="line">    rank = dict() </span><br><span class="line">    rank = &#123;x:<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> G.keys()&#125; </span><br><span class="line">    rank[root] = <span class="number">1</span> </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        tmp = &#123;x:<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> G.keys()&#125;</span><br><span class="line">        <span class="keyword">for</span> i, ri <span class="keyword">in</span> G.items():</span><br><span class="line">            <span class="keyword">for</span> j, wij <span class="keyword">in</span> ri.items():</span><br><span class="line">                <span class="keyword">if</span> j <span class="keyword">not</span> <span class="keyword">in</span> tmp:</span><br><span class="line">                    tmp[j] = <span class="number">0</span> </span><br><span class="line">                tmp[j] += <span class="number">0.6</span> * rank[i] / (<span class="number">1.0</span> * len(ri)) </span><br><span class="line">                <span class="keyword">if</span> j == root:</span><br><span class="line">                    tmp[j] += <span class="number">1</span> - alpha</span><br><span class="line">        rank = tmp </span><br><span class="line">    <span class="keyword">return</span> rank</span><br></pre></td></tr></table></figure><p>虽然PersonalRank算法可以通过随机游走进行比较好的理论解释，但该算法在时间复杂度上有明显的缺点。因为在为每个用户进行推荐时，都需要在整个用户物品二分图上进行迭代，直到整个图上的每个顶点的PR值收敛。这一过程的时间复杂度非常高，不仅无法在线提供实时推荐，甚至离线生成推荐结果也很耗时。</p><p>为了解决PersonalRank每次都需要在全图迭代并因此造成时间复杂度很高的问题，给出两种解决方案。第一种就是减少迭代次数，在收敛之前就停止。这样会影响最终的精度，但一般来说影响不会特别大。另一种方法就是从矩阵论出发，重新设计算法。</p><p>对矩阵运算比较熟悉的读者可以轻松将PersonalRank转化为矩阵的形式。令M为用户物品二分图的转移概率矩阵，即：$$M(v, v’) = \frac {1}{\vert out(v) \vert}​$$<br>进而迭代公式可以转化为：<br>$$<br>\begin{align}<br>r&amp; = (1-\alpha)r_0 + \alpha M^Tr \\<br>&amp; = (1-\alpha)(1-\alpha M^T)^{-1}r_0<br>\end{align}<br>$$</p><p>因此，只需要一次$(1-\alpha M^T)^{-1}$，这里$1-\alpha M^T$是稀疏矩阵。关于如何对稀疏矩阵快速求逆，可以参考矩阵计算方面的书籍和论文(比如Song Li的“Fast Algorithms For Sparse Matrix Inverse Compuataions”（2009）)。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第2章-利用用户行为数据&quot;&gt;&lt;a href=&quot;#第2章-利用用户行为数据&quot; class=&quot;headerlink&quot; title=&quot;第2章 利用用户行为数据&quot;&gt;&lt;/a&gt;第2章 利用用户行为数据&lt;/h1&gt;&lt;p&gt;用户行为数据中蕴涵着很多不是那么显而易见的规律，而个性化推荐算法的任务就是通过计算机去发现这些规律，从而为产品的设计提供指导，提高用户体验。&lt;br&gt;基于&lt;strong&gt;用户行为分析&lt;/strong&gt;的推荐算法是个性化推荐系统的重要算法，学术界一般将这种类型的算法称为&lt;strong&gt;协同过滤算法&lt;/strong&gt;。顾名思义，协同过滤就是指用户可以齐心协力，通过不断地和网站互动，使自己的推荐列表能够不断过滤掉自己不感兴趣的物品，从而越来越满足自己的需求。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实践读书笔记（一）</title>
    <link href="https://rilzob.com/2018/11/23/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://rilzob.com/2018/11/23/推荐系统实践读书笔记（一）/</id>
    <published>2018-11-23T04:36:25.802Z</published>
    <updated>2019-03-24T03:19:19.749Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第1章-好的推荐系统"><a href="#第1章-好的推荐系统" class="headerlink" title="第1章 好的推荐系统"></a>第1章 好的推荐系统</h1><p>在研究如何设计推荐系统前，了解什么是好的推荐系统至关重要。只有了解了优秀的推荐系统的特征，我们才能在设计推荐系统时根据实际情况进行取舍。</p><p>本章分3个步骤回答这个问题：</p><ol><li>首先介绍了什么是推荐系统、推荐系统的主要任务、推荐系统和分类目录以及搜索引擎的区别等；</li><li>然后按照不同领域分门别类地介绍目前业界常见的个性化推荐应用；</li><li>最后介绍推荐系统的评测，通过介绍评测指标给出“好”的定义，从而最终解答“什么是好的推荐系统”这个问题。</li></ol><a id="more"></a><h2 id="1-1-什么是推荐系统"><a href="#1-1-什么是推荐系统" class="headerlink" title="1.1 什么是推荐系统"></a>1.1 什么是推荐系统</h2><h3 id="推荐系统产生的背景"><a href="#推荐系统产生的背景" class="headerlink" title="推荐系统产生的背景"></a>推荐系统产生的背景</h3><p>随着信息技术和互联网的发展，人们逐渐从<strong>信息匮乏</strong>的时代走入了<strong>信息过载(overload)</strong>的时代。在这个时代，无论是信息消费者还是信息产生着都遇到了很大的挑战：作为信息消费者，如何从大量信息中找到自己感兴趣的信息是一件非常困难的的事情；作为信息产生者，如何让自己生产的信息脱颖而出，受到广大用户的关注，也是一件非常困难的事情。推荐系统就是解决这一矛盾的重要工具。</p><h3 id="推荐系统的任务"><a href="#推荐系统的任务" class="headerlink" title="推荐系统的任务"></a>推荐系统的任务</h3><p>推荐系统的任务就是联系用户和信息，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。</p><h3 id="推荐系统与搜索引擎的异同"><a href="#推荐系统与搜索引擎的异同" class="headerlink" title="推荐系统与搜索引擎的异同"></a>推荐系统与搜索引擎的异同</h3><p>众所周知，为了解决信息过载的问题，最具代表的解决方案是分类目录(雅虎)和搜索引擎(谷歌)。最初的分类目录网站将著名的网站分门别类，从而方便用户根据类别查找网站，然而随着互联网规模的不断扩大，门户网站也只能覆盖少量的热门网站，越来越不满足用户的需求。因此搜索引擎诞生了。但是搜索引擎需要用户主动提供准确的关键词来寻找信息，因此不能解决用户的很多其他需求，比如用户无法提供准确的关键词时，搜索引擎就无能为力了。</p><p>和搜索引擎一样，推荐系统也是一种帮助用户快速发现有用信息的工具。但和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。因此从某种意义上说，推荐系统和搜索引擎对于用户来说是两个互补的工具。搜索引擎满足了用户有明确目的时的主动查找需求，而推荐系统能够在用户没有明确目的的时候他们发现感兴趣的新内容。</p><h3 id="生活中常见的推荐过程"><a href="#生活中常见的推荐过程" class="headerlink" title="生活中常见的推荐过程"></a>生活中常见的推荐过程</h3><p>以看电影为例：</p><ol><li>向朋友咨询。这种方式在推荐系统中称为<strong>社会化推荐(social recommendation)</strong>，即让好友给自己推荐物品。</li><li>寻找与自己之前看过的电影在内容上相似的电影。这种推荐方式在推荐系统中称为<strong>基于内容的推荐(content-based filtering)</strong>。</li><li>查看排行榜，看看别人都在看什么电影又或者看看和自己兴趣相近的人看什么电影。这种方式称为<strong>基于协同过滤(collaborative filtering)</strong>的推荐。</li></ol><p>从上面三种方法可以看出，推荐算法的<strong>本质</strong>是通过一定的方式将用户和物品联系起来，而不同的推荐系统利用了不同的方式。</p><h2 id="1-2-个性化推荐系统的应用"><a href="#1-2-个性化推荐系统的应用" class="headerlink" title="1.2 个性化推荐系统的应用"></a>1.2 个性化推荐系统的应用</h2><p>在互联网的各类网站中都可以看到推荐系统的应用，而个性化推荐系统在这些网站中的主要作用是通过分析大量用户行为日志，给不同用户提供不同的个性化页面展示，以提高网站的点击率和转化率。</p><p>尽管不同的网站使用不同的推荐系统技术，但总地来说，几乎所有推荐系统应用都是由<strong>前台的展示页面</strong>、<strong>后台的日志系统</strong>、<strong>推荐算法系统</strong>3部分构成。</p><h3 id="1-2-1-电子商务"><a href="#1-2-1-电子商务" class="headerlink" title="1.2.1 电子商务"></a>1.2.1 电子商务</h3><p>著名的电子商务网站亚马逊(Amazon)是个性化推荐系统的积极应用者和推广者。Amazon的推荐系统融入到了其各类产品中，其中最主要的应用是个性化商品推荐列表和相关商品的推荐列表。</p><h4 id="个性化推荐列表"><a href="#个性化推荐列表" class="headerlink" title="个性化推荐列表"></a>个性化推荐列表</h4><h5 id="基于物品的推荐算法-item-based-method"><a href="#基于物品的推荐算法-item-based-method" class="headerlink" title="基于物品的推荐算法(item-based method)"></a>基于物品的推荐算法(item-based method)</h5><p>该算法给用户用户推荐那些和他们之前喜欢的物品相似的物品。</p><h6 id="个性化推荐列表组成部分"><a href="#个性化推荐列表组成部分" class="headerlink" title="个性化推荐列表组成部分"></a>个性化推荐列表组成部分</h6><ol><li>推荐结果的标题、缩略图以及其他内容属性。</li><li>推荐结果的平均分。</li><li>推荐理由。并且允许用户修正这一推荐</li></ol><h5 id="基于好友的推荐算法"><a href="#基于好友的推荐算法" class="headerlink" title="基于好友的推荐算法"></a>基于好友的推荐算法</h5><p>该算法按照用户在Facebook的好友关系，给用户推荐他们的好友在亚马逊上喜欢的物品。<br>基于该种推荐算法生成的推荐列表的组成部分与基于物品的推荐列表类似，只不过这里的推荐理由换成了喜欢过相关物品的用户好友的头像。</p><h4 id="相关推荐列表"><a href="#相关推荐列表" class="headerlink" title="相关推荐列表"></a>相关推荐列表</h4><p>Amazon有两种相关商品列表：</p><ul><li>包含购买了这个商品的用户也经常购买的其他商品</li><li>包含浏览过这个商品的用户经常购买的其他商品</li></ul><p>这两种相关推荐列表的区别就是使用了不同用户行为计算物品的相关性。<br> 此外，相关推荐列表最重要的应用就是<strong>打包销售(cross selling)</strong>。</p><h3 id="1-2-2-电影和视频网站"><a href="#1-2-2-电影和视频网站" class="headerlink" title="1.2.2 电影和视频网站"></a>1.2.2 电影和视频网站</h3><p>代表公司Netflix、Youtube、Hulu。其中Netflix和Youtube的算法与Amazon的算法类似，也是基于物品的推荐算法，即给用户推荐和他们曾经喜欢的视频相似的视频。</p><h3 id="1-2-3-个性化音乐网络电台"><a href="#1-2-3-个性化音乐网络电台" class="headerlink" title="1.2.3 个性化音乐网络电台"></a>1.2.3 个性化音乐网络电台</h3><p>个性化推荐的成功应用需要两个条件：</p><ol><li>存在信息过载。因为如果用户可以很容易地从所有物品中找到喜欢的物品，就不需要个性化推荐了。</li><li>用户大部分时候没有特别明确的需求。因为用户如果有明确的需求，可以直接通过搜索引擎找到感兴趣的物品。</li></ol><p>在这两个条件下，个性化网络电台无疑是最合适的个性化推荐产品。<br>目前有很多知名的个性化音乐网络电台。国际上著名的有Pandora和Last.fm，国内的代表则是网易云音乐。这三种应用虽然都是个性化网络电台，但背后的技术却不太一样。</p><h4 id="Pandora"><a href="#Pandora" class="headerlink" title="Pandora"></a>Pandora</h4><p>Pandora背后的音乐推荐系统主要来自于一个叫做音乐基因工程的项目。Pandora的算法主要基于内容，其音乐家和研究人员亲自听了上万首来自不同歌手的歌，然后对歌曲的不同特性(比如旋律、节奏、编曲和歌词等)<br>进行标注，这些标注被称为音乐的基因。然后，Pandora会根据专家标注的基因计算歌曲的相似度，并给用户推荐和他之前喜欢的音乐在基因上相似的其他音乐。</p><h4 id="Last-fm"><a href="#Last-fm" class="headerlink" title="Last.fm"></a>Last.fm</h4><p>Last.fm记录了所有用户的听歌记录以及用户对歌曲的反馈，在这一基础上计算出不同用户在歌曲上的喜好相似度，从而给用户推荐和他有相似听歌爱好的其他用户喜欢的歌曲。同时，Last.fm也建立了一个社交网络，让用户能够和其他用户建立联系，同时也能让用户给好友推荐自己喜欢的歌曲。和Pandora相比，Last.fm没有使用专家标注，而是主要利用用户行为计算歌曲的相似度。</p><h4 id="音乐推荐的特点"><a href="#音乐推荐的特点" class="headerlink" title="音乐推荐的特点"></a>音乐推荐的特点</h4><p>2011年的Recsys大会专门要求了Pandora和研究人员对音乐推荐系统进行了演讲。演讲人总结了音乐推荐的如下特点：</p><ol><li>物品空间大；</li><li>消费每首歌的代价很小；</li><li>物品种类丰富；</li><li>听一首歌耗时很少；</li><li>物品重用率很高；</li><li>用户充满激情；</li><li>上下文相关；</li><li>次序很重要；</li><li>很多播放列表资源；</li><li>不需要用户全神贯注；</li><li>高度社会化；</li></ol><p>上面这些特点决定了音乐是一种非常适合用来推荐的物品。因此，尽管现在很多推荐系统都是作为一个应用存在与网站中，比如Amazon的商品推荐和Netflix的电影推荐，但唯有音乐推荐系统可以支持独立的个性化推荐网站，比如Pandora、Last.fm和豆瓣网络电台。</p><h3 id="1-2-4-社交网络"><a href="#1-2-4-社交网络" class="headerlink" title="1.2.4 社交网络"></a>1.2.4 社交网络</h3><p>社交网络中的个性化推荐主要应用于3个方面：</p><ol><li>利用用户的社交网络信息对用户进行个性化的物品推荐；</li><li>信息流的会话推荐；</li><li>给用户推荐好友；</li></ol><h3 id="1-2-5-个性化阅读"><a href="#1-2-5-个性化阅读" class="headerlink" title="1.2.5 个性化阅读"></a>1.2.5 个性化阅读</h3><p>个性化阅读同样符合前面提出的需要个性化推荐的两个因素：首先，互联网上的文章很多，用户面临信息过载的问题；其次，用户很多时候并没有必须看某篇具体文章的需求，他们只是想通过阅读特定领域的文章了解这些领域的动态。</p><h4 id="Google-Reader"><a href="#Google-Reader" class="headerlink" title="Google Reader"></a>Google Reader</h4><p>Google Reader是一款流行的社会化阅读工具。它允许用户关注自己感兴趣的人，然后看到所关注用户分享的文章。</p><h4 id="Zite"><a href="#Zite" class="headerlink" title="Zite"></a>Zite</h4><p>和Google Reader不同，个性化阅读工具Zite则是收集用户对文章的偏好信息。在每篇文章右侧，Zite都允许用户给出喜欢或不喜欢的反馈，然后通过分析用户的反馈数据不停地更新用户的个性化文章列表。</p><h4 id="Digg"><a href="#Digg" class="headerlink" title="Digg"></a>Digg</h4><p>Digg是一家著名的新闻阅读网站。Digg首先根据用户的Digg历史计算用户之间的兴趣相似度，然后给用户推荐和他兴趣相似的用户喜欢的文章。</p><h3 id="1-2-6-基于位置的服务"><a href="#1-2-6-基于位置的服务" class="headerlink" title="1.2.6 基于位置的服务"></a>1.2.6 基于位置的服务</h3><p>随着移动设备的飞速发展，用户的位置信息已经非常容易获取，而位置是一种很重要的上下文信息，基于位置给用户推荐离他近的且他感兴趣的服务，用户就更有可能去消费。</p><h4 id="Foursquare"><a href="#Foursquare" class="headerlink" title="Foursquare"></a>Foursquare</h4><p>基于位置的服务往往和社交网络结合在一起。其中Foursquare推出了探索功能，给用户推荐好友在附近的行为。</p><h3 id="1-2-7-个性化邮件"><a href="#1-2-7-个性化邮件" class="headerlink" title="1.2.7 个性化邮件"></a>1.2.7 个性化邮件</h3><p>使对用户重要的邮件能够让用户优先浏览。</p><h4 id="Tapestry"><a href="#Tapestry" class="headerlink" title="Tapestry"></a>Tapestry</h4><p>目前在文献中能够查到的第一个推荐系统Tapestry就是一个个性化邮件推荐系统，它通过分析用户阅读邮件的历史行为和习惯对新邮件进行重新排序，从而提高用户的工作效率。</p><h3 id="1-2-8-个性化广告"><a href="#1-2-8-个性化广告" class="headerlink" title="1.2.8 个性化广告"></a>1.2.8 个性化广告</h3><p>个性化广告投放目前已经成为了一门独立的学科——计算广告，但该学科和推荐系统在很多基础理论和方法上是相通的，比如它们的目的都是联系用户和物品，只是在个性化广告中，物品就是广告。</p><h4 id="个性化广告投放和狭义个性化推荐的区别"><a href="#个性化广告投放和狭义个性化推荐的区别" class="headerlink" title="个性化广告投放和狭义个性化推荐的区别"></a>个性化广告投放和狭义个性化推荐的区别</h4><p>个性化推荐着重于帮助用户找到可能令他们感兴趣的物品，而广告推荐着重于帮助广告找到可能对它们感兴趣的用户，即一个是以用户为核心，而另一个是以广告为核心。</p><h4 id="个性化广告投放技术"><a href="#个性化广告投放技术" class="headerlink" title="个性化广告投放技术"></a>个性化广告投放技术</h4><p>目前个性化广告投放技术主要分为3种：</p><ol><li><strong>上下文广告</strong>。通过分析用户正在浏览的网页内容，投放和网页内容相关的广告。代表系统是谷歌的Adsense。</li><li><strong>搜索广告</strong>。通过分析用户在当前会话中的搜索记录，判断用户的搜索目的，投放和用户目的相关的广告。</li><li><strong>个性化展示广告</strong>。我们经常在很多网站看到大量的展示广告(就是那些大的横幅图片)，它们是根据用户的兴趣，对不同用户投放不同的展示广告。</li></ol><p>Yahoo发表了大量个性化广告方面的论文，而最成功的则是Facebook。</p><h2 id="1-3-推荐系统评测"><a href="#1-3-推荐系统评测" class="headerlink" title="1.3 推荐系统评测"></a>1.3 推荐系统评测</h2><p>一个完整的推荐系统一般存在3个参与方：用户、物品提供者和提供推荐系统的网站。同时好的推荐系统设计，能够让推荐系统本身收集到高质量的用户反馈，不断完善推荐的质量，增加用户和网站的交互，提高网站的收入。因此在评测一个推荐算法时，需要同时考虑三方的利益，一个好的推荐系统是能够令三方共赢的系统。<br>在推荐系统的早期研究中，很多人将好的推荐系统定义为能够作出准确预测的推荐系统。但是，后来很多研究表明，准确的预测并不代表好的推荐。举个极端点的例子，某推荐系统预测明天太阳将从东方升起，虽然预测准确率为100%，却是一种没有意义的预测。所以，好的推荐系统不仅仅能够准确预测用户的行为，而且能够扩展用户的视野，帮助用户发现那些他们可能会感兴趣，但却不那么容易发现的东西。<br>为了全面评测推荐系统对三方利益的影响，本章从不同角度出发，提出不同的指标。</p><h3 id="1-3-1-推荐系统实验方法"><a href="#1-3-1-推荐系统实验方法" class="headerlink" title="1.3.1 推荐系统实验方法"></a>1.3.1 推荐系统实验方法</h3><p>首先介绍计算和获得这些指标的主要实验方法。推荐系统中主要有3种评测推荐效果的实验方法，即<strong>离线实验(offline experiment)、用户调查(user study)和在线实验(online experiment)</strong>。</p><h4 id="离线实验"><a href="#离线实验" class="headerlink" title="离线实验"></a>离线实验</h4><h5 id="离线实验的方法"><a href="#离线实验的方法" class="headerlink" title="离线实验的方法"></a>离线实验的方法</h5><p>离线实验的方法一般由如下几个步骤构成：</p><ol><li>通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集；</li><li>将数据集按照一定的规则分成训练集和测试集；</li><li>在训练集上训练用户兴趣模型，在测试集上进行预测；</li><li>通过事先定义的离线指标评测算法评测在测试集上的预测结果。</li></ol><p>从上面的步骤可以看到，推荐系统的离线实验都是在数据集上完成的，也就是说它不需要一个实际的系统来供它实验，而只要有一个从实验系统日志中提取的数据集即可。</p><h5 id="离线实验的优缺点"><a href="#离线实验的优缺点" class="headerlink" title="离线实验的优缺点"></a>离线实验的优缺点</h5><p>这种实验方法的好处是不需要真实用户参与，可以直接快速地计算出来，从而方便、快速地测试大量不同的算法。<br>它的主要缺点是无法获得很多商业上关注的指标，如点击率、转化率等，而找到和商业指标非常相关的离线指标也是很困难的事情。</p><h4 id="用户调查"><a href="#用户调查" class="headerlink" title="用户调查"></a>用户调查</h4><p>用户调查是推荐系统评测的一个重要工具，很多离线时没有办法评测的与用户主观感受有关的指标都可以通过用户调查获得。</p><h5 id="用户调查的方法"><a href="#用户调查的方法" class="headerlink" title="用户调查的方法"></a>用户调查的方法</h5><p>用户调查需要有一些真实用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，我们需要观察和记录他们的行为，并让他们回答一些问题。最后，我们需要通过分析他们的行为和答案了解测试系统的性能。</p><h5 id="用户调查的优缺点"><a href="#用户调查的优缺点" class="headerlink" title="用户调查的优缺点"></a>用户调查的优缺点</h5><p>它的优点是可以获得很多体现用户主观感受的指标，相对在线实验风险很低，出现错误后很容易弥补。<br>缺点是招募测试用户代价较大，很难组织大规模的测试用户，因此会使测试结果的统计意义不足。此外，在很多时候设计双盲实验非常困难，而且用户在测试环境下的行为和真实环境下的行为可能有所不同，因而在测试环境下收集的测试指标可能在真实环境下无法重现。</p><h4 id="在线实验"><a href="#在线实验" class="headerlink" title="在线实验"></a>在线实验</h4><p>在完成离线实验和必要的用户调查后，可以将推荐系统上线做AB测试，将它和旧的算法进行比较。</p><h5 id="在线实验的方法"><a href="#在线实验的方法" class="headerlink" title="在线实验的方法"></a>在线实验的方法</h5><p>AB测试是一种很常用的在线评测算法的实验方法。它通过一定的规则将用户随机分成几组，并对不同组的用户采用不同的算法，然后通过统计不同用户的各种不同的评测指标比较不同算法。</p><blockquote><p>网站<a href="http://www.abtests.com/给出了很多通过实际AB测试提高网站用户满意度的例子，从中我们可以学习到如何进行合理的AB测试。" target="_blank" rel="noopener">http://www.abtests.com/给出了很多通过实际AB测试提高网站用户满意度的例子，从中我们可以学习到如何进行合理的AB测试。</a></p></blockquote><h5 id="在线实验的优缺点"><a href="#在线实验的优缺点" class="headerlink" title="在线实验的优缺点"></a>在线实验的优缺点</h5><p>AB测试的优点是可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标。<br>AB测试的缺点主要是周期比较长，必须进行长期的实验才能得到可靠的结果。因此一般不会用AB测试测试所有的算法，而只是用它测试那些在离线实验和用户调查中表现很好的算法。其次，一个大型网站的AB测试系统的设计也是一项复杂的工程。</p><p>一般来说，一个新的推荐算法最终上线，需要完成上面所说的3个实验。</p><ol><li>首先，需要通过离线实验证明它在很多离线指标上优于现有的算法。</li><li>然后，需要通过用户调查确定它的用户满意度不低于现有的算法。</li><li>最后，通过在线的AB测试确定它在我们关心的指标上优于现有的算法。</li></ol><h3 id="1-3-2-评测指标"><a href="#1-3-2-评测指标" class="headerlink" title="1.3.2 评测指标"></a>1.3.2 评测指标</h3><h4 id="1-用户满意度"><a href="#1-用户满意度" class="headerlink" title="1.用户满意度"></a>1.用户满意度</h4><p> 用户作为推荐系统的重要参与者，其满意度是评测推荐系统的最重要指标。但是，用户满意度没有办法离线计算，只能通过用户调查或在线实验方式获得。<br> 用户调查获得用户满意度主要是通过调查问卷的形式。用户对推荐系统的满意度分为不同的层次。因此在设计问卷时需要考虑到用户各方面的感受，这样用户才能针对问题给出自己准确的回答。<br> 在在线系统中，用户满意度主要通过一些对用户行为的统计得到。更一般的情况下，我们可以用点击率、用户停留时间和转化率等指标度量用户的满意度。</p><h4 id="2-预测准确度"><a href="#2-预测准确度" class="headerlink" title="2.预测准确度"></a>2.预测准确度</h4><p> 预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。<br> 计算方法：在计算该指标时需要有一个离线的数据集，该数据集包含用户的历史行为记录。然后，将该数据集通过时间分为训练集和测试集。最后，通过在训练集上建立用户的行为和兴趣模型预测用户在测试集上的行为，并计算预测行为和测试集上实际行为的重合度作为预测准确度。<br> 不同的研究方向有不同的预测准确度指标。</p><h5 id="评分预测"><a href="#评分预测" class="headerlink" title="评分预测"></a>评分预测</h5><p> 预测用户对物品评分的行为称为评分预测。<br> 评分预测的预测准确度一般通过<strong>均方根误差(RMSE)</strong>和<strong>平均绝对误差(MAE)</strong>计算。对于测试集中的一个用户u和物品i，令$r_{ui}$是用户u对物品i的实际评分，而$\hat r_{ui}$是推荐算法给出的预测评分，那么RMSE的定义为：$$RMSE = \frac {\sqrt{\sum_{u,i \in T} (r_{ui} - \hat r_{ui})^2}}{|T|}$$</p><p>MAE采用绝对值计算预测误差，它的定义为：$$MAE=\frac{\sum_{u,i \in T}|r_{ui}-\hat r_{ui}|} {|T|}$$</p><p>假设我们用一个列表<code>records</code>存放用户评分数据，令<code>records[i] = [u,i,rui,pui]</code>，其中<code>rui</code>是用户u对物品i的实际评分，<code>pui</code>是算法预测出来的用户u对物品i的评分，下面代码实现了RME和MAE的计算过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMSE</span><span class="params">(records)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> math.sqrt(sum([(rui - pui) * (rui - pui) <span class="keyword">for</span> u,i,rui,pui <span class="keyword">in</span> records])/float(len(records)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MAE</span><span class="params">(records)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum([abs(rui-pui) <span class="keyword">for</span> u,i,rui,pui <span class="keyword">in</span> records])/float(len(records))</span><br></pre></td></tr></table></figure><p>关于RMSE和MAE这两个指标的优缺点，Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚(平方项的惩罚)，因而对系统的评测更加苛刻。研究表明，如果评分系统是基于基数建立的(即用户给的评分都是整数)，那么对预测结果取整可能会降低MAE的误差。 </p><h5 id="TopN推荐"><a href="#TopN推荐" class="headerlink" title="TopN推荐"></a>TopN推荐</h5><p>网站在提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做TopN推荐。TopN推荐的预测准确率一般通过准确率(precision)/召回率(recall)度量。<br>$R(u)$是根据用户在训练集上的行为给用户作出的推荐列表，而$T(u)$是用户在测试集上的行为列表。那么，推荐结果的召回率定义为：$$Recall=\frac {\sum_{u \in U} |R(u) \cap T(u)|}{\sum_{u \in U} |T(u)|} $$<br>推荐结果的准确率定义为：$$Precision=\frac {\sum_{u \in U} |R(u) \cap T(u)|}{\sum_{u \in U} |R(u)|} $$<br>下面的Python代码同时计算出了一个推荐算法的准确率和召回率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PrecisionRecall</span><span class="params">(test, N)</span>:</span></span><br><span class="line">    hit = <span class="number">0</span></span><br><span class="line">    n_recall = <span class="number">0</span></span><br><span class="line">    n_precision = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> test.items():</span><br><span class="line">        rank = Recommend(user, N)</span><br><span class="line">        hit += len(rank &amp; item)  <span class="comment"># hit是推荐列表与行为列表相交的部分</span></span><br><span class="line">        n_recall += len(items)  </span><br><span class="line">        n_precision += N</span><br><span class="line">        <span class="keyword">return</span> [hit/(<span class="number">1.0</span>*n_recall),hit/(<span class="number">1.0</span>*n_precision)]</span><br></pre></td></tr></table></figure><p>有的时候，为了全面评测TopN推荐的准确率和召回率，一般会选取不同的推荐列表长度N，计算出一组准确率和召回率，然后画出准确率/召回率曲线(precision/recall curve)。</p><h5 id="关于评分预测和TopN推荐的讨论"><a href="#关于评分预测和TopN推荐的讨论" class="headerlink" title="关于评分预测和TopN推荐的讨论"></a>关于评分预测和TopN推荐的讨论</h5><p>评分预测一直是推荐系统研究的热点，绝大多数推荐系统的研究都是基于用户评分数据的评分预测。这主要是因为，一方面推荐系统的早期研究组GroupLens的研究主要就是基于电影评分 数据MovieLens进行的，其次，Netflix大赛也主要面向评分预测问题。因而，很多研究人员都将 研究精力集中在优化评分预测的RMSE上。</p><p>对此，亚马逊前科学家Greg Linden有不同的看法。2009年, 他在Communications of the ACM 网站发表了一篇文章 (“What is a Good Recommendation Algorithm？”,参见<a href="http://cacm.acm.org/blogs/blog-cacm/22925-what-is-a-goodrecommendation-algorithm/fulltext/" target="_blank" rel="noopener">http://cacm.acm.org/blogs/blog-cacm/22925-what-is-a-goodrecommendation-algorithm/fulltext/</a>) ，指出电影推荐的目的是找到用户最有可能感兴趣的电影，而不是预测用户看了电影后会给电影什么样的评分。因此，TopN推荐更符合实际的应用需求。也许有一部电影用户看了之后会给很高的分数，但用户看的可能性非常小。因此，预测用户是否会看一部电影， 应该比预测用户看了电影后会给它什么评分更加重要。因此，本书主要也是讨论TopN推荐。</p><h4 id="3-覆盖度"><a href="#3-覆盖度" class="headerlink" title="3.覆盖度"></a>3.覆盖度</h4><p>覆盖度(coverage)描述一个推荐系统对物品长尾的发掘能力。覆盖率有不同的定义方法，最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。假设系统的用户集合为$U$，推荐系统给每个用户推荐一个长度为N的物品列表$R(u)$。那么推荐系统的覆盖率可以通过下面的公式计算：$$Coverage=\frac{| \bigcup_{u \in U} R(u)|}{|I|}$$<br>从上面的定义可以看到，覆盖率是一个内容提供商会关心的指标。一个好的推荐系统不仅需要有比较高的用户满意度，也要有较高的覆盖率。<br>但是上面的定义过于粗略。覆盖率为100%的系统可以有无数的物品流行度分布。为了更细致地描述推荐系统发掘长尾的能力，需要统计推荐列表中不同物品出现次数的分布。如果所有的物品都出现在推荐列表中，且出现的次数差不多，那么推荐系统发掘长尾的能力就很好。因此，可以通过研究物品在推荐列表中出现次数的分布描述推荐系统挖掘长尾的能力。如果这个分布比较平，那么说明推荐系统的覆盖率较高，而如果这个分布较陡峭，说明推荐系统的覆盖率较低。在信息论和经济学中有两个著名的指标可以用来定义覆盖率。<br>第一个是<strong>信息熵</strong>：$$H = -\sum_{i=1}^n p(i) \log p(i)$$<br>这里$p(i)$是物品i的流行度除以所有物品的流行度之和。<br>第二个是<strong>基尼系数(Gini Index)</strong>：$$G = \frac{1}{n-1} \sum_{j=1}^n (2j-n-1)p(i_j)$$<br>这里$i_j$是按照物品流行度$p()$从小到大排序的物品列表中第j个物品。<br>下面代码用来计算给定物品流行度分布后的基尼系数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GiniIndex</span><span class="params">(p)</span>:</span></span><br><span class="line">    j = <span class="number">1</span></span><br><span class="line">    n = len(p)</span><br><span class="line">    G = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item, weight <span class="keyword">in</span> sorted(p.items(), key=itemgetter(<span class="number">1</span>)):</span><br><span class="line">        G += (<span class="number">2</span> * j - n - <span class="number">1</span>) * weight</span><br><span class="line">    <span class="keyword">return</span> G / float(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/2018/11/23/推荐系统实践读书笔记（一）/屏幕快照 2018-11-24 下午2.58.23.png" alt="基尼系数的计算原理"></p><p>社会学领域有一个著名的马太效应，即所谓强者更强，弱者更弱的效应。推荐系统的初衷是希望消除马太效应，使得各种物品都能被展示给对它们感兴趣的某一类人群。但是，很多研究表明现在主流的推荐算法（比如协同过滤算法）是具有马太效应的。评测推荐系统是否具有马太效应的简单办法就是使用基尼系数。如果G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G2 &gt; G1，就说明推荐算法具有马太效应。</p><h4 id="4-多样性"><a href="#4-多样性" class="headerlink" title="4.多样性"></a>4.多样性</h4><p>为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域，即推荐结果需要具有多样性。多样性推荐列表的好处用一句俗话表述就是“不在一棵树上吊死”。尽管用户的兴趣在较长的时间跨度中是一样的，但具体到用户访问推荐系统的某一刻，其兴趣往往是单一的，那么如果推荐列表只能覆盖用户的一个兴趣点，而这个兴趣点不是用户这个时刻的兴趣点，推荐列表就不会让用户满意。反之，如果推荐列表比较多样，覆盖了用户绝大多数的兴趣点，那么就会增加用户找到感兴趣物品的概率。因此给用户的推荐列表也需要满足用户广泛的兴趣，即具有多样性。<br>多样性描述了推荐列表中物品两两之间的不相似性。因此，多样性和相似性是对应的。假设$s(i,j) \in [0,1]$定义了物品i和j之间的相似度，那么用户u的推荐列表$R(u)$的多样性定义如下：$$Diversity=1-\frac {\sum_{i,j \in R(u),i \neq j} s(i,j)}{\frac{1}{2}|R(u)|(|R(u)|-1)}$$<br>而推荐系统的整体多样性可以定义为所有用户推荐列表多样性的平均值：$$Diversity=\frac{1}{|U|} \sum_{u \in U}Diversity(R(u))$$<br>从上面的定义可以看到，不同的物品相似度度量函数$s(i, j)$可以定义不同的多样性。如果用内容相似度描述物品间的相似度，我们就可以得到内容多样性函数，如果用协同过滤的相似度函数描述物品间的相似度，就可以得到协同过滤的多样性函数。</p><h4 id="5-新颖性"><a href="#5-新颖性" class="headerlink" title="5.新颖性"></a>5.新颖性</h4><p>新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。在一个网站中实现新颖性的最简单办法是，把那些用户之前在网站中对其有过行为的物品从推荐列表中过滤掉。<br>O’scar Celma在博士论文“Music Recommendation and Discovery in the Long Tail”(参见“Music Recommendation and Discovery in the Long Tail”，地址为<a href="http://mtg.upf.edu/static/media/PhD_ocelma.pdf" target="_blank" rel="noopener">http://mtg.upf.edu/static/media/PhD_ocelma.pdf</a>) 中研究了新颖度的评测。评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。因此，如果推荐结果中物品的平均热门程度较低，那么推荐结果就可能有比较高的新颖性。<br>但是，用推荐结果的平均流行度度量新颖性比较粗略，因为不同用户不知道的东西是不同的。因此，要准确地统计新颖性需要做用户调查。<br>最近几年关于多样性和新颖性的研究越来越受到推荐系统研究人员的关注。ACM的推荐系统会议在2011年有一个专门的研讨会讨论推荐的多样性和新颖性。 (参见“International Workshop on Novelty and Diversity in Recommender Systems”，地址为<a href="http://ir.ii.uam.es/divers2011/" target="_blank" rel="noopener">http://ir.ii.uam.es/divers2011/</a>) 该研讨会的组织者认为，通过牺牲精度来提高多样性和新颖性是很容易的，而困难的是如何在不牺牲精度的情况下提高多样性和新颖性。关心这两个指标的读者可以关注一下这个研讨会最终发表的论文。</p><h4 id="6-惊喜度"><a href="#6-惊喜度" class="headerlink" title="6.惊喜度"></a>6.惊喜度</h4><p>惊喜度（serendipity）是最近这几年推荐系统领域最热门的话题。惊喜度和新颖度作为推荐系统的指标，它们之间的区别并非两个词在中文里含义的区别而是意义上的区别。<br>可以举一个例子说明这两种指标的区别。假设一名用户喜欢周星驰的电影，然后我们给他推荐了一部叫做《临歧》的电影（该电影是1983年由刘德华、周星驰、梁朝伟合作演出的，很少有人知道这部有周星驰出演的电影），而该用户不知道这部电影，那么可以说这个推荐具有新颖性。但是，这个推荐并没有惊喜度，因为该用户一旦了解了这个电影的演员，就不会觉得特别奇怪。但如果我们给用户推荐张艺谋导演的《红高粱》，假设这名用户没有看过这部电影，那么他看完这部电影后可能会觉得很奇怪，因为这部电影和他的兴趣一点关系也没有，但如果用户看完电影<br>后觉得这部电影很不错，那么就可以说这个推荐是让用户惊喜的。这个例子的原始版本来自于Guy Shani的论文(参见Guy Shani和 Asela Gunawardana的“Evaluating Recommendation Systems”) ，他的基本意思就是，如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。<br>目前并没有什么公认的惊喜度指标定义方式，这里只给出一种定性的度量方式。上面提到，令用户惊喜的推荐结果是和用户历史上喜欢的物品不相似，但用户却觉得满意的推荐。那么，定义惊喜度需要首先定义推荐结果和用户历史上喜欢的物品的相似度，其次需要定义用户对推荐结果的满意度。前面也曾提到，用户满意度只能通过问卷调查或者在线实验获得，而推荐结果和用户历史上喜欢的物品相似度一般可以用内容相似度定义。也就是说，如果获得了一个用户观看电影的历史，得到这些电影的演员和导演集合A，然后给用户推荐一个不属于集合A的导演和演员创作的电影，而用户表示非常满意，这样就实现了一个惊喜度很高的推荐。因此提高推荐惊喜度需要提高推荐结果的用户满意度，同时降低推荐结果和用户历史兴趣的相似度。惊喜度的问题最近几年获得了学术界的一定关注，但这方面的工作还不是很成熟。相关工作可以参考Yuan Cao Zhang等的论文(参见Yuan Cao Zhang、Diarmuid Ó Séaghdha、Daniele Quercia和 Tamas Jambor的“Auralist: introducing serendipity into music recommendation.”)和Tomoko Murakami等的论文 (参见Tomoko Murakami、 Koichiro. Mori和Ryohei Orihara的“ Metrics for evaluating the serendipity of recommendationlists”)。</p><h4 id="7-信任度"><a href="#7-信任度" class="headerlink" title="7.信任度"></a>7.信任度</h4><p>对于基于机器学习的自动推荐系统，同样存在信任度（trust）的问题，如果用户信任推荐系统，那就会增加用户和推荐系统的交互。<br>度量推荐系统的信任度只能通过问卷调查的方式，询问用户是否信任推荐系统的推荐结果。<br>提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度(transparency)(参见Henriette Cramer、Vanessa Evers、 Satyan Ramlal、 Maarten van Someren、Lloyd Rutledge、 Natalia Stash、Lora Aroyo和Bob Wielinga的“ The effects of transparency on trust in and acceptance of a content-based art recommender”) ， 而增加推荐系统透明度的主要办法是提供推荐解释。只有让用户了解推荐系统的运行机制，让用 户认同推荐系统的运行机制，才会提高用户对推荐系统的信任度。其次是考虑用户的社交网络 信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。这是因为用户对他们的 好友一般都比较信任，因此如果推荐的商品是好友购买过的，那么他们对推荐结果就会相对比较信任。<br>关于推荐系统信任度的研究(参见Paolo Massa和 Paolo Avesani的“Trust-aware recommender systems”)主要集中在评论网站Epinion的推荐系统上。这是因为Epinion创建了一套用户之间的信任系统来建立用户之间的信任关系，帮助用户判断是否信任当前用户对某一个商品的评论。</p><h4 id="8-实时性"><a href="#8-实时性" class="headerlink" title="8.实时性"></a>8.实时性</h4><p>在很多网站中，因为物品（新闻、微博等）具有很强的时效性，所以需要在物品还具有时效性时就将它们推荐给用户。<br>推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。很多推荐系统都会在离线状态每天计算一次用户推荐列表，然后于在线期间将推荐列表展示给用户。这种设计显然是无法满足实时性的。与用户行为相应的实时性，可以通过推荐列表的变化速率来评测。如果推荐列表在用户有行为后变化不大，或者没有变化，说明推荐系统的实时性不高。<br>实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。</p><h4 id="9-健壮性"><a href="#9-健壮性" class="headerlink" title="9.健壮性"></a>9.健壮性</h4><p>健壮性（即robust,鲁棒性）指标衡量了一个推荐系统抗击作弊的能力。<br>算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。<br>在实际系统中，提高系统的健壮性，除了选择健壮性高的算法，还有以下方法。</p><ul><li>设计推荐系统时尽量使用代价比较高的用户行为。</li><li>在使用数据前，进行攻击检测，从而对数据进行清理。</li></ul><h4 id="10-商业目标"><a href="#10-商业目标" class="headerlink" title="10.商业目标"></a>10.商业目标</h4><p>不同的网站会根据自己的盈利模式设有不同的商业目标。因此，设计推荐系统时需要考虑最终的商业目标，而网站使用推荐系统的目的除了满足用 户发现内容的需求，也需要利用推荐系统加快实现商业上的指标。</p><h4 id="11-总结"><a href="#11-总结" class="headerlink" title="11.总结"></a>11.总结</h4><p><img src="/2018/11/23/推荐系统实践读书笔记（一）/屏幕快照 2018-11-24 下午2.59.19.png" alt="总结获取各种评测指标的途径"><br>对于可以离线优化的指标，作者的看法是应该在给定覆盖率、多样性、新颖性等限制条件下，尽量优化预测准确度。用一个数学公式表达，离线实验的优化目标是：<br>​                最大化预测准确度<br>​                使得 覆盖率 &gt; A<br>​                    多样性 &gt; B<br>​                    新颖性 &gt; C<br>其中，A、B、C的取值应该视不同的应用而定。</p><h3 id="1-3-3-评测维度"><a href="#1-3-3-评测维度" class="headerlink" title="1.3.3 评测维度"></a>1.3.3 评测维度</h3><p>除了应该考虑评测指标，还应考虑评测维度。增加评测维度的目的就是知道一个推荐算法在什么情况下性能最好。这样可以为融合不同推荐算法取得最好的整体性能带来参考。<br>一般情况下，评测维度分为如下3种：</p><ul><li><strong>用户维度</strong>：主要包括用户的人口统计学信息、活跃度以及是不是新用户等。</li><li><strong>物品维度</strong>：包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。</li><li><strong>时间维度</strong>：包括季节，是工作日还是周末，是白天还是晚上等。</li></ul><p>如果能够在推荐系统评测报告中包含不同维度下的系统评测指标，就能帮我们全面地了解推荐系统性能，找到一个看上去比较弱的算法的优势，发现一个看上去比较强的算法的缺点。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第1章-好的推荐系统&quot;&gt;&lt;a href=&quot;#第1章-好的推荐系统&quot; class=&quot;headerlink&quot; title=&quot;第1章 好的推荐系统&quot;&gt;&lt;/a&gt;第1章 好的推荐系统&lt;/h1&gt;&lt;p&gt;在研究如何设计推荐系统前，了解什么是好的推荐系统至关重要。只有了解了优秀的推荐系统的特征，我们才能在设计推荐系统时根据实际情况进行取舍。&lt;/p&gt;
&lt;p&gt;本章分3个步骤回答这个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先介绍了什么是推荐系统、推荐系统的主要任务、推荐系统和分类目录以及搜索引擎的区别等；&lt;/li&gt;
&lt;li&gt;然后按照不同领域分门别类地介绍目前业界常见的个性化推荐应用；&lt;/li&gt;
&lt;li&gt;最后介绍推荐系统的评测，通过介绍评测指标给出“好”的定义，从而最终解答“什么是好的推荐系统”这个问题。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="计算机科学与技术" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/"/>
    
      <category term="推荐系统" scheme="https://rilzob.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Python" scheme="https://rilzob.com/tags/Python/"/>
    
      <category term="读书笔记" scheme="https://rilzob.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>一次顿悟</title>
    <link href="https://rilzob.com/2018/11/11/%E4%B8%80%E6%AC%A1%E9%A1%BF%E6%82%9F/"/>
    <id>https://rilzob.com/2018/11/11/一次顿悟/</id>
    <published>2018-11-11T08:51:35.146Z</published>
    <updated>2018-11-11T08:54:48.305Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直都在搞怎么让百度收录我的Blog的问题，最后也没有解决，耽误了两三天的时间。为什么Google就能爬取Github Pages，百度就不行呢，忍不住去吐槽(这个事真是烦死我)，对了还有Coding也是个坑。</p><p>在解决这个期间也有所思考，并且得到一个顿悟(大喜)，就是人如果想成功就要在高维度上努力而不是低维度。<a id="more"></a></p><p>人的时间都是有限的，并且假设所有人的资质都是相同的(当然是不可能的)，如果能尽可能地节省时间，那么相比其他人就会走得更远。换句话说就是尽可能地让前进的步子迈得更大。说这话什么意思呢?举个例子，如果你高中三年努力考上了清华北大，可能就会比研究生考上清北的人节省出三四年甚至更多的时间。为什么？因为你高中同样花了三年的时间，大学花去了四年时间才和人家只花高中三年的人站在同一高度，有可能还低一些，那么人家是不是比你少花了三四年呢？拿三年换七年血赚啊，谁都明白这个道理。</p><p>上升一个层次再去思考这个问题。有的人本科阶段很努力每天都在研究编程，编程技术很厉害，就比如我。但另一些人会去研究论文，刷绩点，准备出国留学。两者的努力程度是一样的，如果两种人的目标是相同的，那么很明显后者到达目标的速度会明显比前者快很多，这就是努力层次的问题。前者是在低维度努力，而后者是在高纬度努力，类似于高处的人比低处的人看得更远。更形象地距举例就是一个人在高抬腿向前跑，而另一个人则是大跨步向前跑。为什么说前者是低维度而后者是高维度呢？因为前者所做的是有一定经验的人都能去做的，虽然短期可能会得到更现实的成就感，但从长远来看后者所做的开创性研究是不可替代的，更牛逼。</p><p>令人伤心地是之前的我就是第一种人，而我本科前两年的努力都是高抬腿跑。我已经落后很多了。但凡事都有两面性，好的一方面是现在的我意识到了这点，于是我坚定了我要考研清华的决心，只有这样我才能追回来一些时间。相比那些保研的同学我还有一个优势，就是别无选择，我不会因为目标的不确定而恍惚。</p><p>背水一战，向死而生。希望我自己能谨记这次顿悟，会受益匪浅。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近一直都在搞怎么让百度收录我的Blog的问题，最后也没有解决，耽误了两三天的时间。为什么Google就能爬取Github Pages，百度就不行呢，忍不住去吐槽(这个事真是烦死我)，对了还有Coding也是个坑。&lt;/p&gt;
&lt;p&gt;在解决这个期间也有所思考，并且得到一个顿悟(大喜)，就是人如果想成功就要在高维度上努力而不是低维度。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://rilzob.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>Python中常用的内置函数</title>
    <link href="https://rilzob.com/2018/11/09/Python%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/"/>
    <id>https://rilzob.com/2018/11/09/Python中常用的内置函数/</id>
    <published>2018-11-09T01:04:38.129Z</published>
    <updated>2018-11-09T01:25:23.410Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python中常用的内置函数"><a href="#Python中常用的内置函数" class="headerlink" title="Python中常用的内置函数"></a>Python中常用的内置函数</h1><p>Python内置函数(built-in)是随着Python解释器的运行而被创建的。在Python程序中，你可以随时调用这些函数，而且不需要定义。在开发过程中，合理地使用这些内置函数能极大地提升你的开发效率。</p><p>这篇文章是对我在开发过程中经常遇到的内置函数的用法总结。<a id="more"></a></p><h2 id="enumerate-函数"><a href="#enumerate-函数" class="headerlink" title="enumerate()函数"></a>enumerate()函数</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p><code>enumerate()</code>函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合成一个索引序列，同时列出数据和数据下标，一般用于for循环中。</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">enumerate(sequence, [start=<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul><li>sequence: 一个序列、迭代器或其他可迭代对象。</li><li>start: 下标开始位置。</li></ul><h4 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h4><p>返回enumerate(枚举)对象。</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;seasons = [<span class="string">'Spring'</span>, <span class="string">'Summer'</span>, <span class="string">'Fall'</span>, <span class="string">'Winter'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(enumerate(seasons))</span><br><span class="line">[(<span class="number">0</span>, <span class="string">'Spring'</span>), (<span class="number">1</span>, <span class="string">'Summer'</span>), (<span class="number">2</span>, <span class="string">'Fall'</span>), (<span class="number">3</span>, <span class="string">'Winter'</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(enumerate(seasons, start=<span class="number">1</span>))       <span class="comment"># 下标从 1 开始</span></span><br><span class="line">[(<span class="number">1</span>, <span class="string">'Spring'</span>), (<span class="number">2</span>, <span class="string">'Summer'</span>), (<span class="number">3</span>, <span class="string">'Fall'</span>), (<span class="number">4</span>, <span class="string">'Winter'</span>)]</span><br></pre></td></tr></table></figure><h4 id="普通的for循环"><a href="#普通的for循环" class="headerlink" title="普通的for循环"></a>普通的for循环</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;i = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seq = [<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> element <span class="keyword">in</span> seq:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> i, seq[i]</span><br><span class="line"><span class="meta">... </span>    i +=<span class="number">1</span></span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="number">0</span> one</span><br><span class="line"><span class="number">1</span> two</span><br><span class="line"><span class="number">2</span> three</span><br></pre></td></tr></table></figure><h4 id="for循环使用enumerate"><a href="#for循环使用enumerate" class="headerlink" title="for循环使用enumerate()"></a>for循环使用enumerate()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;seq = [<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i, element <span class="keyword">in</span> enumerate(seq):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> i, element</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="number">0</span> one</span><br><span class="line"><span class="number">1</span> two</span><br><span class="line"><span class="number">2</span> three</span><br></pre></td></tr></table></figure><h2 id="isinstance-函数"><a href="#isinstance-函数" class="headerlink" title="isinstance()函数"></a>isinstance()函数</h2><h3 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h3><p><code>isinstance()</code>函数用来判断一个对象是否是一个已知类型，类似<code>type()</code>。</p><blockquote><p> <em><code>isinstance()</code>和<code>type()</code>的区别：</em></p><ul><li><em><code>type()</code>不会认为子类是一种父类类型，不考虑继承关系；</em></li><li><em><code>isinstance()</code>会认为子类是一种父类类型，考虑继承关系；</em></li></ul><p><em>如果要判断两个类型是否相同推荐使用<code>isinstance()</code></em></p></blockquote><h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">isinstance(object, classinfo)</span><br></pre></td></tr></table></figure><h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><ul><li>object: 实例对象;</li><li>classinfo: 可以直接或间接是类名、基本类型或者由它们组成的元组；</li></ul><p><em>说明：</em></p><p><em>对于基本类型来说classinfo可以是：</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int, float, bool, complex, str(字符串), list, dict(字典), set, tuple</span><br></pre></td></tr></table></figure><p><em>要注意的是，classinfo的字符串是<strong>str</strong>而不是<strong>string</strong>，字典也是简写<strong>dict</strong>。</em></p><p><em>实例：</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arg=<span class="number">123</span></span><br><span class="line">isinstance(arg, int)    <span class="comment">#输出True</span></span><br><span class="line">isinstance(arg, str)    <span class="comment">#输出False</span></span><br><span class="line">isinstance(arg, string) <span class="comment">#报错</span></span><br></pre></td></tr></table></figure><h4 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a>返回值</h4><p>如果对象的类型与classinfo的类型相同则返回True，否则返回False。</p><h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;a = <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance (a,int)</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance (a,str)</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance (a,(str,int,list))    <span class="comment"># 是元组中的一个返回 True</span></span><br><span class="line"><span class="keyword">True</span></span><br></pre></td></tr></table></figure><h4 id="type与instance的区别"><a href="#type与instance的区别" class="headerlink" title="type与instance的区别"></a>type与instance的区别</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line">isinstance(A(), A)    <span class="comment"># returns True</span></span><br><span class="line">type(A()) == A        <span class="comment"># returns True</span></span><br><span class="line">isinstance(B(), A)    <span class="comment"># returns True</span></span><br><span class="line">type(B()) == A        <span class="comment"># returns False</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Python中常用的内置函数&quot;&gt;&lt;a href=&quot;#Python中常用的内置函数&quot; class=&quot;headerlink&quot; title=&quot;Python中常用的内置函数&quot;&gt;&lt;/a&gt;Python中常用的内置函数&lt;/h1&gt;&lt;p&gt;Python内置函数(built-in)是随着Python解释器的运行而被创建的。在Python程序中，你可以随时调用这些函数，而且不需要定义。在开发过程中，合理地使用这些内置函数能极大地提升你的开发效率。&lt;/p&gt;
&lt;p&gt;这篇文章是对我在开发过程中经常遇到的内置函数的用法总结。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://rilzob.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Python中的列表生成式</title>
    <link href="https://rilzob.com/2018/11/05/Python%E4%B8%AD%E7%9A%84%E5%88%97%E8%A1%A8%E7%94%9F%E6%88%90%E5%BC%8F/"/>
    <id>https://rilzob.com/2018/11/05/Python中的列表生成式/</id>
    <published>2018-11-05T13:28:09.197Z</published>
    <updated>2018-11-05T13:31:49.896Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python中的列表生成式"><a href="#Python中的列表生成式" class="headerlink" title="Python中的列表生成式"></a>Python中的列表生成式</h1><p>顾名思义，<strong>列表生成式</strong>就是用来生成列表的特定语法形式的表达式。列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。<a id="more"></a></p><h2 id="语法格式"><a href="#语法格式" class="headerlink" title="语法格式"></a>语法格式</h2><h3 id="基础语法格式"><a href="#基础语法格式" class="headerlink" title="基础语法格式"></a>基础语法格式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[exp <span class="keyword">for</span> iter_var <span class="keyword">in</span> iterable]</span><br></pre></td></tr></table></figure><h4 id="工作过程"><a href="#工作过程" class="headerlink" title="工作过程"></a>工作过程</h4><ul><li>迭代iterable中的每个元素；</li><li>每次迭代都先把结果赋值给iter_var，然后通过exp得到一个新的计算值；</li><li>最后所有通过exp得到的计算值以一个新的列表的形式返回；</li></ul><p>相当于：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">L = []</span><br><span class="line"><span class="keyword">for</span> iter_var <span class="keyword">in</span> iterable:</span><br><span class="line">    L.append(exp)</span><br></pre></td></tr></table></figure><h3 id="带过滤功能的语法格式"><a href="#带过滤功能的语法格式" class="headerlink" title="带过滤功能的语法格式"></a>带过滤功能的语法格式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[exp <span class="keyword">for</span> iter_var <span class="keyword">in</span> iterable if_exp]</span><br></pre></td></tr></table></figure><h4 id="工作过程-1"><a href="#工作过程-1" class="headerlink" title="工作过程"></a>工作过程</h4><ul><li>迭代iterable中的每个元素，每次迭代都先判断if_exp表达式是否成立，即判断将iter_var代入if_exp后表达式的结果，如果为真则进行下一步，如果为假则进行下一次迭代；</li><li>把迭代结果赋值给iter_var，然后通过exp得到一个新的计算值；</li><li>最后把所有通过exp得到的计算值以一个新列表的形式返回；</li></ul><p>相当于：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L = []</span><br><span class="line"><span class="keyword">for</span> iter_var <span class="keyword">in</span> iterable:</span><br><span class="line">    if_exp:</span><br><span class="line">        L.append(exp)</span><br></pre></td></tr></table></figure><h3 id="循环嵌套的语法格式"><a href="#循环嵌套的语法格式" class="headerlink" title="循环嵌套的语法格式"></a>循环嵌套的语法格式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[exp <span class="keyword">for</span> iter_var_A <span class="keyword">in</span> iterable_A <span class="keyword">for</span> iter_var_B <span class="keyword">in</span> iterable_B]</span><br></pre></td></tr></table></figure><h4 id="工作过程-2"><a href="#工作过程-2" class="headerlink" title="工作过程"></a>工作过程</h4><ul><li>迭代iterable_A中的每个元素，但是每迭代iterable_A中的一个元素，就把iterable_B中的所有元素都迭代一遍；</li><li>将每次迭代的变量iterable_var_A和iterable_var_B传入表达式exp(当然可以只传入两者之一)，计算出结果；</li><li>最后把所有通过exp得到的结果以一个新的列表的形式返回；</li></ul><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>其实列表生成式是Python中的一种“语法糖”，也就是说列表生成式是Python提供的一种生成列表的简洁形式，应用列表生成式可以快速生成一个新的list。它最主要的应用场景是：<strong>根据已存在的可迭代对象推导出一个新的list</strong>。</p><h2 id="使用实例"><a href="#使用实例" class="headerlink" title="使用实例"></a>使用实例</h2><p>我们可以对几个生成列表的要求分别通过“不使用列表生成式”和“使用列表生成式”来实现，然后做个对比总结。</p><h3 id="实例1：生成一个从3到10的数字列表"><a href="#实例1：生成一个从3到10的数字列表" class="headerlink" title="实例1：生成一个从3到10的数字列表"></a>实例1：生成一个从3到10的数字列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不使用列表生成式</span></span><br><span class="line">list1 = list(range(<span class="number">3</span>, <span class="number">11</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用列表生成式</span></span><br><span class="line">list2 = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>, <span class="number">11</span>)]</span><br></pre></td></tr></table></figure><h3 id="实例2：生成一个2n-1的数字列表，n为从3到10的数字"><a href="#实例2：生成一个2n-1的数字列表，n为从3到10的数字" class="headerlink" title="实例2：生成一个2n+1的数字列表，n为从3到10的数字"></a>实例2：生成一个2n+1的数字列表，n为从3到10的数字</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不使用列表生成式</span></span><br><span class="line">list3 = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">3</span>, <span class="number">11</span>):</span><br><span class="line">    list3.append(<span class="number">2</span>*n + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 使用列表生成式</span></span><br><span class="line">list4 = [<span class="number">2</span>*n + <span class="number">1</span> <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">3</span>, <span class="number">11</span>)]</span><br></pre></td></tr></table></figure><h3 id="实例3：过滤出指定的数字列表中的值大于20的元素"><a href="#实例3：过滤出指定的数字列表中的值大于20的元素" class="headerlink" title="实例3：过滤出指定的数字列表中的值大于20的元素"></a>实例3：过滤出指定的数字列表中的值大于20的元素</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">L = [<span class="number">3</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">14</span>, <span class="number">19</span>, <span class="number">33</span>, <span class="number">26</span>, <span class="number">57</span>, <span class="number">99</span>]</span><br><span class="line"><span class="comment"># 不使用列表生成式</span></span><br><span class="line">list5 = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> L:</span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">20</span>:</span><br><span class="line">        list5.append(x)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 使用列表生成式</span></span><br><span class="line">list6 = [x <span class="keyword">for</span> x <span class="keyword">in</span> L <span class="keyword">if</span> x &lt; <span class="number">20</span>]</span><br></pre></td></tr></table></figure><h3 id="实例4：计算两个集合的全排列，并将结果保存至一个新的列表中"><a href="#实例4：计算两个集合的全排列，并将结果保存至一个新的列表中" class="headerlink" title="实例4：计算两个集合的全排列，并将结果保存至一个新的列表中"></a>实例4：计算两个集合的全排列，并将结果保存至一个新的列表中</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">L1 = [<span class="string">'香蕉'</span>, <span class="string">'苹果'</span>, <span class="string">'橙子'</span>]</span><br><span class="line">L2 = [<span class="string">'可乐'</span>, <span class="string">'牛奶'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用列表生成式</span></span><br><span class="line">list7 = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> L1:</span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> L2:</span><br><span class="line">        list7.append((x,y))</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 使用列表生成式</span></span><br><span class="line">list8 = [(x,y) <span class="keyword">for</span> x <span class="keyword">in</span> L1 <span class="keyword">for</span> y <span class="keyword">in</span> L2]</span><br></pre></td></tr></table></figure><h3 id="实例5：将一个字典转换成由一组元组组成的列表，元组的格式为-key-value"><a href="#实例5：将一个字典转换成由一组元组组成的列表，元组的格式为-key-value" class="headerlink" title="实例5：将一个字典转换成由一组元组组成的列表，元组的格式为(key,  value)"></a>实例5：将一个字典转换成由一组元组组成的列表，元组的格式为(key,  value)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">D = &#123;<span class="string">'Tom'</span>: <span class="number">15</span>, <span class="string">'Jerry'</span>: <span class="number">18</span>, <span class="string">'Peter'</span>: <span class="number">13</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用列表生成式</span></span><br><span class="line">list9 = []</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> D.items():</span><br><span class="line">    list9.append((k, v))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 使用列表生成式</span></span><br><span class="line">list10 = []</span><br><span class="line">list10 = [(k, v) <span class="keyword">for</span> k, v <span class="keyword">in</span> D.items()]</span><br></pre></td></tr></table></figure><p>可见，在一些情况下使用列表生成式确实要方便、简洁很多，使用一行代码就搞定了。</p><h2 id="列表生成式与map、filter等高阶函数对比"><a href="#列表生成式与map、filter等高阶函数对比" class="headerlink" title="列表生成式与map、filter等高阶函数对比"></a>列表生成式与map、filter等高阶函数对比</h2><p>列表生成式的功能与之前文章提到的<code>map()</code>和<code>filter()</code>高阶函数功能很像，比如下面两个例子：</p><h3 id="实例1：把一个列表中所有的字符串转换为小写，非字符串元素保留原样"><a href="#实例1：把一个列表中所有的字符串转换为小写，非字符串元素保留原样" class="headerlink" title="实例1：把一个列表中所有的字符串转换为小写，非字符串元素保留原样"></a>实例1：把一个列表中所有的字符串转换为小写，非字符串元素保留原样</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">L = [<span class="string">'TOM'</span>, <span class="string">'Peter'</span>, <span class="number">10</span>, <span class="string">'Jerry'</span>]</span><br><span class="line"><span class="comment"># 用列表生成式实现</span></span><br><span class="line">list1 = [x.lower() <span class="keyword">if</span> isinstance(x, str) <span class="keyword">else</span> x <span class="keyword">for</span> x <span class="keyword">in</span> L]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用map()函数实现</span></span><br><span class="line">list2 = list(map(<span class="keyword">lambda</span> x : x.lower() <span class="keyword">if</span> isinstance(x, str) <span class="keyword">else</span> x, L))</span><br></pre></td></tr></table></figure><h3 id="实例2：把一个列表中所有的字符串转换为小写，非字符串元素移除"><a href="#实例2：把一个列表中所有的字符串转换为小写，非字符串元素移除" class="headerlink" title="实例2：把一个列表中所有的字符串转换为小写，非字符串元素移除"></a>实例2：把一个列表中所有的字符串转换为小写，非字符串元素移除</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">L = [<span class="string">'TOM'</span>, <span class="string">'Peter'</span>, <span class="number">10</span>, <span class="string">'Jerry'</span>]</span><br><span class="line"><span class="comment"># 用列表生成式实现</span></span><br><span class="line">list3 = [x.lower() <span class="keyword">for</span> x <span class="keyword">in</span> L <span class="keyword">if</span> isinstance(x, str)] </span><br><span class="line"><span class="comment"># 注意：这里for x in L相对if isinstance的先后位置，与上一个示例相比较</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用map()和filter()函数实现</span></span><br><span class="line">list4 = list(map(<span class="keyword">lambda</span> x: x.lower(), filter(<span class="keyword">lambda</span> x: isinstance(x, str), L)))</span><br></pre></td></tr></table></figure><p>对于大部分需求来讲，使用列表生成式和使用高阶函数都能实现。但是<code>map</code>和<code>filter</code>等一些高阶函数在Python3.x中的返回值类型变成了Iterator(迭代器)对象，这对于那些元素数量很大或无限的可迭代对象来说显然是更合适的，因为可以避免不必要的内存空间浪费。</p><blockquote><p>引用文章：</p><ol><li><a href="https://www.cnblogs.com/yyds/p/6281453.html" target="_blank" rel="noopener">Python之列表生成式、生成器、可迭代对象与迭代器 - 云游道士 - 博客园</a></li><li><a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431779637539089fd627094a43a8a7c77e6102e3a811000" target="_blank" rel="noopener">列表生成式 - 廖雪峰的官方网站</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Python中的列表生成式&quot;&gt;&lt;a href=&quot;#Python中的列表生成式&quot; class=&quot;headerlink&quot; title=&quot;Python中的列表生成式&quot;&gt;&lt;/a&gt;Python中的列表生成式&lt;/h1&gt;&lt;p&gt;顾名思义，&lt;strong&gt;列表生成式&lt;/strong&gt;就是用来生成列表的特定语法形式的表达式。列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://rilzob.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Python中的装饰器</title>
    <link href="https://rilzob.com/2018/11/04/Python%E4%B8%AD%E7%9A%84%E8%A3%85%E9%A5%B0%E5%99%A8/"/>
    <id>https://rilzob.com/2018/11/04/Python中的装饰器/</id>
    <published>2018-11-04T05:34:51.795Z</published>
    <updated>2018-11-04T05:46:29.257Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python中的装饰器"><a href="#Python中的装饰器" class="headerlink" title="Python中的装饰器"></a>Python中的装饰器</h1><p>装饰器本质上是一个<strong>Python函数</strong>，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的<strong>返回值也是一个函数对象</strong>。</p><p>它经常用于有切面需求的场景，比如：插入日志、性能检测、事务处理、缓存、权限校验等场景。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量与函数功能无关的雷同代码并继续重用。概括地讲，装饰器的作用就是为已经存在对象添加额外的功能。<a id="more"></a></p><p>先看一个简单例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'i am foo'</span>)</span><br></pre></td></tr></table></figure><p>现在有一个新的需求，希望可以记录下函数的执行日志，于是在代码中添加日志代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'i am foo'</span>)</span><br><span class="line">    logging.info(<span class="string">"foo is running"</span>)</span><br></pre></td></tr></table></figure><p>如果有些函数也有类似的需求，怎么做？都写一个logging在函数内？这样就造成了大量雷同的代码，为了减少重复写代码，可以这么做，重新定义一个函数专门处理日志，日志处理完之后再执行真正的业务代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_logging</span><span class="params">(func)</span>:</span></span><br><span class="line">    logging.warning(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">    func()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'i am bar'</span>)</span><br><span class="line"></span><br><span class="line">use_logging(bar)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># WARNING:root:bar is running</span></span><br><span class="line"><span class="comment"># i am bar</span></span><br></pre></td></tr></table></figure><p>逻辑上不难理解，但这样的话，我们每次都要将一个函数作为参数传递给<code>use_logging</code>函数。而且这种方式已经破坏了原有的代码逻辑结构，之前执行业务逻辑时，执行运行<code>bar()</code>，但是现在不得不改成<code>use_logging(bar)</code>。那么有没有更好的方式呢？当然有，答案就是<strong>装饰器</strong>。</p><h2 id="简单装饰器"><a href="#简单装饰器" class="headerlink" title="简单装饰器"></a>简单装饰器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_logging</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        logging.warning(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'i am bar'</span>)</span><br><span class="line">    </span><br><span class="line">bar = use_logging(bar)</span><br><span class="line">bar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># WARNING:root:bar is running</span></span><br><span class="line"><span class="comment"># i am bar</span></span><br></pre></td></tr></table></figure><p>函数<code>use_logging</code>就是装饰器，它把执行真正业务方法的func包裹在函数里面，看起来像<code>bar</code>被<code>logging</code>包起来，被装饰了。在这个例子中，函数进入和退出时，被称为一个横切面(Aspect)，这种编程方法称为<strong>面向切面的编程(Aspect-Oriented Programming)</strong>。</p><p><strong>@符号是装饰器的语法糖</strong>，在定义函数的时候使用，避免再一次赋值操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_logging</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        logging.warning(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@use_logging</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'i am foo'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">@use_logging</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'i am bar'</span>)</span><br><span class="line">    </span><br><span class="line">bar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># WARNING:root:bar is running</span></span><br><span class="line"><span class="comment"># i am bar</span></span><br></pre></td></tr></table></figure><p>如上所示，使用@符号我们就可以省去<code>bar = use_logging(bar)</code>这一句了，直接调用<code>bar()</code>即可得到想要的结果。如果我们有其他的类似函数，我们就可以继续调用装饰器来修饰函数，而不用重新修改函数或者增加新的封装。这样，我们就提高了程序的可重复利用性，并增加了程序的可读性。</p><p><em>装饰器在Python中使用如此方便都要归功于Python的函数能像普通的对象一样能作为参数传递给其他函数，可以被赋值给其他的变量，可以作为返回值，可以被定义在另一个函数内。</em></p><h2 id="带参数的装饰器"><a href="#带参数的装饰器" class="headerlink" title="带参数的装饰器"></a>带参数的装饰器</h2><p>装饰器还有更大的灵活性，例如带参数的装饰器：在上面的装饰器调用中，比如<code>@use_logging</code>，该装饰器唯一的参数就是执行业务的函数。装饰器的语法云溪我们在调用时，提供其他参数比如<code>@decorator(a)</code>。这样，就为装饰器的编写和使用提供了更大的灵活性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_logging</span><span class="params">(level)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorator</span><span class="params">(func)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> level == <span class="string">"warn"</span>:</span><br><span class="line">                logging.warning(<span class="string">"%s is running"</span> % func.__name__)</span><br><span class="line">            <span class="keyword">return</span> func(*args)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@use_logging(level="warn")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(name=<span class="string">'foo'</span>)</span>:</span></span><br><span class="line">    print(<span class="string">"i am %s"</span> % name)</span><br><span class="line">    </span><br><span class="line">foo()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># WARNING:root:bar is running</span></span><br><span class="line"><span class="comment"># i am bar</span></span><br></pre></td></tr></table></figure><p>上面的<code>use_logging</code>是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我们使用<code>@use_logging(level=&quot;warn&quot;)</code>调用的时候，Python能够发现这一层的封装，并把参数传递到装饰器的环境中。</p><h2 id="类装饰器"><a href="#类装饰器" class="headerlink" title="类装饰器"></a>类装饰器</h2><p>再来看看类装饰器，相比函数装饰器，类装饰器具有灵活度大，高内聚，封装性等优点。使用类装饰器还可以依赖类内部的<code>__call__</code>方法，当使用@形式将装饰器附加到函数上时，就会调用此方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        self._func = func</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'class decorator running'</span>)</span><br><span class="line">        self._func()</span><br><span class="line">        print(<span class="string">'class decorator ending'</span>)</span><br><span class="line">        </span><br><span class="line"><span class="meta">@Foo</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">print(<span class="string">'bar'</span>)</span><br><span class="line">    </span><br><span class="line">bar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># class decorator running</span></span><br><span class="line"><span class="comment"># bar</span></span><br><span class="line"><span class="comment"># class decorator ending</span></span><br></pre></td></tr></table></figure><h2 id="functools-wraps"><a href="#functools-wraps" class="headerlink" title="functools.wraps"></a>functools.wraps</h2><p>使用装饰器极大地复用了代码，但是它有个缺点就是原函数的元信息不见了，比如函数的<code>docstring</code>、<code>__name__</code>、参数列表，先看例子：</p><p>装饰器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logged</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">with_logging</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        print(func.__name__ + <span class="string">"was called"</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> with_logging</span><br></pre></td></tr></table></figure><p>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@logged</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''do some math'''</span></span><br><span class="line">    <span class="keyword">return</span> x + x * x</span><br></pre></td></tr></table></figure><p>该函数完全等价于：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''do some math'''</span></span><br><span class="line">    <span class="keyword">return</span> x + x * x</span><br><span class="line"></span><br><span class="line">f = logged(f)</span><br></pre></td></tr></table></figure><p>不难发现，函数<code>f</code>被<code>with_logging</code>替代了，当然它的<code>docstring</code>、<code>__name__</code>就变成了<code>with_logging</code>函数的信息了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(f.__name__)</span><br><span class="line">print(f.__doc__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># with_logging</span></span><br><span class="line"><span class="comment"># None</span></span><br></pre></td></tr></table></figure><p>这个问题就比较严重了，好在我们有<code>function.wraps</code>，<code>wraps</code>本身也是一个装饰器，它能把<strong>原函数的元信息拷贝到装饰器函数中</strong>，这使得装饰器函数也有和原函数一样的元信息了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logged</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(func)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">with_logging</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        print(func.__name__ + <span class="string">"was called"</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> with_logging</span><br><span class="line"></span><br><span class="line"><span class="meta">@logged</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''do some math'''</span></span><br><span class="line">    <span class="keyword">return</span> x + x * x</span><br><span class="line"></span><br><span class="line">print(f.__name__)</span><br><span class="line">print(f.__doc__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># f</span></span><br><span class="line"><span class="comment"># do some math</span></span><br></pre></td></tr></table></figure><h2 id="内置装饰器"><a href="#内置装饰器" class="headerlink" title="内置装饰器"></a>内置装饰器</h2><p><code>@staticmethod</code>、<code>@classmethod</code>、<code>@property</code></p><p>装饰器的顺序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@a</span></span><br><span class="line"><span class="meta">@b</span></span><br><span class="line"><span class="meta">@c</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br></pre></td></tr></table></figure><p>等效于</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f = a(b(c(f)))</span><br></pre></td></tr></table></figure><blockquote><p>引用资料：</p><ol><li><a href="https://www.zhihu.com/question/26930016" target="_blank" rel="noopener">如何理解Python装饰器？ - 知乎</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Python中的装饰器&quot;&gt;&lt;a href=&quot;#Python中的装饰器&quot; class=&quot;headerlink&quot; title=&quot;Python中的装饰器&quot;&gt;&lt;/a&gt;Python中的装饰器&lt;/h1&gt;&lt;p&gt;装饰器本质上是一个&lt;strong&gt;Python函数&lt;/strong&gt;，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的&lt;strong&gt;返回值也是一个函数对象&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;它经常用于有切面需求的场景，比如：插入日志、性能检测、事务处理、缓存、权限校验等场景。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量与函数功能无关的雷同代码并继续重用。概括地讲，装饰器的作用就是为已经存在对象添加额外的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://rilzob.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Python中的Lambda函数及其用法</title>
    <link href="https://rilzob.com/2018/11/01/Python%E4%B8%AD%E7%9A%84Lambda%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E7%94%A8%E6%B3%95/"/>
    <id>https://rilzob.com/2018/11/01/Python中的Lambda函数及其用法/</id>
    <published>2018-11-01T11:11:40.177Z</published>
    <updated>2018-11-01T11:17:16.258Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python中的Lambda函数及其用法"><a href="#Python中的Lambda函数及其用法" class="headerlink" title="Python中的Lambda函数及其用法"></a>Python中的Lambda函数及其用法</h1><p><strong>Lambda函数</strong>又称为匿名函数，<strong>匿名函数</strong>就是没有名字的函数。有些函数如果只是临时一用，而且它的业务逻辑也很简单时，就可以将其定义为匿名函数。</p><p>匿名函数有个限制，就是<strong>只能有一个表达式</strong>，不用写<code>return</code>，返回值就是该表达式的结果。</p><p>用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数。<a id="more"></a></p><p>先来看个简单的lambda函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">lambda</span> x, y : x + y</span><br><span class="line">&lt;function &lt;<span class="keyword">lambda</span>&gt; at <span class="number">0x102bc1c80</span>&gt;</span><br></pre></td></tr></table></figure><p><code>x</code>和<code>y</code>是函数的两个参数，冒号后面的表达式是函数的返回值，很明显这个匿名函数就是在求两个变量的和，但作为一个函数，没有名字如何使用呢？</p><p>这里我们暂且给这个匿名函数绑定一个名字，这样使得我们调用匿名函数成为可能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>add = <span class="keyword">lambda</span> x, y : x+y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add</span><br><span class="line">&lt;function &lt;<span class="keyword">lambda</span>&gt; at <span class="number">0x102bc2140</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><p>它等同于常规函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">add1</span><span class="params">(x, y)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> x+y</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add1</span><br><span class="line">&lt;function add1 at <span class="number">0x102bc1c80</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add1(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><h2 id="lambda函数的使用场景-函数式编程"><a href="#lambda函数的使用场景-函数式编程" class="headerlink" title="lambda函数的使用场景(函数式编程)"></a>lambda函数的使用场景(函数式编程)</h2><h3 id="sorted函数"><a href="#sorted函数" class="headerlink" title="sorted函数"></a>sorted函数</h3><p>例如：一个整数列表，要求按照列表中元素的绝对值大小升序排列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>list1 = [<span class="number">3</span>,<span class="number">5</span>,<span class="number">-4</span>,<span class="number">-1</span>,<span class="number">0</span>,<span class="number">-2</span>,<span class="number">-6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted(list1, key=<span class="keyword">lambda</span> x: abs(x))</span><br><span class="line">[<span class="number">0</span>, <span class="number">-1</span>, <span class="number">-2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>, <span class="number">-6</span>]</span><br></pre></td></tr></table></figure><p>排序函数<code>sorted</code>支持接收一个函数作为参数，该参数作为<code>sorted</code>的排序依据，这里按照列表元素的绝对值进行排序。</p><p>当然，也可以通过普通函数来实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> abs(x)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted(list1, key=foo)</span><br><span class="line">[<span class="number">0</span>, <span class="number">-1</span>, <span class="number">-2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>, <span class="number">-6</span>]</span><br></pre></td></tr></table></figure><p>只不过是使用这种方式，代码看起来不够<strong>Pythonic</strong>而已。</p><p><em>lambda：这是Python支持的一种有趣的语法，它允许你快速定义单行的最小函数，可以用在任何需要函数的地方:</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>add = <span class="keyword">lambda</span> x,y : x+y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add(<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"><span class="number">11</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="keyword">lambda</span> x,y:x+y)(<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"><span class="number">11</span></span><br></pre></td></tr></table></figure><h3 id="map-reduce-filter函数"><a href="#map-reduce-filter函数" class="headerlink" title="map,reduce,filter函数"></a>map,reduce,filter函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求1~20的平方</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(map(<span class="keyword">lambda</span> x:x*x,range(<span class="number">1</span>,<span class="number">21</span>))) </span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>, <span class="number">100</span>, <span class="number">121</span>, <span class="number">144</span>, <span class="number">169</span>, <span class="number">196</span>, <span class="number">225</span>, <span class="number">256</span>, <span class="number">289</span>, <span class="number">324</span>, <span class="number">361</span>, <span class="number">400</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求1~20之间的偶数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(filter(<span class="keyword">lambda</span> x:x%<span class="number">2</span> == <span class="number">0</span>,range(<span class="number">1</span>,<span class="number">21</span>))) </span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">20</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求1~100之和,再加上10000</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(<span class="keyword">lambda</span> x,y:x+y,range(<span class="number">1</span>,<span class="number">101</span>),<span class="number">10000</span>)</span><br><span class="line"><span class="number">15050</span></span><br></pre></td></tr></table></figure><h3 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h3><p><em>闭包：一个定义在函数内部的函数，闭包使得变量即使脱离了该函数的作用域也依然能被访问到。</em></p><p>看一个用lambda函数作为闭包的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(n)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> <span class="keyword">lambda</span> x:x+n <span class="comment"># 将匿名函数作为返回值返回</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add2 = add(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add2(<span class="number">15</span>)</span><br><span class="line"><span class="number">20</span></span><br></pre></td></tr></table></figure><p>这里的<code>lambda</code>函数就是一个闭包，在全局作用域范围中，<code>add2(15)</code>可以正常执行且返回值为20。之所以返回20是因为在<code>add</code>局部作用域中，变量n的值在闭包的作用下也可以被访问到。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><ol><li><a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431843456408652233b88b424613aa8ec2fe032fd85a000" target="_blank" rel="noopener">匿名函数 - 廖雪峰的官方网站</a></li><li><a href="https://www.cnblogs.com/huangbiquan/p/8030298.html" target="_blank" rel="noopener">深入理解Lambda函数及其用法 - 碧水幽幽泉 - 博客园</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Python中的Lambda函数及其用法&quot;&gt;&lt;a href=&quot;#Python中的Lambda函数及其用法&quot; class=&quot;headerlink&quot; title=&quot;Python中的Lambda函数及其用法&quot;&gt;&lt;/a&gt;Python中的Lambda函数及其用法&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Lambda函数&lt;/strong&gt;又称为匿名函数，&lt;strong&gt;匿名函数&lt;/strong&gt;就是没有名字的函数。有些函数如果只是临时一用，而且它的业务逻辑也很简单时，就可以将其定义为匿名函数。&lt;/p&gt;
&lt;p&gt;匿名函数有个限制，就是&lt;strong&gt;只能有一个表达式&lt;/strong&gt;，不用写&lt;code&gt;return&lt;/code&gt;，返回值就是该表达式的结果。&lt;/p&gt;
&lt;p&gt;用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://rilzob.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Python中的高阶函数</title>
    <link href="https://rilzob.com/2018/10/29/Python%E4%B8%AD%E7%9A%84%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0/"/>
    <id>https://rilzob.com/2018/10/29/Python中的高阶函数/</id>
    <published>2018-10-29T15:06:28.791Z</published>
    <updated>2018-10-30T01:40:04.219Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python中的高阶函数"><a href="#Python中的高阶函数" class="headerlink" title="Python中的高阶函数"></a>Python中的高阶函数</h1><p>在熟悉了Python基础知识后，我们已经可以做一些脚本开发，或者简单的程序。然而，当我们开发较为复杂的程序时，仅使用基础知识内容就会显得比较吃力。这时，了解Python中的一些高级特性会使我们的开发过程变得简单和快乐。</p><p>在函数式编程中，我们可以将函数当做变量一样自由使用。一个函数接收另一个函数作为参数，这种函数称之为<strong>高阶函数(Higher-order-Functions)</strong>。</p><a id="more"></a><p>看一个简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(g, arr)</span>:</span></span><br><span class="line"><span class="keyword">return</span> [g(x) <span class="keyword">for</span> x <span class="keyword">in</span> arr]</span><br></pre></td></tr></table></figure><p>上面的代码中<code>func</code>是一个高阶函数，它接收两个参数，第一个参数是函数，第二个参数是数组，<code>func</code>的功能是将函数g逐个作用于数组arr上，并返回一个新的数组。比如，我们可以这样用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">double</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">2</span> * x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line">list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">arr1 = func(double, list) <span class="comment"># arr1 = [2, 4, 6, 8]</span></span><br><span class="line">arr2 = func(square, list) <span class="comment"># arr2 = [1, 4, 9, 16]</span></span><br></pre></td></tr></table></figure><p>说到高阶函数，就不得不提到<strong>闭包</strong>，这里介绍一下Python中闭包的定义：</p><blockquote><p>如果在一个内部函数里，对外部作用域(但不是全局作用域)的变量进行引用，那么内部函数就被认为是闭包(closure)。</p></blockquote><p>就拿此例来说，内部函数<code>double</code>中包含了对函数<code>func</code>中局部变量<code>list</code>的引用，这就是闭包。</p><p><code>map</code> <code>reduce</code> <code>filter</code> <code>sorted</code>是Python中较为常用的内置高阶函数，它们为函数式编程提供了不少便利。</p><p><em>说明：本文介绍的内置高阶函数的定义可能会因为Python版本的不同而有所不同，文章以Python3.x版本中的定义为标准。</em></p><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p><code>map</code>函数的使用形式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">map(function, iterable, ...)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：这里函数一定要作为map的第一个参数，而不是第二个参数，否则会产生报错。</p><p><strong>解释</strong>：function函数会作用于可迭代对象的每一个元素，生成结果，并返回一个迭代器。更加具体一点说就是<code>map</code>函数接收两个参数，一个是函数，一个是Iterable，<code>map</code>将传入的函数依次作用到Iterable的每个元素，并把结果作为新的Iterator返回。</p><p>举例说明，比如我们一个函数f(x)=x^2，要把这个函数作用在一个list[1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用<code>map()</code>实现。</p><p>现在，我们用Python代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> x * x</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = map(f, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(r)</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>]</span><br></pre></td></tr></table></figure><p><code>map</code>传入的第一个参数是<code>f</code>，即函数对象本身。由于结果<code>r</code>是一个Iterator，Iterator是惰性序列，因此需要通过<code>list()</code>函数让它把整个序列都计算出来并返回一个list。</p><p>你可能会想，不需要<code>map</code>函数，写一个循环，也可以计算出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]:</span><br><span class="line">    L.append(f(n))</span><br><span class="line">print(L)</span><br></pre></td></tr></table></figure><p>的确可以，但是，从上面的循环代码，能一眼看明白“把f(x)作用在list的每一个元素并把结果生成一个新的list”吗？明显可读性就差了很多。</p><p>所以，<code>map</code>作为高阶函数，体现了Python的设计原则优雅、明确、简单，<strong>事实上它把运算规则抽象化</strong>。因此，我们不但可以计算简单的f(x)=x2，还可以计算任意复杂的函数，比如，把list中的所有数字转化为字符串格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(map(str, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>]</span><br></pre></td></tr></table></figure><p>只需一行代码。</p><p>看一些简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>map(square, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">&lt;map at <span class="number">0x106adfe48</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(map(square, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(map(<span class="keyword">lambda</span> x: x * x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))   <span class="comment"># 使用 lambda</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(map(<span class="keyword">lambda</span> x, y: x + y, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]))</span><br><span class="line">[<span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>]</span><br></pre></td></tr></table></figure><p>再来看个复杂一点例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">double</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triple</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span> *x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line">funcs = [double, triple, square]  <span class="comment"># 列表元素是函数对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 相当于 [double(4), triple(4), square(4)]</span></span><br><span class="line">value = list(map(<span class="keyword">lambda</span> f: f(<span class="number">4</span>), funcs))</span><br><span class="line">print(value)</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">[<span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>]</span><br></pre></td></tr></table></figure><p>最后我想要说明一点，迭代器有一个特点，就是所有的迭代器对象都可以作为<code>next()</code>内置函数的参数调用，每调用一次，就按角标顺序返回一个值，还是用代码讲吧：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iter = map(<span class="keyword">lambda</span> x: x * x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">print(next(iter)) <span class="comment"># 打印值为：1</span></span><br><span class="line">print(next(iter)) <span class="comment"># 打印值为：4</span></span><br><span class="line">print(next(iter)) <span class="comment"># 打印值为：9</span></span><br><span class="line">print(next(iter)) <span class="comment"># 打印值为：16</span></span><br><span class="line">print(next(iter)) <span class="comment"># 抛出StopIteration 异常</span></span><br></pre></td></tr></table></figure><h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><p><code>reduce</code>函数的使用形式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduce(function, iterable[, initializer])</span><br></pre></td></tr></table></figure><p><strong>解释</strong>：<code>reduce</code>函数必须接受两个参数，先将iterable的前两个item传给function，即function(item1, item2)，函数的返回值和iterable的下一个item再传给function，即function(function(item1, item2), item3)，如此迭代，直到iterable没有元素，如果有initializer，则作为初始值调用。</p><p>也就是说：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)</span><br><span class="line"><span class="comment"># 列表中是以从左到右作为优先顺序</span></span><br></pre></td></tr></table></figure><p>看一些例子，就能很快理解了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(<span class="keyword">lambda</span> x, y: x * y, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># 相当于 ((1 * 2) * 3) * 4</span></span><br><span class="line"><span class="number">24</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(<span class="keyword">lambda</span> x, y: x - y, [<span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>], <span class="number">20</span>)  <span class="comment"># ((20 - 8) - 5) - 1</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = <span class="keyword">lambda</span> a, b: a <span class="keyword">if</span> (a &gt; b) <span class="keyword">else</span> b   <span class="comment"># 两两比较，取最大值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(f, [<span class="number">5</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">10</span>])</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fn</span><span class="params">(x, y)</span>:</span> <span class="comment"># 把序列[1, 3, 5, 7, 9]变换成整数13579</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> x * <span class="number">10</span> + y</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(fn, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line"><span class="number">13579</span></span><br></pre></td></tr></table></figure><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p><code>filter</code>函数用于过滤元素，它的使用形式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter(function, iterable)</span><br></pre></td></tr></table></figure><p><strong>解释</strong>：和<code>map</code>类似，<code>filter</code>也接收一个函数和一个序列。但和<code>map</code>不同的是，<code>filter</code>把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。将function依次作用于iterable的每个item上，即function(item)，用function返回值为True的item构成iterator作为<code>filter</code>的最终返回值。</p><p>看一些例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>even_num = list(filter(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>even_num</span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>odd_num = list(filter(<span class="keyword">lambda</span> x: x % <span class="number">2</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>odd_num</span><br><span class="line">[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(filter(<span class="keyword">lambda</span> x: x &lt; <span class="string">'g'</span>, <span class="string">'hijack'</span>))</span><br><span class="line"><span class="string">'ac'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>filter(<span class="keyword">lambda</span> x: x &lt; <span class="string">'g'</span>, <span class="string">'hijack'</span>)</span><br><span class="line">&lt;filter object at <span class="number">0x1034b4080</span>&gt;   <span class="comment"># python3</span></span><br></pre></td></tr></table></figure><p>可见用<code>filter</code>这个高阶函数，关键在于正确实现一个“筛选”函数。</p><p>注意到<code>filter</code>函数返回的同样是一个Iterator，也就是一个惰性序列，所以要强迫<code>filter</code>完成计算结果，需要用<code>list()</code>函数获得所有结果。</p><h2 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a>sorted</h2><p><code>sorted</code>函数用于对list进行排序，它的使用形式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sorted(iterable, *, key=<span class="keyword">None</span>, reverse=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p><strong>解释</strong>：<code>sorted</code>有两个可选参数，必须指定为关键字参数。将key指定的函数作用于iterable的每一个元素上，并根据key函数返回的结果进行排序，最终返回一个新的排序列表。key默认值为None，即直接比较元素大小。</p><p>reverse是一个布尔值。如果设置为True，则列表元素将按照比较结果相反的方式进行排序。</p><p>看一些例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted([<span class="number">36</span>, <span class="number">5</span>, <span class="number">-12</span>, <span class="number">9</span>, <span class="number">-21</span>])</span><br><span class="line">[<span class="number">-21</span>, <span class="number">-12</span>, <span class="number">5</span>, <span class="number">9</span>, <span class="number">36</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted([<span class="number">36</span>, <span class="number">5</span>, <span class="number">-12</span>, <span class="number">9</span>, <span class="number">-21</span>], key=abs)</span><br><span class="line">[<span class="number">5</span>, <span class="number">9</span>, <span class="number">-12</span>, <span class="number">-21</span>, <span class="number">36</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted([<span class="string">'bob'</span>, <span class="string">'about'</span>, <span class="string">'Zoo'</span>, <span class="string">'Credit'</span>])</span><br><span class="line">[<span class="string">'Credit'</span>, <span class="string">'Zoo'</span>, <span class="string">'about'</span>, <span class="string">'bob'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted([<span class="string">'bob'</span>, <span class="string">'about'</span>, <span class="string">'Zoo'</span>, <span class="string">'Credit'</span>], key=str.lower)</span><br><span class="line">[<span class="string">'about'</span>, <span class="string">'bob'</span>, <span class="string">'Credit'</span>, <span class="string">'Zoo'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted([<span class="string">'bob'</span>, <span class="string">'about'</span>, <span class="string">'Zoo'</span>, <span class="string">'Credit'</span>], key=str.lower, reverse=<span class="keyword">True</span>)</span><br><span class="line">[<span class="string">'Zoo'</span>, <span class="string">'Credit'</span>, <span class="string">'bob'</span>, <span class="string">'about'</span>]</span><br></pre></td></tr></table></figure><p>从上述例子可以看出，高阶函数的抽象能力是非常强大的，而且核心代码可以保持得非常简洁。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>可接受其他函数作为参数的函数称为高阶函数；</li><li><code>map</code> <code>reduce</code> <code>filter</code> <code>sorted</code>为函数式编程提供了不少便利，可使代码变得更简洁；</li><li>通过<code>map()</code>来对Iterable中的每个元素进行相同的函数处理最终返回一个Iterator。</li><li><code>reduce()</code>类似栈的思想，先让栈顶的两个元素出栈作为函数的两个参数，再将函数的返回值入栈，然后再让栈顶两个元素出栈，不断循环下去，直到栈里没有元素为止。</li><li><code>filter()</code>的作用是从一个序列中筛选出符合条件的元素。由于<code>filter()</code>使用了惰性计算，所以只有在取<code>filter()</code>结果的时候，才会真正筛选并每次返回下一个筛出的元素。</li><li><code>sorted()</code>也是一个高阶函数。用<code>sorted()</code>排序的关键在于实现一个映射函数。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ol><li><a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014317849054170d563b13f0fa4ce6ba1cd86e18103f28000" target="_blank" rel="noopener">高阶函数 - 廖雪峰的官方网站</a></li><li><a href="http://wiki.jikexueyuan.com/project/explore-python/Functional/map_reduce_filter.html" target="_blank" rel="noopener">map/reduce/filter - Python 之旅 - 极客学院Wiki</a></li><li><a href="http://wiki.jikexueyuan.com/project/explore-python/Functional/high_order_func.html" target="_blank" rel="noopener">高阶函数 - Python 之旅 - 极客学院Wiki</a></li><li><a href="https://www.jianshu.com/p/bd595a0006cd" target="_blank" rel="noopener">Python笔记(二)：高级特性之高阶函数 - 简书</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Python中的高阶函数&quot;&gt;&lt;a href=&quot;#Python中的高阶函数&quot; class=&quot;headerlink&quot; title=&quot;Python中的高阶函数&quot;&gt;&lt;/a&gt;Python中的高阶函数&lt;/h1&gt;&lt;p&gt;在熟悉了Python基础知识后，我们已经可以做一些脚本开发，或者简单的程序。然而，当我们开发较为复杂的程序时，仅使用基础知识内容就会显得比较吃力。这时，了解Python中的一些高级特性会使我们的开发过程变得简单和快乐。&lt;/p&gt;
&lt;p&gt;在函数式编程中，我们可以将函数当做变量一样自由使用。一个函数接收另一个函数作为参数，这种函数称之为&lt;strong&gt;高阶函数(Higher-order-Functions)&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://rilzob.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Python中@classmethod和@staticmethod的区别</title>
    <link href="https://rilzob.com/2018/10/27/Python%E4%B8%AD@classmethod%E5%92%8C@staticmethod%E7%9A%84%E5%8C%BA%E5%88%AB%E7%9A%84%E5%89%AF%E6%9C%AC/"/>
    <id>https://rilzob.com/2018/10/27/Python中@classmethod和@staticmethod的区别的副本/</id>
    <published>2018-10-27T04:46:02.147Z</published>
    <updated>2018-10-27T04:50:31.333Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python中-classmethod和-staticmethod的区别"><a href="#Python中-classmethod和-staticmethod的区别" class="headerlink" title="Python中@classmethod和@staticmethod的区别"></a>Python中@classmethod和@staticmethod的区别</h1><p>接上一篇介绍<a href="https://rilzob.github.io/2018/10/27/Python%E4%B8%AD@staticmethod%E5%92%8C@classmethod%E7%9A%84%E7%94%A8%E6%B3%95%E7%9A%84%E5%89%AF%E6%9C%AC/" target="_blank" rel="noopener">Python中@staticmethod和@classmethod的用法</a>的文章。虽然<code>@classmethod</code>和<code>@staticmethod</code>非常相似，但两个修饰符的使用情况仍<strong>略有不同</strong>。<a id="more"></a></p><p>从它们的使用上来看：</p><ul><li><code>@classmethod</code>必须引用一个类对象作为第一个参数，即第一个参数需要是表示自身类的cls参数。同时<code>@classmethod</code>因持有cls参数，所以可以调用类的属性，类的方法，实例化对象等，避免硬编码。</li><li><code>@staticmethod</code>则可以完全没有参数，但在<code>@staticmethod</code>中要调用到这个类的一些属性方法，只能直接类名.属性名或类名.方法名()。</li></ul><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Date</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, day=<span class="number">0</span>, month=<span class="number">0</span>, year=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.day = day</span><br><span class="line">        self.month = month</span><br><span class="line">        self.year = year</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_string</span><span class="params">(cls, date_as_string)</span>:</span></span><br><span class="line">        day, month, year = map(int, date_as_string.split(<span class="string">"-"</span>))</span><br><span class="line">        date1 = cls(day, month, year)</span><br><span class="line">        <span class="keyword">return</span> date1</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_date_valid</span><span class="params">(date_as_string)</span>:</span></span><br><span class="line">        day, month, year = map(int, date_as_string.split(<span class="string">"-"</span>))</span><br><span class="line">        <span class="keyword">return</span> day &lt;= <span class="number">31</span> <span class="keyword">and</span> month &lt;= <span class="number">12</span> <span class="keyword">and</span> year &lt;= <span class="number">3999</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">date2 = Date.from_string(<span class="string">"27-10-2018"</span>)</span><br><span class="line">is_date = Date.is_date_valid(<span class="string">"27-10-2018"</span>)</span><br></pre></td></tr></table></figure><h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><p>让我们假设这样一个类的例子，用来处理日期信息(这将是我们的样板)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Date</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, day=<span class="number">0</span>, month=<span class="number">0</span>, year=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.day = day</span><br><span class="line">        self.month = month</span><br><span class="line">        self.year = year</span><br></pre></td></tr></table></figure><p>显然，这个类可以用来存储关于某些日期的信息(没有时区信息；假设所有日期都以UTC表示)。</p><p>这个类中有<code>__init__</code>，它是Python类实例的初始化方法，它接收参数作为类实例方法，具有第一个非可选参数<code>self</code>(作为对新创建实例的引用)。</p><h3 id="Class-Method"><a href="#Class-Method" class="headerlink" title="Class Method"></a>Class Method</h3><p>我们有一些任务，通过使用<code>@classmethod</code>可以很好地完成它们。</p><p><em>假设我们想要创建许多Date类实例，其日期信息来自外部输入(编码格式为’dd-mm-year’的字符串)，并假设我们必须在项目源代码的不同位置执行此操作。</em></p><p>所以我们这里必须做到：</p><ol><li>解析输入的字符串以接收日、月、年作为三个整数变量或由这些变量组成的三元组。</li><li>通过将上面求到的值传递给初始化调用来创建Date类实例。</li></ol><p>代码看起来会是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">day, month, year = map(int, string_date.split(<span class="string">'-'</span>))</span><br><span class="line">date1 = Date(day, month, year)</span><br></pre></td></tr></table></figure><p>如果使用<code>@classmethod</code>修饰符写在类中，将会是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_string</span><span class="params">(cls, date_as_string)</span>:</span></span><br><span class="line">        day, month, year = map(int, date_as_string.split(<span class="string">'-'</span>))</span><br><span class="line">        date1 = cls(day, month, year)</span><br><span class="line">        <span class="keyword">return</span> date1</span><br><span class="line"></span><br><span class="line">date2 = Date.from_string(<span class="string">'27-10-2018'</span>)</span><br></pre></td></tr></table></figure><p>让我们更仔细地看看上面的代码实现，并回想一下我们做了什么？</p><ul><li><p>我们在一个地方实现了日期字符串解析函数，现在它可以重用。</p></li><li><p>将日期字符串解析函数封装在类中并且工作正常(当然你可以在其他地方实现日期字符串解析作为单个函数，但这个解决方案更适合OOP范例)。</p></li><li><code>cls</code>是一个保存<strong>类本身</strong>的对象，而不是类的实例。这很酷😎，因为如果我们继承Date类，所有子类也会定义<code>from_string()</code>。</li></ul><h3 id="Static-Method"><a href="#Static-Method" class="headerlink" title="Static Method"></a>Static Method</h3><p><code>@staticmethod</code>确实与<code>@classmethod</code>很相似，但<code>@staticmethod</code>不需要任何强制性参数(如类方法或实例方法)。</p><p>让我们看看下一个任务(下一个用例):</p><p><em>假设我们有一个日期字符串，我们想要以某种方式进行验证它是否符合要求的格式。此任务也需要封装在Date类中，但不需要实例化它。</em></p><p>这里使用<code>@staticmethod</code>就会很有效。让我们看一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_date_valid</span><span class="params">(date_as_string)</span>:</span></span><br><span class="line">    day, month, year = map(int, date_as_string.split(<span class="string">'-'</span>))</span><br><span class="line">    <span class="keyword">return</span> day &lt;= <span class="number">31</span> <span class="keyword">and</span> month &lt;= <span class="number">12</span> <span class="keyword">and</span> year &lt;= <span class="number">3999</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># usage:</span></span><br><span class="line">is_date = Date.is_date_valid(<span class="string">'27-10-2018'</span>)</span><br></pre></td></tr></table></figure><p>运行上述代码得到<code>is_date</code>是个boolen型变量，而非<code>is_date_valid</code>函数返回的day，month，year三个整型数据。</p><p>因此，我们可以从<code>@staticmethod</code>的使用中看到，我们无法访问类的内容——它基本上只是一个函数，在语法上称为方法，无法访问对象及其内部(字段和其他类方法)。而使用<code>@classmethod</code>却可以做到。</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>上面的文章已经很全面地总结了<code>@classmethod</code>和<code>@staticmethod</code>的区别。在这里我想强调当你<strong>创建构造函数</strong>时，你应该选择<code>@classmethod</code>而不是<code>@staticmethod</code>的另一个原因。在上面的例子中，使用<code>@classmethod</code> <code>from_string()</code>作为Factory，接收不符合<code>__init__</code>要求的参数创建Date类实例。使用<code>@staticmethod</code>可以完成同样的操作，如下面代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Date</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, month, day, year)</span>:</span></span><br><span class="line">    self.month = month</span><br><span class="line">    self.day   = day</span><br><span class="line">    self.year  = year</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">display</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"&#123;0&#125;-&#123;1&#125;-&#123;2&#125;"</span>.format(self.month, self.day, self.year)</span><br><span class="line"></span><br><span class="line"><span class="meta">  @staticmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">millenium</span><span class="params">(month, day)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Date(month, day, <span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">new_year = Date(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2013</span>)               <span class="comment"># Creates a new Date object</span></span><br><span class="line">millenium_new_year = Date.millenium(<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># also creates a Date object. </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Proof:</span></span><br><span class="line">new_year.display()           <span class="comment"># "1-1-2013"</span></span><br><span class="line">millenium_new_year.display() <span class="comment"># "1-1-2000"</span></span><br><span class="line"></span><br><span class="line">isinstance(new_year, Date) <span class="comment"># True</span></span><br><span class="line">isinstance(millenium_new_year, Date) <span class="comment"># True</span></span><br></pre></td></tr></table></figure><p>运行结果显示<code>new_year</code>和<code>millenium_new_year</code>都是Date类实例。</p><p>但是，如果仔细观察就会发现，<code>millenium_new_year</code>是以<strong>硬编码</strong>的方式创建的Date类实例。这意味着即使一个类继承Date类，该子类仍将创建普通的Date对象即父类对象，而不具有该子类本身的任何属性。请参阅以下示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DateTime</span><span class="params">(Date)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">display</span><span class="params">(self)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> <span class="string">"&#123;0&#125;-&#123;1&#125;-&#123;2&#125; - 00:00:00PM"</span>.format(self.month, self.day, self.year)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">datetime1 = DateTime(<span class="number">10</span>, <span class="number">10</span>, <span class="number">1990</span>)</span><br><span class="line">datetime2 = DateTime.millenium(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">isinstance(datetime1, DateTime) <span class="comment"># True</span></span><br><span class="line">isinstance(datetime2, DateTime) <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line">datetime1.display() <span class="comment"># returns "10-10-1990 - 00:00:00PM"</span></span><br><span class="line">datetime2.display() <span class="comment"># returns "10-10-2000" because it's not a DateTime object but a Date object. Check the implementation of the millenium method on the Date class</span></span><br></pre></td></tr></table></figure><p>DateTime类继承Date类，因此具有Date类的<code>millenium()</code>方法。<code>datetime2</code>通过调用DateTime继承来的<code>millenium()</code>方法来创建DateTime类实例。然而代码却显示<code>datetime2</code>并不是DateTime类实例(<code>isinstance(datetime2, DateTime) # False</code>)。怎么回事？<strong>这是因为使用了<code>@staticmethod</code>修饰符</strong>。</p><p>在大多数情况下，这是你不希望出现的。如果你想要的是一个”完整“的类实例，并且是通过调用它的父类方法所创建的话，那么<code>@classmethod</code>就是你所需要的。</p><p>将<code>Date.millenium()</code>重写为(这是上述代码中唯一改变的部分)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">millenium</span><span class="params">(cls, month, day)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cls(month, day, <span class="number">2000</span>)</span><br></pre></td></tr></table></figure><p>确保该类的创建不是通过硬编码。<code>cls</code>可以是任何子类，生成的对象将正确地成为cls的实例。我们来试试吧：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">datetime1 = DateTime(<span class="number">10</span>, <span class="number">10</span>, <span class="number">1990</span>)</span><br><span class="line">datetime2 = DateTime.millenium(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">isinstance(datetime1, DateTime) <span class="comment"># True</span></span><br><span class="line">isinstance(datetime2, DateTime) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">datetime1.display() <span class="comment"># "10-10-1990 - 00:00:00PM"</span></span><br><span class="line">datetime2.display() <span class="comment"># "10-10-2000 - 00:00:00PM"</span></span><br></pre></td></tr></table></figure><p>看吧，用<code>@classmethod</code>替代<code>@staticmethod</code>你不希望出现的情况就会消失。<strong>使用了<code>@staticmethod</code>修饰符定义构造函数就是问题出现的关键。</strong></p><p>文章的内容有点多，可能需要花一些时间进行理解，最后提供一个小示例帮助大家加深记忆一下<code>@classmethod</code>和<code>@staticmethod</code>的<strong>主要不同</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span><span class="params">(object)</span>:</span></span><br><span class="line">    bar = <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'foo'</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">static_foo</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'static_foo'</span></span><br><span class="line">        <span class="keyword">print</span> A.bar</span><br><span class="line"> </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">class_foo</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'class_foo'</span></span><br><span class="line">        <span class="keyword">print</span> cls.bar</span><br><span class="line">        cls().foo()</span><br><span class="line"> </span><br><span class="line">A.static_foo()</span><br><span class="line">A.class_foo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">static_foo</span><br><span class="line"><span class="number">1</span></span><br><span class="line">class_foo</span><br><span class="line"><span class="number">1</span></span><br><span class="line">foo</span><br></pre></td></tr></table></figure><blockquote><p>引用文章：</p><ol><li><a href="https://blog.csdn.net/handsomekang/article/details/9615239" target="_blank" rel="noopener">飘逸的python - @staticmethod和@classmethod的作用与区别 - mattkang - CSDN博客</a></li><li><a href="https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner" target="_blank" rel="noopener">python - Meaning of @classmethod and @staticmethod for beginner? - Stack Overflow</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Python中-classmethod和-staticmethod的区别&quot;&gt;&lt;a href=&quot;#Python中-classmethod和-staticmethod的区别&quot; class=&quot;headerlink&quot; title=&quot;Python中@classmethod和@staticmethod的区别&quot;&gt;&lt;/a&gt;Python中@classmethod和@staticmethod的区别&lt;/h1&gt;&lt;p&gt;接上一篇介绍&lt;a href=&quot;https://rilzob.github.io/2018/10/27/Python%E4%B8%AD@staticmethod%E5%92%8C@classmethod%E7%9A%84%E7%94%A8%E6%B3%95%E7%9A%84%E5%89%AF%E6%9C%AC/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Python中@staticmethod和@classmethod的用法&lt;/a&gt;的文章。虽然&lt;code&gt;@classmethod&lt;/code&gt;和&lt;code&gt;@staticmethod&lt;/code&gt;非常相似，但两个修饰符的使用情况仍&lt;strong&gt;略有不同&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://rilzob.com/categories/Python/"/>
    
    
  </entry>
  
</feed>
